<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习知识点汇总 | Personnal Blog of YUAN Tingyi</title><meta name="author" content="YUAN Tingyi"><meta name="copyright" content="YUAN Tingyi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[toc] 为什么二分类不用 MSE 损失函数？ 对于二分类问题，损失函数不采用均方误差（Mean Squared Error，MSE）至少可以从两个角度来分析。 均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下 $$ J_{M S E}&#x3D;\frac{1}{N} \sum_{i&#x3D;1}^{N}\l">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习知识点汇总">
<meta property="og:url" content="http://example.com/2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/index.html">
<meta property="og:site_name" content="Personnal Blog of YUAN Tingyi">
<meta property="og:description" content="[toc] 为什么二分类不用 MSE 损失函数？ 对于二分类问题，损失函数不采用均方误差（Mean Squared Error，MSE）至少可以从两个角度来分析。 均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下 $$ J_{M S E}&#x3D;\frac{1}{N} \sum_{i&#x3D;1}^{N}\l">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/9.jpg">
<meta property="article:published_time" content="2022-06-01T12:43:33.320Z">
<meta property="article:modified_time" content="2022-06-09T09:51:43.374Z">
<meta property="article:author" content="YUAN Tingyi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/9.jpg"><link rel="shortcut icon" href="/img/favicon-32x32.png"><link rel="canonical" href="http://example.com/2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习知识点汇总',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-09 17:51:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/9.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Personnal Blog of YUAN Tingyi</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习知识点汇总</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-06-01T12:43:33.320Z" title="Created 2022-06-01 20:43:33">2022-06-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-06-09T09:51:43.374Z" title="Updated 2022-06-09 17:51:43">2022-06-09</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习知识点汇总"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[toc]</p>
<h2 id="为什么二分类不用-MSE-损失函数？">为什么二分类不用 MSE 损失函数？</h2>
<p>对于二分类问题，损失函数不采用均方误差（Mean Squared Error，MSE）至少可以从两个角度来分析。</p>
<p>均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下</p>
<p>$$<br>
J_{M S E}=\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>
$$</p>
<h3 id="数据分布角度">数据分布角度</h3>
<p>首先，使用 MSE 意味着假设数据采样误差是遵循正态分布的。用贝叶斯门派的观点来看，意味着作了高斯先验的假设。实际上，可以分为两类（即二分类）的数据集是遵循伯努利分布。</p>
<p>如果假设误差遵循正态分布，并使用最大似然估计（Maximum Likelihood Estimation，MLE），我们将得出 MSE 正是用于优化模型的损失函数。</p>
<p>首先，正态/高斯分布$\mathcal{N}$由两个参数$(\mu, \sigma)$定义，训练数据$(x, y)$包括特征$x$和实际观测值$y$。简单来说，每当我们采样数据时，观测值有时会与真实值相匹配，有时观测值会因某些误差而失真。我们假设所有观测到的数据都带有一定的误差（即$\epsilon$），并且误差遵循均值为$0$，方差未知的正态分布。我们可以这样来看，实际观测值$y$通常围绕待预测的目标值$\hat{y}$呈正态分布。</p>
<p>在一定的假设下， 我们可以通过最大化似然（MLE）推导出均方差损失的形式。假设模型预测值与真实值之间的误差服从高斯分布$(\mu = 0, \sigma = 1)$，则给定一个$x_i$，模型输出真实值$y_i$的概率为:</p>
<p>$$<br>
p\left(y_{i} \mid x_{i}\right)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}-\hat{y_{i}}\right)^{2}}{2}\right)<br>
$$</p>
<p>进一步我们假设数据集中 N 个样本点之间相互独立，则给定所有$x$输出所有真实值$y$的概率，即似然 Likelihood，为所有$p(y_i \mid x_i)$的累乘:</p>
<p>$$<br>
L(x, y)=\prod_{i=1}^{N} \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}\right)<br>
$$</p>
<p>通常为了方便计算，我们最大化对数似然函数（maximize log-likelihood）</p>
<p>$$<br>
LL(x, y) = log(L(x,y)) = -\frac{N}{2} \log 2 \pi-\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>
$$</p>
<p>去掉无关的第一项，然后转化为最小化负对数似然（Minimize Negative Log-Likelihood）</p>
<p>$$<br>
N L L(x, y)=\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>
$$</p>
<p>可以看到这个实际上就是均方差损失的形式。也就是说在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
<p>对于平均绝对误差 Mean Absolute Error (MAE) 是另一类常用的损失函数，也称为 L1 Loss。其基本形式如下:</p>
<p>$$<br>
J_{M A E}=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|<br>
$$</p>
<p>我们也可以通过类似的过程进行推导：<br>
假设模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution$(\mu = 0, b = 1)$，则给定一个 $x_i$ 模型输出真实值$y_i$的概率为:</p>
<p>$$<br>
p\left(y_{i} \mid x_{i}\right)=\frac{1}{2} \exp \left(-\left|y_{i}-\hat{y}_{i}\right|\right)<br>
$$</p>
<p>$$<br>
L(x, y)=\prod_{i=1}^{N} \frac{1}{2} \exp \left(-\left|y_{i}-\hat{y}<em>{i}\right|\right) \<br>
L L(x, y)=N \ln \frac{1}{2}-\sum</em>{i=1}^{N}\left|y_{i}-\hat{y}<em>{i}\right| \<br>
N L L(x, y)=\sum</em>{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|<br>
$$</p>
<h3 id="优化角度">优化角度</h3>
<p>逻辑回归是分类的一种，输出包含 sigmoid（也可以是其他非线性激活函数），而如果还用 mse 做损失函数的话：</p>
<p>$$<br>
\text { Loss }=\frac{1}{2} \sum_{i=1}^{n}\left(y_{i}-\hat{y}<em>{i}\right)^{2}=\frac{1}{2} \sum</em>{i=1}^{n}\left(y_{i}-\sigma\left(w x_{i}+b\right)\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\frac{1}{1+e^{-\left(w x_{i}+b\right)}}\right)^{2}<br>
$$</p>
<p>MSE 函数对于二分类问题来说是非凸的，有多个极值点，所以不适用做损失函数了。<code>sigmoid</code>激活函数的输入很可能直接就在平坦区域，那么导数就几乎是 0，梯度就几乎不会被反向传递，梯度直接消失了。所以 mse 做损失函数的时候最后一层不能用 sigmoid 做激活函数，其他层可以用 sigmoid 做激活函数。。简而言之，如果使用 MSE 损失函数训练二分类模型，则不能保证将损失函数最小化。这是因为 MSE 函数期望实数输入在范围$(-\infin, \infin)$中，而二分类模型通过 <code>Sigmoid</code> 函数输出范围为$(0, 1)$的概率。</p>
<p>当将一个无界的值传递给 MSE 函数时，在目标值 处有一个明确最小值的情况下，会形成一条漂亮的 U 形（凸）曲线。另一方面，当将来自 Sigmoid 等函数的有界值传递给 MSE 函数时，可能会导致结果并不是凸的。</p>
<p>当然，用其他损失函数只能保证在第一步不会直接死掉，反向传播如果激活函数和归一化做得不好，同样会梯度消失。所以从梯度这个原因说 mse 不好不是很正确。</p>
<h3 id="用交叉熵损失函数后还会有梯度消失的问题吗？">用交叉熵损失函数后还会有梯度消失的问题吗？</h3>
<p>梯度消失问题存在 2 个地方：</p>
<ul>
<li>
<p>1.损失函数对权值 w 求导，这是误差反向传播的第一步，mse 的损失函数会在损失函数求导这一个层面上就导致梯度消失；所以使用交叉熵损失函数。</p>
</li>
<li>
<p>2.误差反向传播时，链式求导也会使得梯度消失。使用交叉熵损失函数也不能避免反向传播带来的梯度消失，此时规避梯度消失的方法：</p>
<ul>
<li>ReLU 等激活函数；</li>
<li>输入归一化、每层归一化；</li>
<li>网络结构上调整，比如 LSTM、GRU 等深度神经网络，不管用什么损失函数，隐含层的激活函数如果用 sigmoid，肯定会梯度消失，训练无效。如果是浅层神经网络，影响可能不是很大，和神经网络的输入有关，如果输入经过了归一化，结果靠谱，如果没有归一化，梯度也直接消失，训练肯定失败。</li>
</ul>
</li>
</ul>
<p>损失函数和激活函数决定的是模型会不会收敛，也影响训练速度；优化器决定的是模型能不能跳出局部极小值、跳出鞍点、能不能快速下降这些问题的。</p>
<h2 id="LSTM-原理">LSTM 原理</h2>
<p>LSTM 是循环神经网络 RNN 的变种，其包含三个门，分别是<code>输入门</code>、<code>遗忘门</code>和<code>输出门</code>。</p>
<h3 id="LSTM-与-GRU-的区别">LSTM 与 GRU 的区别</h3>
<ul>
<li>GRU 只有两个门(update 和 reset)，LSTM 有三个门(forget, input, output)，GRU 直接将<code>hidden state</code>传给下一个单元，而 LSTM 用 memory cell 把<code>hidden state</code>包装起来。</li>
<li>LSTM 和 GRU 的性能在很多任务上不分伯仲。</li>
<li>GRU 的参数更少，因此更容易收敛，但在大数据集上，LSTM 性能表现更好。</li>
</ul>
<h2 id="Transformer-的原理">Transformer 的原理</h2>
<p>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的 Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization 层。</p>
<h3 id="Transformer-的计算公式，QKV-怎么得到">Transformer 的计算公式，QKV 怎么得到</h3>
<p>QKV 分别是由输入 X 通过线性变换得到的。</p>
<p>$$<br>
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>
$$</p>
<h3 id="multi-head-的作用">multi-head 的作用</h3>
<p>多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用多个卷积核的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。</p>
<h3 id="residual-连接">residual 连接</h3>
<p>在 transformer 的 encoder 和 decoder 中，各用到了 6 层的 attention 模块，每一个 attention 模块又和一个 FeedForward 层（简称 FFN）相接。对每一层的 attention 和 FFN，都采用了一次残差连接，即把每一个位置的输入数据和输出数据相加，使得 Transformer 能够有效训练更深的网络。在残差连接过后，再采取 Layer Nomalization 的方式。</p>
<p>resnet 的思想：残差模块能让训练变得更加简单，如果输入值和输出值的差值过小，那么可能梯度会过小，导致出现梯度小时的情况，残差网络的好处在于当残差为 0 时，改成神经元只是对前层进行一次线性堆叠，使得网络梯度不容易消失，性能不会下降。</p>
<h2 id="relu-的公式和优缺点，relu-在-0-的位置可导吗，不可导怎么处理">relu 的公式和优缺点，relu 在 0 的位置可导吗，不可导怎么处理</h2>
<p>$$<br>
relu(x) = max(x, 0)<br>
$$</p>
<p>优点：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快，求导方便</li>
<li>加速网络训练</li>
</ul>
<p>缺点包括：</p>
<ul>
<li>由于负数部分恒为 0，会导致一些神经元无法激活</li>
<li>输出不是以 0 为中心</li>
</ul>
<p>由于 relu 函数的左导数和右导数不相等，所以其不可导。针对这种类型的激活函数，可以使用次梯度来解决。</p>
<p>次梯度方法(subgradient method)是传统的梯度下降方法的拓展，用来处理不可导的凸函数。它的优势是比传统方法处理问题范围大，劣势是算法收敛速度慢。但是，由于它对不可导函数有很好的处理方法，所以学习它还是很有必要的。</p>
<p>$$<br>
c \le \frac{f(x) - f(x_0)}{x - x_0}<br>
$$</p>
<p>对于 relu 函数，当 x&gt;0 时，导数为 1，当 x&lt;0 时导数为 0。因此 relu 函数在 x=0 的次梯度 c ∈ [ 0 , 1 ]，c 可以取[0,1]之间的任意值。</p>
<h2 id="如何处理神经网络中的过拟合问题？">如何处理神经网络中的过拟合问题？</h2>
<ul>
<li>L1/L2 正则化</li>
<li>dropout</li>
<li>data argumentation</li>
<li>early stop</li>
</ul>
<h3 id="L1-L2-正则化的区别">L1/L2 正则化的区别</h3>
<p>在 L1 规范化中，权重通过一个常量向 0 进行缩小；在 L2 规范化中，权重通过一个和 w 成比例的量进行缩小<br>
当一个特定的权重绝对值|w|很大时，L1 规范化的权重缩小要比 L2 规范化小很多；当一个特定的权重绝对值|w|很小时，L1 规范化的权重缩小要比 L2 规范化大很多<br>
L1 规范化倾向于聚集网络的权重在相对少量的高重要度的连接上，而其他权重会被驱使向 0 接近<br>
在 w=0 处偏导数不存在，此时使用无规范化的随机梯度下降规则，因为规范化的效果是缩小权重，不能对一个已经是 0 的权重进行缩小</p>
<h2 id="神经网络都有哪些正则化操作？BN-和-LN-分别用在哪？">神经网络都有哪些正则化操作？BN 和 LN 分别用在哪？</h2>
<p>最常用的正则化技术是 dropout，随机的丢掉一些神经元。还有数据增强，早停，L1 正则化，L2 正则化等。</p>
<ul>
<li>Batch Normalization 是对这批样本的同一维度特征做归一化.</li>
<li>Layer Normalization 是对这单个样本的所有维度特征做归一化。</li>
</ul>
<p>BN 用在图像较多，LN 用在文本较多。</p>
<h2 id="Attention-和全连接的区别是啥？">Attention 和全连接的区别是啥？</h2>
<p>Attention 的最终输出可以看成是一个“在关注部分权重更大的全连接层”。但是它与全连接层的区别在于，注意力机制可以利用输入的特征信息来确定哪些部分更重要。</p>
<p>全连接的作用的是对一个实体进行从一个特征空间到另一个特征空间的映射，而注意力机制是要对来自同一个特征空间的多个实体进行整合。全连接的权重对应的是一个实体上的每个特征的重要性，而注意力机制的输出结果是各个实体的重要性。比如说，一个单词“love”在从 200 维的特征空间转换到 100 维的特征空间时，使用的是全连接，不需要注意力机制，因为特征空间每一维的意义是固定的。而如果我们面对的是词组“I love you”，需要对三个 200 维的实体特征进行整合，整合为一个 200 维的实体，此时就要考虑到实体间的位置可能发生变化，我们下次收到的句子可能是“love you I”，从而需要一个与位置无关的方案。</p>
<h2 id="BP-算法及推导">BP 算法及推导</h2>
<p><strong>BP 算法是“误差反向传播”的简称</strong>，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。</p>
<p>反向传播要求有<strong>对每个输入值期望得到的已知输出，来计算损失函数的梯度</strong>。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的 Delta 规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的<strong>激励函数可微</strong>。</p>
<h3 id="BP-算法">BP 算法</h3>
<p>BP 网络的结构降法的基础上。BP 网络的输入输出关系实质上是一种映射关系：一个 输入 m 输出的 BP 神经网络所完成的功能是从 一维欧氏空间向 m 维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是 BP 算法得以应用的基础。</p>
<p>反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。</p>
<p>BP 算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。</p>
<h3 id="推导">推导</h3>
<p>$$<br>
z^l = W^la^{l-1} + b^l \<br>
a^l = \sigma(z^l)<br>
$$</p>
<p>其中，$z^l$表示第$l$层$(l=1,2,…,L)$经过激活函数之前的输出，而$a^l$表示第 l 层经过激活函数之后的输出，$\sigma$表示激活函数。</p>
<p>定义误差函数:</p>
<p>$$<br>
C=\frac{1}{2}\left|a^{L}-y\right|_{2}^{2}<br>
$$</p>
<p>其中，$L$代表多层感知机总的层数， $a^L$表示多层感知机第$L$层经过激活函数后的输出，即神经网络所预测的输出值。而 y 是训练数据中对应输入 x 实际的输出值 y。</p>
<p>为了方便进一步的计算推导，以及避免重复计算，我们引入一个中间量$\delta^l$，我们称它为第 l 层的 delta 误差，表示误差函数对于神经网络第 l 层激活前输出值的偏导数，即$\delta^l = \frac{\partial C}{\partial z^l}$。</p>
<p>$$<br>
\delta^{L}=\frac{\partial C}{\partial z^{L}}=\frac{\partial C}{\partial a^{L}} \frac{\partial a^{L}}{\partial z^{L}}=\left(a^{L}-y\right) \odot \sigma^{\prime}\left(z^{L}\right)<br>
$$</p>
<p>求得了输出层的 delta 误差，误差函数 C 对于输出层参数的导数，即对权重矩阵以及偏置项的导数可通过输出层的 delta 误差求得如下，这里使用了求导的链式法则</p>
<p>$$<br>
\frac{\partial C}{\partial W^{L}}=\frac{\partial C}{\partial z^{L}} \frac{\partial z^{L}}{\partial W^{L}}=\delta^{L}\left(a^{L-1}\right)^{T} \<br>
\frac{\partial C}{\partial b^{L}}=\frac{\partial C}{\partial z^{L}} \frac{\partial z^{L}}{\partial b^{L}}=\delta^{L} \odot \mathbf{1}=\delta^{L}<br>
$$</p>
<p>我们可以很容易看到，一旦求出了当前层的 delta 误差，误差函数对当前层各参数的导数便可以相应的求出。</p>
<p>得到了最后一层的 delta 误差，我们接下来需要将 delta 误差逆向传播，即不断地根据后一层的 delta 误差求得前一层的 delta 误差，最终求得每一层的 delta 误差。其实在这里我们主要利用的是求导的链式法则。假设我们已经求得第 l+1 层的 delta 误差，我们可以将第 l 层的 delta 误差表示如下</p>
<p>$$<br>
\delta^{l}=\frac{\partial C}{\partial z^{l}}=\frac{\partial C}{\partial z^{l+1}} \frac{\partial z^{l+1}}{\partial z^{l}}=\delta^{l+1} \frac{\partial z^{l+1}}{\partial z^{l}}<br>
$$</p>
<p>因为</p>
<p>$$<br>
z^{l + 1} = W^{l + 1} a^l + b^{l + 1} = W^{l + 1}\sigma(z^l) + b^{l + 1}<br>
$$</p>
<p>所以</p>
<p>$$<br>
\delta^{l} = (W^{l + 1})^T\delta^{l + 1} \odot\sigma^{\prime}(z^l)<br>
$$</p>
<p>在求得每一层的 delta 误差后，我们可以很容易地求出误差函数 C 对于每一层参数的梯度：</p>
<p>$$<br>
\frac{\partial C}{\partial W^{l}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial W^{l}}=\delta^{l}\left(a^{l-1}\right)^{T} \<br>
\frac{\partial C}{\partial b^{l}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial b^{l}}=\delta^{l} 1=\delta^{l}<br>
$$</p>
<p>最后我们可以通过梯度下降法来对每一层的参数进行更新：</p>
<p>$$<br>
W^{l} =W^{l}-\eta \frac{\partial C}{\partial W^{l}} \<br>
b^{l} =b^{l}-\eta \frac{\partial C}{\partial b^{l}}<br>
$$</p>
<h2 id="梯度消失和梯度爆炸的问题是如何产生的？如何解决？">梯度消失和梯度爆炸的问题是如何产生的？如何解决？</h2>
<p>由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用 sigmoid, tanh 激活函数的话，由于导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 w 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。</p>
<p>因为 sigmoid 导数最大为 1/4，故只有当 abs(w)&gt;4 时才可能出现梯度爆炸，因此最普遍发生的是梯度消失问题。</p>
<p>解决方法通常包括</p>
<ul>
<li>使用 ReLU 等激活函数，梯度只会为 0 或者 1，每层的网络都可以得到相同的更新速度</li>
<li>采用 LSTM</li>
<li>进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内</li>
<li>使用正则化，这样会限制参数 w 的大小，从而防止梯度爆炸</li>
<li>设计网络层数更少的网络进行模型训练</li>
<li>batch normalization</li>
</ul>
<h2 id="语言模型中，Bert-为什么在-masked-language-model-中采用了-80-、10-、10-的策略？">语言模型中，Bert 为什么在 masked language model 中采用了 80%、10%、10%的策略？</h2>
<p>如果训练的时候 100%都是 Mask，那么在 fine-tune 的时候，所有的词时候已知的，不存在[Mask]，那么模型就只知道根据其他词的信息来预测当前词，而不会直接利用这个词本身的信息，会凭空损失一部分信息，对下游任务不利。</p>
<p>还有 10% random token 是因为如果都用原 token，模型在预训练时可能会偷懒，不去建模单词间的依赖关系，直接照抄当前词</p>
<p>[MASK] 是以一种显式的方式告诉模型『这个词我不告诉你，你自己从上下文里猜』，从而防止信息泄露。如果 [MASK] 以外的部分全部都用原 token，模型会学到『如果当前词是 [MASK]，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』。这样一来，在 finetune 阶段，所有词都是正常单词，模型就照抄所有词，不提取单词间的依赖关系了。</p>
<p>以一定的概率填入 random token，就是让模型时刻堤防着，在任意 token 的位置都需要把当前 token 的信息和上下文推断出的信息相结合。这样一来，在 finetune 阶段的正常句子上，模型也会同时提取这两方面的信息，因为它不知道它所看到的『正常单词』到底有没有被动过手脚的。</p>
<h2 id="Bert-现有的问题有哪些？">Bert 现有的问题有哪些？</h2>
<ul>
<li>Bert 模型过于庞大，参数太多，无论是 feature-based approach 还是 fine-tune approach 都很慢；而且因为表示是上下文相关的，上线的应用需要实时处理，时间消耗太大；</li>
<li>Bert 给出来的中文模型中，是以字为基本单位的，很多需要词向量的应用无法直接使用；同时该模型无法识别很多生僻词，都是 UNK；</li>
<li>Bert 模型作为自回归模型，由于模型结构的问题，无法给出句子概率值</li>
</ul>
<h2 id="非平衡数据集的处理方法有哪些？">非平衡数据集的处理方法有哪些？</h2>
<ul>
<li>采用更好的评价指标，例如 F1、AUC 曲线等，而不是 Recall、Precision</li>
<li>进行过采样，随机重复少类别的样本来增加它的数量；</li>
<li>进行欠采样，随机对多类别样本降采样</li>
<li>通过在已有数据上添加噪声来生成新的数据</li>
<li>修改损失函数，添加新的惩罚项，使得小样本的类别被判断错误的损失增大，迫使模型重视小样本的数据</li>
<li>使用组合/集成方法解决样本不均衡，在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果；</li>
</ul>
<h2 id="交叉熵损失与-KL-散度的区别">交叉熵损失与 KL 散度的区别</h2>
<p>KL 散度是相对熵(relative entropy)，用来衡量两个概率分布之间的差异，对于两个概率分布 $p(x), q(x)$ ,其中 $p(x)$ 是真实概率分布，而 $q(x)$ 是数据计算得到的概率分布，其相对熵的计算公式为:</p>
<p>$$<br>
K L(p | q)=-\int p(x) \ln q(x) d x-\left(\int p(x) \ln p(x) d x\right)<br>
$$</p>
<p>当且仅当$p(x) == q(x)$时，其值为 0；其前半部分$-\int p(x) \ln q(x) d x$为交叉熵的表达公式，或者说相对熵等于交叉熵减去数据真实分布的熵。<br>
由于真实的概率分布是固定的，因此公式中的后半部分是常数，所以优化交叉熵损失也等效于优化相对熵损失。</p>
<h2 id="熵、条件熵、互信息、相对熵">熵、条件熵、互信息、相对熵</h2>
<h3 id="熵">熵</h3>
<p>熵是一个随机变量不确定性的度量。对于一个离散型变量，定义为：</p>
<p>$$<br>
H(x) = -\sum_{x\in X}p(x)\log{p(x)}<br>
$$</p>
<p>一个随机性变量的熵越大，就表示不确定性越大，也就是说随机变量包含的信息量越大。<br>
熵只依赖于$X$的分布，与$X$的取值无关。</p>
<h3 id="条件熵">条件熵</h3>
<p>条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性，$H(Y|X)$ 定义为在给定条件 $X$ 下，$Y$ 的条件概率分布的熵对 $X$ 的数学期望：</p>
<p>$$<br>
H(Y\mid X) = \sum_{x\in X}p(x)H(Y\mid X=x)<br>
$$</p>
<h3 id="互信息">互信息</h3>
<p>互信息表示在得知 $Y$ 后，原来信息量减少了多少。</p>
<p>$$<br>
I(X;Y) = \sum_{x\in X}\sum_{y\in Y}p(x, y)\log{\frac{p(x, y)}{p(x)p(y)}} \<br>
I(X;Y) = H(X) - H(X\mid Y) = H(Y) - H(Y\mid X)<br>
$$</p>
<p>如果$X$与$Y$相互独立，则互信息为 0。</p>
<h3 id="相对熵">相对熵</h3>
<p>KL 散度（Kullback-Leibler divergence）和相对熵是等价的，KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度越小表示两个分布越接近。也就是说 KL 散度是不对称的，且 KL 散度的值是非负数。</p>
<p>$$<br>
D_{KL}(P//Q) = -\int{p(x)\log{q(x)}dx} - (- \int{p(x)\log{q(x)}dx}) = -\int{p(x)\log[\frac{q(x)}{p(x)}]dx}<br>
\<br>
D_{KL}(P//Q) = - \sum_{x\in X}p(x)log(q(x)) - (-\sum_{x\in X}p(x)log(p(x)))<br>
$$</p>
<p>显然$D_{KL}(P//Q)$不等于$D_{KL}(Q//P)$，即 KL 散度不是一个对称量。</p>
<p>JS 散度是基于 KL 散度的变种，度量了两个概率分布的相似度，解决了 KL 散度的非对称问题。如果两个分配 P,Q 离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0。梯度消失了。</p>
<p>$$<br>
J S\left(P_{1} | P_{2}\right)=\frac{1}{2} K L\left(P_{1} | \frac{P_{1}+P_{2}}{2}\right)+\frac{1}{2} K L\left(P_{2} | \frac{P_{1}+P_{2}}{2}\right)<br>
$$</p>
<h2 id="机器学习泛化能力评测指标">机器学习泛化能力评测指标</h2>
<p>泛化能力是模型对未知数据的预测能力。</p>
<h3 id="分类问题">分类问题</h3>
<ul>
<li>
<p>准确率：分类正确的样本占总样本的比例<br>
准确率的缺陷：当正负样本不平衡比例时，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。</p>
</li>
<li>
<p>召回率：分类正确的正样本个数占实际的正样本个数的比例。</p>
</li>
<li>
<p>F1 score：是精确率和召回率的调和平均数，综合反应模型分类的性能。<br>
Precision 值和 Recall 值是既矛盾又统一的两个指标，为了提高 Precision 值，分类器需要尽量在“更有把握”时才把样本预测为正样本，但此时往往会因为过于保 守而漏掉很多“没有把握”的正样本，导致 Recall 值降低。</p>
<p>ROC 曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性 率（True Positive Rate，TPR）。FPR 和 TPR 的计算方法分别为</p>
</li>
<li>
<p>精确度（precision）/查准率：TP/（TP+FP）=TP/P 预测为真中，实际为正样本的概率 s</p>
</li>
<li>
<p>召回率（recall）/查全率：TP/（TP+FN） 正样本中，被识别为真的概率</p>
</li>
<li>
<p>假阳率（False positive rate）：FPR = FP/(FP+TN) 负样本中，被识别为真的概率</p>
</li>
<li>
<p>真阳率（True positive rate）：TPR = TP/（TP+FN） 正样本中，能被识别为真的概率</p>
</li>
<li>
<p>准确率（accuracy）：ACC =（TP+TN）/(P+N) 所有样本中，能被正确识别的概率</p>
<p>上式中，P 是真实的正样本的数量，N 是真实的负样本的数量，TP 是 P 个正样本中被分类器预测为正样本的个数，FP 是 N 个负样本中被分类器预测为正样本的个数。</p>
<p>AUC：AUC 是 ROC 曲线下面的面积，AUC 可以解读为从所有正例中随机选取一个样本 A，再从所有负例中随机选取一个样本 B，分类器将 A 判为正例的概率比将 B 判为正例的概率大的可能性。AUC 反映的是分类器对样本的排序能力。AUC 越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</p>
</li>
<li>
<p>F1-score：在多分类问题中，如果要计算模型的 F1-score，则有两种计算方式，分别为<code>微观micro-F1</code>和<code>宏观macro-F1</code>，这两种计算方式在二分类中与 F1-score 的计算方式一样，所以在二分类问题中，计算 micro-F1=macro-F1=F1-score，micro-F1 和 macro-F1 都是多分类 F1-score 的两种计算方式。</p>
<p>micro-F1：计算方法：先计算所有类别的总的 Precision 和 Recall，然后计算出来的 F1 值即为 micro-F1；</p>
<p>使用场景：在计算公式中考虑到了每个类别的数量，所以适用于数据分布不平衡的情况；但同时因为考虑到数据的数量，所以在数据极度不平衡的情况下，数量较多数量的类会较大的影响到 F1 的值；</p>
<p>marco-F1：计算方法：将所有类别的 Precision 和 Recall 求平均，然后计算 F1 值作为 macro-F1；</p>
<p>使用场景：没有考虑到数据的数量，所以会平等的看待每一类（因为每一类的 precision 和 recall 都在 0-1 之间），会相对受高 precision 和高 recall 类的影响较大。</p>
</li>
</ul>
<h3 id="回归问题">回归问题</h3>
<ul>
<li>
<p>RMSE(Root Mean Square Error, 均方根误差)：</p>
<p>$$<br>
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i} - y_i)^2}<br>
$$</p>
<p>RMSE 经常被用来衡量回归模型的好坏。RMSE 能够很好地反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的离群点（Outlier）时，即使离群点 数量非常少，也会让 RMSE 指标变得很差。</p>
</li>
<li>
<p>MAPE(Mean Absolute Percentage Error)：</p>
<p>$$<br>
M A P E=\frac{100 %}{n} \sum_{i=1}^{n}\left|\frac{\hat{y}<em>{i}-y</em>{i}}{y_{i}}\right|<br>
$$</p>
<p>引入别的评价指标，MAPE，平均绝对百分比误差。相比 RMSE，MAPE 相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。</p>
</li>
</ul>
<h2 id="bagging、boosting、stacking-的异同">bagging、boosting、stacking 的异同</h2>
<h3 id="Bagging-算法-套袋发">Bagging 算法(套袋发)</h3>
<p>bagging 的算法过程如下：</p>
<p>从原始样本集中使用 Bootstraping 方法随机抽取 n 个训练样本，共进行 k 轮抽取，得到 k 个训练集（k 个训练集之间相互独立，元素可以有重复）。</p>
<p>对于 n 个训练集，我们训练 k 个模型，（这个模型可根据具体的情况而定，可以是决策树，knn 等）</p>
<p>对于分类问题：由投票表决产生的分类结果；对于回归问题，由 k 个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。</p>
<h3 id="Boosting（提升法）">Boosting（提升法）</h3>
<p>boosting 的算法过程如下：</p>
<p>对于训练集中的每个样本建立权值 wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。</p>
<p>同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost 给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。)</p>
<h3 id="Bagging-和-Boosting-的主要区别">Bagging 和 Boosting 的主要区别</h3>
<p>样本选择上: Bagging 采取 Bootstraping 的是随机有放回的取样，Boosting 的每一轮训练的样本是固定的，改变的是买个样的权重。</p>
<p>样本权重上：Bagging 采取的是均匀取样，且每个样本的权重相同，Boosting 根据错误率调整样本权重，错误率越大的样本权重会变大</p>
<p>预测函数上：Bagging 所以的预测函数权值相同，Boosting 中误差越小的预测函数其权值越大。</p>
<p>并行计算: Bagging 的各个预测函数可以并行生成;Boosting 的各个预测函数必须按照顺序迭代生成。</p>
<h2 id="Focal-loss">Focal loss</h2>
<p>针对样本不均衡问题提的出损失函数。</p>
<p>二分类交叉熵损失函数：</p>
<p>$$<br>
CE = \left{\begin{array}{rlr}<br>
-\log §, &amp; \text { if } &amp; y=1 \<br>
-\log (1-p), &amp; \text { if } &amp; y=0<br>
\end{array}\right.<br>
$$</p>
<p>为了解决正负样本不平衡的问题，在交叉熵损失的前面加上一个参数$\alpha$：</p>
<p>$$<br>
CE = \left{\begin{array}{rlr}<br>
-\alpha\log §, &amp; \text { if } &amp; y=1 \<br>
-(1-\alpha)\log (1-p), &amp; \text { if } &amp; y=0<br>
\end{array}\right.<br>
$$</p>
<p>尽管$\alpha$平衡了正负样本，但对难易样本的不平衡没有任何帮助。而实际上，目标检测中大量的候选目标都是像下图一样的易分样本。这些样本的损失很低，但是由于数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是 GHM 的主要改进对象）</p>
<p>Focal Loss 的思想：把高置信度§样本的损失再降低一些。</p>
<p>$$<br>
FL = \left{\begin{array}{rll}<br>
-\alpha(1-p)^{\gamma} \log §, &amp; \text { if } &amp; y=1 \<br>
-(1-\alpha) p^{\gamma} \log (1-p), &amp; \text { if } &amp; y=0<br>
\end{array}\right.<br>
$$</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://example.com">YUAN Tingyi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/">http://example.com/2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/9.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/06/01/5c4327cec0e84a29a399ea713e35e1e2/"><img class="prev-cover" src="/img/5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">MobileNet v1/v2</div></div></a></div><div class="next-post pull-right"><a href="/2022/06/01/9de2a5517a4448639d901c311628b8d2/"><img class="next-cover" src="/img/9.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">交叉熵（Cross Entropy）</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">YUAN Tingyi</div><div class="author-info__description">XianrenYty</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XianrenYty"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BA%8C%E5%88%86%E7%B1%BB%E4%B8%8D%E7%94%A8-MSE-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">为什么二分类不用 MSE 损失函数？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E8%A7%92%E5%BA%A6"><span class="toc-number">1.1.</span> <span class="toc-text">数据分布角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E8%A7%92%E5%BA%A6"><span class="toc-number">1.2.</span> <span class="toc-text">优化角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%90%8E%E8%BF%98%E4%BC%9A%E6%9C%89%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">用交叉熵损失函数后还会有梯度消失的问题吗？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">LSTM 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-%E4%B8%8E-GRU-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.1.</span> <span class="toc-text">LSTM 与 GRU 的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">Transformer 的原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%EF%BC%8CQKV-%E6%80%8E%E4%B9%88%E5%BE%97%E5%88%B0"><span class="toc-number">3.1.</span> <span class="toc-text">Transformer 的计算公式，QKV 怎么得到</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">3.2.</span> <span class="toc-text">multi-head 的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#residual-%E8%BF%9E%E6%8E%A5"><span class="toc-number">3.3.</span> <span class="toc-text">residual 连接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#relu-%E7%9A%84%E5%85%AC%E5%BC%8F%E5%92%8C%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%8Crelu-%E5%9C%A8-0-%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%8F%AF%E5%AF%BC%E5%90%97%EF%BC%8C%E4%B8%8D%E5%8F%AF%E5%AF%BC%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86"><span class="toc-number">4.</span> <span class="toc-text">relu 的公式和优缺点，relu 在 0 的位置可导吗，不可导怎么处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">如何处理神经网络中的过拟合问题？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1-L2-%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">5.1.</span> <span class="toc-text">L1&#x2F;L2 正则化的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%AD%A3%E5%88%99%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%9FBN-%E5%92%8C-LN-%E5%88%86%E5%88%AB%E7%94%A8%E5%9C%A8%E5%93%AA%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">神经网络都有哪些正则化操作？BN 和 LN 分别用在哪？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention-%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E5%95%A5%EF%BC%9F"><span class="toc-number">7.</span> <span class="toc-text">Attention 和全连接的区别是啥？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP-%E7%AE%97%E6%B3%95%E5%8F%8A%E6%8E%A8%E5%AF%BC"><span class="toc-number">8.</span> <span class="toc-text">BP 算法及推导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BP-%E7%AE%97%E6%B3%95"><span class="toc-number">8.1.</span> <span class="toc-text">BP 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC"><span class="toc-number">8.2.</span> <span class="toc-text">推导</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E9%97%AE%E9%A2%98%E6%98%AF%E5%A6%82%E4%BD%95%E4%BA%A7%E7%94%9F%E7%9A%84%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">9.</span> <span class="toc-text">梯度消失和梯度爆炸的问题是如何产生的？如何解决？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%EF%BC%8CBert-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8-masked-language-model-%E4%B8%AD%E9%87%87%E7%94%A8%E4%BA%86-80-%E3%80%8110-%E3%80%8110-%E7%9A%84%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="toc-number">10.</span> <span class="toc-text">语言模型中，Bert 为什么在 masked language model 中采用了 80%、10%、10%的策略？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bert-%E7%8E%B0%E6%9C%89%E7%9A%84%E9%97%AE%E9%A2%98%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">11.</span> <span class="toc-text">Bert 现有的问题有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">12.</span> <span class="toc-text">非平衡数据集的处理方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E4%B8%8E-KL-%E6%95%A3%E5%BA%A6%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">13.</span> <span class="toc-text">交叉熵损失与 KL 散度的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%86%B5%E3%80%81%E6%9D%A1%E4%BB%B6%E7%86%B5%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF%E3%80%81%E7%9B%B8%E5%AF%B9%E7%86%B5"><span class="toc-number">14.</span> <span class="toc-text">熵、条件熵、互信息、相对熵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%86%B5"><span class="toc-number">14.1.</span> <span class="toc-text">熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E7%86%B5"><span class="toc-number">14.2.</span> <span class="toc-text">条件熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%92%E4%BF%A1%E6%81%AF"><span class="toc-number">14.3.</span> <span class="toc-text">互信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E7%86%B5"><span class="toc-number">14.4.</span> <span class="toc-text">相对熵</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87"><span class="toc-number">15.</span> <span class="toc-text">机器学习泛化能力评测指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">15.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">15.2.</span> <span class="toc-text">回归问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bagging%E3%80%81boosting%E3%80%81stacking-%E7%9A%84%E5%BC%82%E5%90%8C"><span class="toc-number">16.</span> <span class="toc-text">bagging、boosting、stacking 的异同</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging-%E7%AE%97%E6%B3%95-%E5%A5%97%E8%A2%8B%E5%8F%91"><span class="toc-number">16.1.</span> <span class="toc-text">Bagging 算法(套袋发)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Boosting%EF%BC%88%E6%8F%90%E5%8D%87%E6%B3%95%EF%BC%89"><span class="toc-number">16.2.</span> <span class="toc-text">Boosting（提升法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging-%E5%92%8C-Boosting-%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB"><span class="toc-number">16.3.</span> <span class="toc-text">Bagging 和 Boosting 的主要区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Focal-loss"><span class="toc-number">17.</span> <span class="toc-text">Focal loss</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/fd61228310d24c999c625f49b503f7ce/" title="train_test_split 参数详解"><img src="/img/8.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="train_test_split 参数详解"/></a><div class="content"><a class="title" href="/2022/06/01/fd61228310d24c999c625f49b503f7ce/" title="train_test_split 参数详解">train_test_split 参数详解</a><time datetime="2022-06-01T12:52:00.000Z" title="Created 2022-06-01 20:52:00">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/224bdd4444be4fe0b7826959bc6b40a4/" title="StandardScaler(sklearn)参数详解"><img src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="StandardScaler(sklearn)参数详解"/></a><div class="content"><a class="title" href="/2022/06/01/224bdd4444be4fe0b7826959bc6b40a4/" title="StandardScaler(sklearn)参数详解">StandardScaler(sklearn)参数详解</a><time datetime="2022-06-01T12:51:39.966Z" title="Created 2022-06-01 20:51:39">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/2fd9e712ce6b4a49a24775586e47e44b/" title="八大排序算法(Python实现)"><img src="/img/6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="八大排序算法(Python实现)"/></a><div class="content"><a class="title" href="/2022/06/01/2fd9e712ce6b4a49a24775586e47e44b/" title="八大排序算法(Python实现)">八大排序算法(Python实现)</a><time datetime="2022-06-01T12:51:26.086Z" title="Created 2022-06-01 20:51:26">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/9d8547900901416593ba964e6757c6c3/" title="ResNet"><img src="/img/2.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ResNet"/></a><div class="content"><a class="title" href="/2022/06/01/9d8547900901416593ba964e6757c6c3/" title="ResNet">ResNet</a><time datetime="2022-06-01T12:50:56.055Z" title="Created 2022-06-01 20:50:56">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/b6a6cbce3d7f45a8b2511edf86fb5a24/" title="Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）"><img src="/img/8.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）"/></a><div class="content"><a class="title" href="/2022/06/01/b6a6cbce3d7f45a8b2511edf86fb5a24/" title="Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）">Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）</a><time datetime="2022-06-01T12:48:23.363Z" title="Created 2022-06-01 20:48:23">2022-06-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By YUAN Tingyi</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://example.com/2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/'
    this.page.identifier = '2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/'
    this.page.title = '深度学习知识点汇总'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Valine' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>