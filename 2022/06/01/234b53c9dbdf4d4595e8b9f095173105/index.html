<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>paddle2.0高层API实现自定义数据集文本分类中的情感分析任务 | Personnal Blog of YUAN Tingyi</title><meta name="author" content="YUAN Tingyi"><meta name="copyright" content="YUAN Tingyi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[toc]  本文包含了：  12345- 自定义文本分类数据集继承- 文本分类数据处理- 循环神经网络RNN, LSTM- ·seq2vec·- pretrained预训练模型 『深度学习 7 日打卡营·day4』 零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。  课程地址  传送门：https:&#x2F;&#x2F;aistudio.baidu.com&#x2F;aist">
<meta property="og:type" content="article">
<meta property="og:title" content="paddle2.0高层API实现自定义数据集文本分类中的情感分析任务">
<meta property="og:url" content="http://example.com/2022/06/01/234b53c9dbdf4d4595e8b9f095173105/index.html">
<meta property="og:site_name" content="Personnal Blog of YUAN Tingyi">
<meta property="og:description" content="[toc]  本文包含了：  12345- 自定义文本分类数据集继承- 文本分类数据处理- 循环神经网络RNN, LSTM- ·seq2vec·- pretrained预训练模型 『深度学习 7 日打卡营·day4』 零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。  课程地址  传送门：https:&#x2F;&#x2F;aistudio.baidu.com&#x2F;aist">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/6.jpg">
<meta property="article:published_time" content="2022-06-01T12:47:34.641Z">
<meta property="article:modified_time" content="2022-06-01T12:47:42.075Z">
<meta property="article:author" content="YUAN Tingyi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/6.jpg"><link rel="shortcut icon" href="/img/favicon-32x32.png"><link rel="canonical" href="http://example.com/2022/06/01/234b53c9dbdf4d4595e8b9f095173105/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'paddle2.0高层API实现自定义数据集文本分类中的情感分析任务',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-01 20:47:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/6.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Personnal Blog of YUAN Tingyi</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">paddle2.0高层API实现自定义数据集文本分类中的情感分析任务</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-06-01T12:47:34.641Z" title="Created 2022-06-01 20:47:34">2022-06-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-06-01T12:47:42.075Z" title="Updated 2022-06-01 20:47:42">2022-06-01</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="paddle2.0高层API实现自定义数据集文本分类中的情感分析任务"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[toc]</p>
<blockquote>
<p>本文包含了：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- 自定义文本分类数据集继承</span><br><span class="line">- 文本分类数据处理</span><br><span class="line">- 循环神经网络RNN, LSTM</span><br><span class="line">- ·seq2vec·</span><br><span class="line">- pretrained预训练模型</span><br></pre></td></tr></table></figure>
<p>『深度学习 7 日打卡营·day4』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
<li></li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>情感分析是自然语言处理领域一个老生常谈的任务。句子情感分析目的是为了判别说者的情感倾向，比如在某些话题上给出的的态度明确的观点，或者反映的情绪状态等。情感分析有着广泛应用，比如电商评论分析、舆情分析等。</p>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/febb8a1478e34258953e56611ddc76cd20b412fec89845b0a4a2e6b9f8aae774" hspace='10'/> <br />
</p>
<h2 id="环境介绍">环境介绍</h2>
<ul>
<li>
<p>PaddlePaddle 框架，AI Studio 平台已经默认安装最新版 2.0。</p>
</li>
<li>
<p>PaddleNLP，深度兼容框架 2.0，是飞桨框架 2.0 在 NLP 领域的最佳实践。</p>
</li>
</ul>
<p>这里使用的是 beta 版本，马上也会发布 rc 版哦。AI Studio 平台后续会默认安装 PaddleNLP，在此之前可使用如下命令安装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载paddlenlp</span></span><br><span class="line">!pip install --upgrade paddlenlp==<span class="number">2.0</span><span class="number">.0</span>b4 -i https://pypi.org/simple</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Requirement already up-to-date: paddlenlp==2.0.0b4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.0b4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (4.1.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.4.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.42.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.9.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (1.2.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: numpy&gt;=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py-&gt;paddlenlp==2.0.0b4) (1.16.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py-&gt;paddlenlp==2.0.0b4) (1.15.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: protobuf&gt;=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (3.14.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Flask-Babel&gt;=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (1.0.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Pillow&gt;=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (7.1.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: flake8&gt;=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (3.8.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (1.21.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (0.7.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: flask&gt;=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (1.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (2.22.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (0.8.53)</span><br><span class="line">Requirement already satisfied, skipping upgrade: scikit-learn&gt;=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval-&gt;paddlenlp==2.0.0b4) (0.22.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2019.3)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Babel&gt;=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.8.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Jinja2&gt;=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.10.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: importlib-metadata; python_version &lt; &quot;3.8&quot; in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.23)</span><br><span class="line">Requirement already satisfied, skipping upgrade: mccabe&lt;0.7.0,&gt;=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.6.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pycodestyle&lt;2.7.0,&gt;=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.6.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pyflakes&lt;2.3.0,&gt;=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.2.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.10.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (5.1.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: nodeenv&gt;=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.3.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: virtualenv&gt;=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (16.7.9)</span><br><span class="line">Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.3.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: identify&gt;=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.4.10)</span><br><span class="line">Requirement already satisfied, skipping upgrade: cfgv&gt;=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.0.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Werkzeug&gt;=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask&gt;=1.1.1-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.16.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: itsdangerous&gt;=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask&gt;=1.1.1-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.1.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: click&gt;=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask&gt;=1.1.1-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (7.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.8)</span><br><span class="line">Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.25.6)</span><br><span class="line">Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2019.9.11)</span><br><span class="line">Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (3.0.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: future&gt;=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.18.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pycryptodome&gt;=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (3.9.9)</span><br><span class="line">Requirement already satisfied, skipping upgrade: joblib&gt;=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;paddlenlp==2.0.0b4) (0.14.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: scipy&gt;=0.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;paddlenlp==2.0.0b4) (1.3.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2&gt;=2.5-&gt;Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: zipp&gt;=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version &lt; &quot;3.8&quot;-&gt;flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.6.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp&gt;=0.5-&gt;importlib-metadata; python_version &lt; &quot;3.8&quot;-&gt;flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (7.2.0)</span><br></pre></td></tr></table></figure>
<p>查看安装的版本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddlenlp</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(paddle.__version__, paddlenlp.__version__)</span><br><span class="line"></span><br><span class="line">paddle.set_device(<span class="string">&#x27;gpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2.0.0 2.0.0b4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CUDAPlace(0)</span><br></pre></td></tr></table></figure>
<h2 id="PaddleNLP-和-Paddle-框架是什么关系？">PaddleNLP 和 Paddle 框架是什么关系？</h2>
<p><img src="" alt=""></p>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/165924e86d9f4b5fa5d6fdee9e8496bf01be524e61f341b3879aceba48ae80fb" width = "300" height = "250"  hspace='10'/> <br />
</p><br></br>
<ul>
<li>Paddle 框架是基础底座，提供深度学习任务全流程 API。PaddleNLP 基于 Paddle 框架开发，适用于 NLP 任务。</li>
</ul>
<p>PaddleNLP 中数据处理、数据集、组网单元等 API 未来会沉淀到框架<code>paddle.text</code>中。</p>
<ul>
<li>代码中继承</li>
</ul>
<p><code>class TSVDataset(paddle.io.Dataset)</code></p>
<h2 id="使用飞桨完成深度学习任务的通用流程">使用飞桨完成深度学习任务的通用流程</h2>
<ul>
<li>
<p>数据集和数据处理<br>
<code>paddle.io.Dataset </code><br>
<code>paddle.io.DataLoader</code><br>
<code>paddlenlp.data </code></p>
</li>
<li>
<p>组网和网络配置</p>
<p><code>paddle.nn.Embedding</code><br>
<code>paddlenlp.seq2vec</code><br>
<code>paddle.nn.Linear</code><br>
<code>paddle.tanh</code></p>
<p><code>paddle.nn.CrossEntropyLoss</code><br>
<code>paddle.metric.Accuracy</code><br>
<code>paddle.optimizer</code></p>
<p><code>model.prepare</code></p>
</li>
<li>
<p>网络训练和评估</p>
<p><code>model.fit</code><br>
<code>model.evaluate</code></p>
</li>
<li>
<p>预测</p>
<p><code>model.predict</code></p>
</li>
</ul>
<p>注意：建议在 GPU 下运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> paddlenlp <span class="keyword">as</span> ppnlp</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Pad, Stack, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">from</span> paddlenlp.datasets <span class="keyword">import</span> MapDatasetWrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_vocab, convert_example</span><br></pre></td></tr></table></figure>
<h1>数据集和数据处理</h1>
<h2 id="自定义数据集">自定义数据集</h2>
<p>映射式(map-style)数据集需要继承<code>paddle.io.Dataset</code></p>
<ul>
<li>
<p><code>__getitem__</code>: 根据给定索引获取数据集中指定样本，在 paddle.io.DataLoader 中需要使用此函数通过下标获取样本。</p>
</li>
<li>
<p><code>__len__</code>: 返回数据集样本个数， paddle.io.BatchSampler 中需要样本个数生成下标序列。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfDefinedDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="built_in">super</span>(SelfDefinedDataset, self).__init__()</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_labels</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">txt_to_list</span>(<span class="params">file_name</span>):</span><br><span class="line">    res_list = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(file_name):</span><br><span class="line">        res_list.append(line.strip().split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> res_list</span><br><span class="line"></span><br><span class="line">trainlst = txt_to_list(<span class="string">&#x27;train.txt&#x27;</span>)</span><br><span class="line">devlst = txt_to_list(<span class="string">&#x27;dev.txt&#x27;</span>)</span><br><span class="line">testlst = txt_to_list(<span class="string">&#x27;test.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过get_datasets()函数，将list数据转换为dataset。</span></span><br><span class="line"><span class="comment"># get_datasets()可接收[list]参数，或[str]参数，根据自定义数据集的写法自由选择。</span></span><br><span class="line"><span class="comment"># train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets([&#x27;train&#x27;, &#x27;dev&#x27;, &#x27;test&#x27;])</span></span><br><span class="line">train_ds, dev_ds, test_ds = SelfDefinedDataset.get_datasets([trainlst, devlst, testlst])</span><br></pre></td></tr></table></figure>
<h2 id="训练数据查看">训练数据查看</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label_list = train_ds.get_labels()</span><br><span class="line"><span class="built_in">print</span>(label_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(train_ds[i])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;0&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;赢在心理，输在出品！杨枝太酸，三文鱼熟了，酥皮焗杏汁杂果可以换个名（九唔搭八）&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;服务一般，客人多，服务员少，但食品很不错&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;東坡肉竟然有好多毛，問佢地點解，佢地仲話係咁架\ue107\ue107\ue107\ue107\ue107\ue107\ue107冇天理，第一次食東坡肉有毛，波羅包就幾好食&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;父亲节去的，人很多，口味还可以上菜快！但是结账的时候，算错了没有打折，我也忘记拿清单了。说好打8折的，收银员没有打，人太多一时自己也没有想起。不知道收银员忘记，还是故意那钱露入自己钱包。。&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;吃野味，吃个新鲜，你当然一定要来广州吃鹿肉啦*价格便宜，量好足，&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;味道几好服务都五错推荐鹅肝乳鸽飞鱼&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;作为老字号，水准保持算是不错，龟岗分店可能是位置问题，人不算多，基本不用等位，自从抢了券，去过好几次了，每次都可以打85以上的评分，算是可以了～粉丝煲每次必点，哈哈，鱼也不错，还会来帮衬的，楼下还可以免费停车！&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;边到正宗啊？味味都咸死人啦，粤菜讲求鲜甜，五知点解感多人话好吃。&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;环境卫生差，出品垃圾，冇下次，不知所为&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;和苑真是精致粤菜第一家，服务菜品都一流&#x27;, &#x27;1&#x27;]</span><br></pre></td></tr></table></figure>
<h2 id="数据处理">数据处理</h2>
<p>为了将原始数据处理成模型可以读入的格式，本项目将对数据作以下处理：</p>
<ul>
<li>首先使用<code>jieba</code>切词，之后将<code>jieba</code>切完后的单词映射词表中单词 id。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/fb492aa32947698a198429ec2cb8e907.png" alt=""></p>
<ul>
<li>使用<code>paddle.io.DataLoader</code>接口多线程异步加载数据。</li>
</ul>
<p>其中用到了 PaddleNLP 中关于数据处理的 API。PaddleNLP 提供了许多关于 NLP 任务中构建有效的数据 pipeline 的常用 API</p>
<table>
<thead>
<tr>
<th>API</th>
<th style="text-align:left">简介</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>paddlenlp.data.Stack</code></td>
<td style="text-align:left">堆叠 N 个具有相同 shape 的输入数据来构建一个 batch，它的输入必须具有相同的 shape，输出便是这些输入的堆叠组成的 batch 数据。</td>
</tr>
<tr>
<td><code>paddlenlp.data.Pad</code></td>
<td style="text-align:left">堆叠 N 个输入数据来构建一个 batch，每个输入数据将会被 padding 到 N 个输入数据中最大的长度</td>
</tr>
<tr>
<td><code>paddlenlp.data.Tuple</code></td>
<td style="text-align:left">将多个组 batch 的函数包装在一起</td>
</tr>
</tbody>
</table>
<p>更多数据处理操作详见： <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md">https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载词汇表文件word_dict.txt，用于构造词-id映射关系。</span></span><br><span class="line"><span class="comment"># !wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载词表</span></span><br><span class="line">vocab = load_vocab(<span class="string">&#x27;./senta_word_dict.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印填补单词及对应向量</span></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    <span class="built_in">print</span>(k, v)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[PAD] 0</span><br></pre></td></tr></table></figure>
<h2 id="构造-dataloder">构造 dataloder</h2>
<p>下面的<code>create_data_loader</code>函数用于创建运行和预测时所需要的<code>DataLoader</code>对象。</p>
<ul>
<li>
<p><code>paddle.io.DataLoader</code>返回一个迭代器，该迭代器根据<code>batch_sampler</code>指定的顺序迭代返回 dataset 数据。异步加载数据。</p>
</li>
<li>
<p><code>batch_sampler</code>：DataLoader 通过 batch_sampler 产生的 mini-batch 索引列表来 dataset 中索引样本并组成 mini-batch</p>
</li>
<li>
<p><code>collate_fn</code>：指定如何将样本列表组合为 mini-batch 数据。传给它参数需要是一个 callable 对象，需要实现对组建的 batch 的处理逻辑，并返回每个 batch 的数据。在这里传入的是<code>prepare_input</code>函数，对产生的数据进行 pad 操作，并返回实际长度等。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reads data and generates mini-batches.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">dataset,</span></span><br><span class="line"><span class="params">                      trans_function=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                      mode=<span class="string">&#x27;train&#x27;</span>,</span></span><br><span class="line"><span class="params">                      batch_size=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                      pad_token_id=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                      batchify_fn=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> trans_function:</span><br><span class="line">        dataset = dataset.apply(trans_function, lazy=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return_list 数据是否以list形式返回</span></span><br><span class="line">    <span class="comment"># collate_fn  指定如何将样本列表组合为mini-batch数据。传给它参数需要是一个callable对象，需要实现对组建的batch的处理逻辑，并返回每个batch的数据。在这里传入的是`prepare_input`函数，对产生的数据进行pad操作，并返回实际长度等。</span></span><br><span class="line">    dataloader = paddle.io.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        return_list=<span class="literal">True</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        collate_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="comment"># python中的偏函数partial，把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。</span></span><br><span class="line">trans_function = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    vocab=vocab,</span><br><span class="line">    unk_token_id=vocab.get(<span class="string">&#x27;[UNK]&#x27;</span>, <span class="number">1</span>),</span><br><span class="line">    is_test=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的数据batch化处理，便于模型batch化运算。</span></span><br><span class="line"><span class="comment"># batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。</span></span><br><span class="line"><span class="comment"># 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.</span></span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=vocab[<span class="string">&#x27;[PAD]&#x27;</span>]),  <span class="comment"># input_ids</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>),  <span class="comment"># seq len</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loader = create_dataloader(</span><br><span class="line">    train_ds,</span><br><span class="line">    trans_function=trans_function,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">dev_loader = create_dataloader(</span><br><span class="line">    dev_ds,</span><br><span class="line">    trans_function=trans_function,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">test_loader = create_dataloader(</span><br><span class="line">    test_ds,</span><br><span class="line">    trans_function=trans_function,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br></pre></td></tr></table></figure>
<h1>模型搭建</h1>
<p>使用<code>LSTMencoder</code>搭建一个 BiLSTM 模型用于进行句子建模，得到句子的向量表示。</p>
<p>然后接一个线性变换层，完成二分类任务。</p>
<ul>
<li><code>paddle.nn.Embedding</code>组建 word-embedding 层</li>
<li><code>ppnlp.seq2vec.LSTMEncoder</code>组建句子建模层</li>
<li><code>paddle.nn.Linear</code>构造二分类器</li>
</ul>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/ecf309c20e5347399c55f1e067821daa088842fa46ad49be90de4933753cd3cf" width = "800" height = "450"  hspace='10'/> <br />
</p><br><center>图1：seq2vec示意图</center></br>
<ul>
<li>除 LSTM 外，<code>seq2vec</code>还提供了许多语义表征方法，详细可参考：<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1283423">seq2vec 介绍</a></li>
</ul>
<!---->
<ul>
<li><code>LSTMEncoder</code>参数：</li>
</ul>
<!---->
<ul>
<li><code>input_size</code>: int，必选。输入特征 Tensor 的最后一维维度。</li>
<li><code>hidden_size</code>: int，必选。lstm 运算的 hidden size。</li>
<li><code>num_layers</code>:int，可选，lstm 层数，默认为 1。</li>
<li><code>direction</code>: str，可选，lstm 运算方向，可选 forward， bidirectional。默认 forward。</li>
<li><code>dropout</code>: float，可选，dropout 概率值。如果设置非 0，则将对每一层 lstm 输出做 dropout 操作。默认为 0.0。</li>
<li><code>pooling_type</code>: str， 可选，默认为 None。可选 sum，max，mean。如<code>pooling_type=None</code>， 则将最后一层 lstm 的最后一个 step hidden 输出作为文本语义表征; 如<code>pooling_type!=None</code>， 则将最后一层 lstm 的所有 step 的 hidden 输出做指定 pooling 操作，其结果作为文本语义表征。</li>
</ul>
<p>更多<code>seq2vec</code>信息参考：<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/paddlenlp/seq2vec/encoder.py">https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/paddlenlp/seq2vec/encoder.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 num_classes,</span></span><br><span class="line"><span class="params">                 emb_dim=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 padding_idx=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 lstm_hidden_size=<span class="number">198</span>,</span></span><br><span class="line"><span class="params">                 direction=<span class="string">&#x27;forward&#x27;</span>,</span></span><br><span class="line"><span class="params">                 lstm_layers=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 pooling_type=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 fc_hidden_size=<span class="number">96</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 首先将输入word id 查表后映射成 word embedding</span></span><br><span class="line">        self.embedder = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size,</span><br><span class="line">            embedding_dim=emb_dim,</span><br><span class="line">            padding_idx=padding_idx)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将word embedding经过LSTMEncoder变换到文本语义表征空间中</span></span><br><span class="line">        self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder(</span><br><span class="line">            emb_dim,</span><br><span class="line">            lstm_hidden_size,</span><br><span class="line">            num_layers=lstm_layers,</span><br><span class="line">            direction=direction,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            pooling_type=pooling_type)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size</span></span><br><span class="line">        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后的分类器</span></span><br><span class="line">        self.output_layer = nn.Linear(fc_hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text, seq_len</span>):</span><br><span class="line">        <span class="comment"># text shape: (batch_size, num_tokens)</span></span><br><span class="line">        <span class="comment"># print(&#x27;input :&#x27;, text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># print(&#x27;after word-embeding:&#x27;, embedded_text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)</span></span><br><span class="line">        <span class="comment"># num_directions = 2 if direction is &#x27;bidirectional&#x27; else 1</span></span><br><span class="line">        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)</span><br><span class="line">        <span class="comment"># print(&#x27;after lstm:&#x27;, text_repr.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, fc_hidden_size)</span></span><br><span class="line">        fc_out = paddle.tanh(self.fc(text_repr))</span><br><span class="line">        <span class="comment"># print(&#x27;after Linear classifier:&#x27;, fc_out.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_classes)</span></span><br><span class="line">        logits = self.output_layer(fc_out)</span><br><span class="line">        <span class="comment"># print(&#x27;output:&#x27;, logits.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># probs 分类概率值</span></span><br><span class="line">        probs = F.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;output probability:&#x27;, probs.shape)</span></span><br><span class="line">        <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line">model= LSTMModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab),</span><br><span class="line">        <span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br></pre></td></tr></table></figure>
<h1>模型配置和训练</h1>
<h2 id="模型配置-2">模型配置</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">        parameters=model.parameters(), learning_rate=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line">loss = paddle.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, loss, metric)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置visualdl路径</span></span><br><span class="line">log_dir = <span class="string">&#x27;./visualdl&#x27;</span></span><br><span class="line">callbacks = paddle.callbacks.VisualDL(log_dir=log_dir)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练">模型训练</h2>
<p>训练过程中会输出 loss、acc 等信息。</p>
<p>这里一共训练了 10 个 epoch，在训练集上准确率约 97%。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Building prefix dict from the default dictionary ...</span><br><span class="line">Dumping model to file cache /tmp/jieba.cache</span><br><span class="line">Loading model cost 0.867 seconds.</span><br><span class="line">Prefix dict has been built successfully.</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  return (isinstance(seq, collections.Sequence) and</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">step  10/125 - loss: 0.6945 - acc: 0.4734 - 234ms/step</span><br><span class="line">step  20/125 - loss: 0.6929 - acc: 0.4914 - 163ms/step</span><br><span class="line">step  30/125 - loss: 0.6919 - acc: 0.5068 - 138ms/step</span><br><span class="line">step  40/125 - loss: 0.6904 - acc: 0.5109 - 125ms/step</span><br><span class="line">step  50/125 - loss: 0.6878 - acc: 0.5145 - 119ms/step</span><br><span class="line">step  60/125 - loss: 0.6949 - acc: 0.5137 - 115ms/step</span><br><span class="line">step  70/125 - loss: 0.6923 - acc: 0.5143 - 113ms/step</span><br><span class="line">step  80/125 - loss: 0.6877 - acc: 0.5125 - 111ms/step</span><br><span class="line">step  90/125 - loss: 0.6898 - acc: 0.5122 - 109ms/step</span><br><span class="line">step 100/125 - loss: 0.6846 - acc: 0.5141 - 107ms/step</span><br><span class="line">step 110/125 - loss: 0.6800 - acc: 0.5156 - 105ms/step</span><br><span class="line">step 120/125 - loss: 0.6790 - acc: 0.5281 - 104ms/step</span><br><span class="line">step 125/125 - loss: 0.6796 - acc: 0.5379 - 102ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6775 - acc: 0.7937 - 96ms/step</span><br><span class="line">step 20/84 - loss: 0.6774 - acc: 0.7941 - 84ms/step</span><br><span class="line">step 30/84 - loss: 0.6776 - acc: 0.7964 - 78ms/step</span><br><span class="line">step 40/84 - loss: 0.6765 - acc: 0.7986 - 74ms/step</span><br><span class="line">step 50/84 - loss: 0.6798 - acc: 0.7972 - 71ms/step</span><br><span class="line">step 60/84 - loss: 0.6748 - acc: 0.7991 - 69ms/step</span><br><span class="line">step 70/84 - loss: 0.6782 - acc: 0.8012 - 68ms/step</span><br><span class="line">step 80/84 - loss: 0.6776 - acc: 0.8011 - 66ms/step</span><br><span class="line">step 84/84 - loss: 0.6750 - acc: 0.8011 - 63ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 2/10</span><br><span class="line">step  10/125 - loss: 0.6812 - acc: 0.7531 - 125ms/step</span><br><span class="line">step  20/125 - loss: 0.6665 - acc: 0.7902 - 110ms/step</span><br><span class="line">step  30/125 - loss: 0.6578 - acc: 0.7987 - 108ms/step</span><br><span class="line">step  40/125 - loss: 0.6452 - acc: 0.7977 - 104ms/step</span><br><span class="line">step  50/125 - loss: 0.6238 - acc: 0.8003 - 103ms/step</span><br><span class="line">step  60/125 - loss: 0.5803 - acc: 0.8124 - 102ms/step</span><br><span class="line">step  70/125 - loss: 0.4889 - acc: 0.8177 - 101ms/step</span><br><span class="line">step  80/125 - loss: 0.4504 - acc: 0.8218 - 100ms/step</span><br><span class="line">step  90/125 - loss: 0.4354 - acc: 0.8266 - 99ms/step</span><br><span class="line">step 100/125 - loss: 0.3977 - acc: 0.8316 - 98ms/step</span><br><span class="line">step 110/125 - loss: 0.4341 - acc: 0.8364 - 97ms/step</span><br><span class="line">step 120/125 - loss: 0.4397 - acc: 0.8417 - 97ms/step</span><br><span class="line">step 125/125 - loss: 0.4236 - acc: 0.8430 - 95ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.4360 - acc: 0.8906 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.4167 - acc: 0.8898 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.4203 - acc: 0.8971 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.3835 - acc: 0.8986 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.3996 - acc: 0.8978 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.4477 - acc: 0.8962 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.4174 - acc: 0.8952 - 63ms/step</span><br><span class="line">step 80/84 - loss: 0.4231 - acc: 0.8960 - 65ms/step</span><br><span class="line">step 84/84 - loss: 0.4522 - acc: 0.8966 - 62ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 3/10</span><br><span class="line">step  10/125 - loss: 0.4684 - acc: 0.8922 - 107ms/step</span><br><span class="line">step  20/125 - loss: 0.4446 - acc: 0.8938 - 96ms/step</span><br><span class="line">step  30/125 - loss: 0.4317 - acc: 0.9008 - 100ms/step</span><br><span class="line">step  40/125 - loss: 0.4128 - acc: 0.9084 - 101ms/step</span><br><span class="line">step  50/125 - loss: 0.4111 - acc: 0.9125 - 98ms/step</span><br><span class="line">step  60/125 - loss: 0.3678 - acc: 0.9182 - 96ms/step</span><br><span class="line">step  70/125 - loss: 0.3552 - acc: 0.9212 - 95ms/step</span><br><span class="line">step  80/125 - loss: 0.3769 - acc: 0.9218 - 94ms/step</span><br><span class="line">step  90/125 - loss: 0.3651 - acc: 0.9234 - 94ms/step</span><br><span class="line">step 100/125 - loss: 0.3755 - acc: 0.9231 - 93ms/step</span><br><span class="line">step 110/125 - loss: 0.3678 - acc: 0.9234 - 93ms/step</span><br><span class="line">step 120/125 - loss: 0.3909 - acc: 0.9249 - 92ms/step</span><br><span class="line">step 125/125 - loss: 0.3978 - acc: 0.9244 - 90ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3998 - acc: 0.9320 - 64ms/step</span><br><span class="line">step 20/84 - loss: 0.3875 - acc: 0.9309 - 65ms/step</span><br><span class="line">step 30/84 - loss: 0.3662 - acc: 0.9305 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.3615 - acc: 0.9322 - 67ms/step</span><br><span class="line">step 50/84 - loss: 0.3896 - acc: 0.9309 - 66ms/step</span><br><span class="line">step 60/84 - loss: 0.3854 - acc: 0.9326 - 65ms/step</span><br><span class="line">step 70/84 - loss: 0.3862 - acc: 0.9317 - 64ms/step</span><br><span class="line">step 80/84 - loss: 0.3754 - acc: 0.9324 - 62ms/step</span><br><span class="line">step 84/84 - loss: 0.4394 - acc: 0.9332 - 60ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 4/10</span><br><span class="line">step  10/125 - loss: 0.4256 - acc: 0.9219 - 129ms/step</span><br><span class="line">step  20/125 - loss: 0.4016 - acc: 0.9305 - 108ms/step</span><br><span class="line">step  30/125 - loss: 0.3773 - acc: 0.9315 - 101ms/step</span><br><span class="line">step  40/125 - loss: 0.3954 - acc: 0.9346 - 96ms/step</span><br><span class="line">step  50/125 - loss: 0.3782 - acc: 0.9353 - 95ms/step</span><br><span class="line">step  60/125 - loss: 0.3464 - acc: 0.9398 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.3456 - acc: 0.9427 - 93ms/step</span><br><span class="line">step  80/125 - loss: 0.3636 - acc: 0.9429 - 93ms/step</span><br><span class="line">step  90/125 - loss: 0.3477 - acc: 0.9435 - 93ms/step</span><br><span class="line">step 100/125 - loss: 0.3602 - acc: 0.9432 - 92ms/step</span><br><span class="line">step 110/125 - loss: 0.3622 - acc: 0.9431 - 92ms/step</span><br><span class="line">step 120/125 - loss: 0.3756 - acc: 0.9439 - 92ms/step</span><br><span class="line">step 125/125 - loss: 0.3703 - acc: 0.9433 - 90ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3898 - acc: 0.9391 - 84ms/step</span><br><span class="line">step 20/84 - loss: 0.3813 - acc: 0.9410 - 73ms/step</span><br><span class="line">step 30/84 - loss: 0.3603 - acc: 0.9414 - 73ms/step</span><br><span class="line">step 40/84 - loss: 0.3644 - acc: 0.9422 - 71ms/step</span><br><span class="line">step 50/84 - loss: 0.3744 - acc: 0.9417 - 70ms/step</span><br><span class="line">step 60/84 - loss: 0.3567 - acc: 0.9437 - 70ms/step</span><br><span class="line">step 70/84 - loss: 0.3745 - acc: 0.9420 - 70ms/step</span><br><span class="line">step 80/84 - loss: 0.3677 - acc: 0.9426 - 68ms/step</span><br><span class="line">step 84/84 - loss: 0.4366 - acc: 0.9432 - 66ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 5/10</span><br><span class="line">step  10/125 - loss: 0.3941 - acc: 0.9328 - 114ms/step</span><br><span class="line">step  20/125 - loss: 0.3838 - acc: 0.9387 - 107ms/step</span><br><span class="line">step  30/125 - loss: 0.3766 - acc: 0.9414 - 102ms/step</span><br><span class="line">step  40/125 - loss: 0.3818 - acc: 0.9439 - 98ms/step</span><br><span class="line">step  50/125 - loss: 0.3641 - acc: 0.9450 - 97ms/step</span><br><span class="line">step  60/125 - loss: 0.3353 - acc: 0.9488 - 96ms/step</span><br><span class="line">step  70/125 - loss: 0.3363 - acc: 0.9510 - 95ms/step</span><br><span class="line">step  80/125 - loss: 0.3508 - acc: 0.9511 - 95ms/step</span><br><span class="line">step  90/125 - loss: 0.3450 - acc: 0.9513 - 95ms/step</span><br><span class="line">step 100/125 - loss: 0.3450 - acc: 0.9514 - 95ms/step</span><br><span class="line">step 110/125 - loss: 0.3547 - acc: 0.9513 - 94ms/step</span><br><span class="line">step 120/125 - loss: 0.3697 - acc: 0.9520 - 93ms/step</span><br><span class="line">step 125/125 - loss: 0.3807 - acc: 0.9512 - 92ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3845 - acc: 0.9414 - 86ms/step</span><br><span class="line">step 20/84 - loss: 0.3750 - acc: 0.9465 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.3583 - acc: 0.9458 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.3670 - acc: 0.9463 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.3688 - acc: 0.9453 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.3614 - acc: 0.9467 - 63ms/step</span><br><span class="line">step 70/84 - loss: 0.3717 - acc: 0.9452 - 64ms/step</span><br><span class="line">step 80/84 - loss: 0.3554 - acc: 0.9458 - 65ms/step</span><br><span class="line">step 84/84 - loss: 0.4361 - acc: 0.9465 - 64ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 6/10</span><br><span class="line">step  10/125 - loss: 0.3749 - acc: 0.9477 - 75ms/step</span><br><span class="line">step  20/125 - loss: 0.3694 - acc: 0.9504 - 75ms/step</span><br><span class="line">step  30/125 - loss: 0.3521 - acc: 0.9539 - 73ms/step</span><br><span class="line">step  40/125 - loss: 0.3791 - acc: 0.9541 - 78ms/step</span><br><span class="line">step  50/125 - loss: 0.3515 - acc: 0.9544 - 81ms/step</span><br><span class="line">step  60/125 - loss: 0.3352 - acc: 0.9574 - 81ms/step</span><br><span class="line">step  70/125 - loss: 0.3314 - acc: 0.9590 - 81ms/step</span><br><span class="line">step  80/125 - loss: 0.3496 - acc: 0.9584 - 82ms/step</span><br><span class="line">step  90/125 - loss: 0.3433 - acc: 0.9582 - 82ms/step</span><br><span class="line">step 100/125 - loss: 0.3400 - acc: 0.9580 - 83ms/step</span><br><span class="line">step 110/125 - loss: 0.3451 - acc: 0.9580 - 83ms/step</span><br><span class="line">step 120/125 - loss: 0.3599 - acc: 0.9589 - 83ms/step</span><br><span class="line">step 125/125 - loss: 0.3598 - acc: 0.9585 - 81ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3754 - acc: 0.9484 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.3714 - acc: 0.9535 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.3558 - acc: 0.9523 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.3603 - acc: 0.9533 - 62ms/step</span><br><span class="line">step 50/84 - loss: 0.3719 - acc: 0.9514 - 61ms/step</span><br><span class="line">step 60/84 - loss: 0.3442 - acc: 0.9525 - 60ms/step</span><br><span class="line">step 70/84 - loss: 0.3654 - acc: 0.9513 - 59ms/step</span><br><span class="line">step 80/84 - loss: 0.3602 - acc: 0.9514 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.4414 - acc: 0.9520 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 7/10</span><br><span class="line">step  10/125 - loss: 0.3673 - acc: 0.9523 - 104ms/step</span><br><span class="line">step  20/125 - loss: 0.3638 - acc: 0.9566 - 93ms/step</span><br><span class="line">step  30/125 - loss: 0.3488 - acc: 0.9599 - 90ms/step</span><br><span class="line">step  40/125 - loss: 0.3790 - acc: 0.9600 - 88ms/step</span><br><span class="line">step  50/125 - loss: 0.3557 - acc: 0.9583 - 88ms/step</span><br><span class="line">step  60/125 - loss: 0.3309 - acc: 0.9604 - 88ms/step</span><br><span class="line">step  70/125 - loss: 0.3366 - acc: 0.9621 - 88ms/step</span><br><span class="line">step  80/125 - loss: 0.3372 - acc: 0.9618 - 87ms/step</span><br><span class="line">step  90/125 - loss: 0.3326 - acc: 0.9615 - 87ms/step</span><br><span class="line">step 100/125 - loss: 0.3365 - acc: 0.9612 - 88ms/step</span><br><span class="line">step 110/125 - loss: 0.3404 - acc: 0.9615 - 88ms/step</span><br><span class="line">step 120/125 - loss: 0.3582 - acc: 0.9626 - 87ms/step</span><br><span class="line">step 125/125 - loss: 0.3549 - acc: 0.9621 - 86ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3800 - acc: 0.9531 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.3618 - acc: 0.9559 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.3520 - acc: 0.9555 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.3568 - acc: 0.9566 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.3752 - acc: 0.9552 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.3430 - acc: 0.9559 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.3786 - acc: 0.9550 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.3554 - acc: 0.9557 - 59ms/step</span><br><span class="line">step 84/84 - loss: 0.3533 - acc: 0.9563 - 57ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 8/10</span><br><span class="line">step  10/125 - loss: 0.3558 - acc: 0.9617 - 109ms/step</span><br><span class="line">step  20/125 - loss: 0.3595 - acc: 0.9641 - 97ms/step</span><br><span class="line">step  30/125 - loss: 0.3484 - acc: 0.9654 - 93ms/step</span><br><span class="line">step  40/125 - loss: 0.3728 - acc: 0.9639 - 90ms/step</span><br><span class="line">step  50/125 - loss: 0.3405 - acc: 0.9639 - 89ms/step</span><br><span class="line">step  60/125 - loss: 0.3275 - acc: 0.9660 - 88ms/step</span><br><span class="line">step  70/125 - loss: 0.3262 - acc: 0.9673 - 87ms/step</span><br><span class="line">step  80/125 - loss: 0.3359 - acc: 0.9668 - 87ms/step</span><br><span class="line">step  90/125 - loss: 0.3285 - acc: 0.9667 - 87ms/step</span><br><span class="line">step 100/125 - loss: 0.3344 - acc: 0.9663 - 87ms/step</span><br><span class="line">step 110/125 - loss: 0.3351 - acc: 0.9666 - 87ms/step</span><br><span class="line">step 120/125 - loss: 0.3564 - acc: 0.9676 - 87ms/step</span><br><span class="line">step 125/125 - loss: 0.3524 - acc: 0.9672 - 86ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3692 - acc: 0.9602 - 73ms/step</span><br><span class="line">step 20/84 - loss: 0.3669 - acc: 0.9582 - 72ms/step</span><br><span class="line">step 30/84 - loss: 0.3471 - acc: 0.9586 - 70ms/step</span><br><span class="line">step 40/84 - loss: 0.3467 - acc: 0.9586 - 69ms/step</span><br><span class="line">step 50/84 - loss: 0.3713 - acc: 0.9573 - 69ms/step</span><br><span class="line">step 60/84 - loss: 0.3442 - acc: 0.9578 - 69ms/step</span><br><span class="line">step 70/84 - loss: 0.3561 - acc: 0.9576 - 69ms/step</span><br><span class="line">step 80/84 - loss: 0.3410 - acc: 0.9579 - 69ms/step</span><br><span class="line">step 84/84 - loss: 0.4010 - acc: 0.9585 - 66ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 9/10</span><br><span class="line">step  10/125 - loss: 0.3517 - acc: 0.9602 - 106ms/step</span><br><span class="line">step  20/125 - loss: 0.3577 - acc: 0.9648 - 95ms/step</span><br><span class="line">step  30/125 - loss: 0.3434 - acc: 0.9669 - 92ms/step</span><br><span class="line">step  40/125 - loss: 0.3667 - acc: 0.9660 - 89ms/step</span><br><span class="line">step  50/125 - loss: 0.3391 - acc: 0.9661 - 89ms/step</span><br><span class="line">step  60/125 - loss: 0.3251 - acc: 0.9680 - 88ms/step</span><br><span class="line">step  70/125 - loss: 0.3235 - acc: 0.9695 - 87ms/step</span><br><span class="line">step  80/125 - loss: 0.3325 - acc: 0.9692 - 87ms/step</span><br><span class="line">step  90/125 - loss: 0.3263 - acc: 0.9694 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.3323 - acc: 0.9692 - 92ms/step</span><br><span class="line">step 110/125 - loss: 0.3316 - acc: 0.9694 - 93ms/step</span><br><span class="line">step 120/125 - loss: 0.3547 - acc: 0.9702 - 93ms/step</span><br><span class="line">step 125/125 - loss: 0.3506 - acc: 0.9699 - 91ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3649 - acc: 0.9609 - 84ms/step</span><br><span class="line">step 20/84 - loss: 0.3670 - acc: 0.9586 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.3463 - acc: 0.9586 - 64ms/step</span><br><span class="line">step 40/84 - loss: 0.3450 - acc: 0.9594 - 62ms/step</span><br><span class="line">step 50/84 - loss: 0.3687 - acc: 0.9583 - 61ms/step</span><br><span class="line">step 60/84 - loss: 0.3484 - acc: 0.9587 - 60ms/step</span><br><span class="line">step 70/84 - loss: 0.3511 - acc: 0.9587 - 59ms/step</span><br><span class="line">step 80/84 - loss: 0.3392 - acc: 0.9592 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.4006 - acc: 0.9597 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 10/10</span><br><span class="line">step  10/125 - loss: 0.3449 - acc: 0.9625 - 89ms/step</span><br><span class="line">step  20/125 - loss: 0.3561 - acc: 0.9676 - 105ms/step</span><br><span class="line">step  30/125 - loss: 0.3506 - acc: 0.9693 - 108ms/step</span><br><span class="line">step  40/125 - loss: 0.3665 - acc: 0.9676 - 109ms/step</span><br><span class="line">step  50/125 - loss: 0.3380 - acc: 0.9675 - 107ms/step</span><br><span class="line">step  60/125 - loss: 0.3235 - acc: 0.9693 - 106ms/step</span><br><span class="line">step  70/125 - loss: 0.3228 - acc: 0.9708 - 105ms/step</span><br><span class="line">step  80/125 - loss: 0.3274 - acc: 0.9706 - 106ms/step</span><br><span class="line">step  90/125 - loss: 0.3256 - acc: 0.9705 - 105ms/step</span><br><span class="line">step 100/125 - loss: 0.3307 - acc: 0.9702 - 103ms/step</span><br><span class="line">step 110/125 - loss: 0.3350 - acc: 0.9700 - 102ms/step</span><br><span class="line">step 120/125 - loss: 0.3551 - acc: 0.9709 - 100ms/step</span><br><span class="line">step 125/125 - loss: 0.3524 - acc: 0.9706 - 98ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3675 - acc: 0.9602 - 89ms/step</span><br><span class="line">step 20/84 - loss: 0.3638 - acc: 0.9602 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.3522 - acc: 0.9599 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.3541 - acc: 0.9600 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.3643 - acc: 0.9587 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.3276 - acc: 0.9600 - 60ms/step</span><br><span class="line">step 70/84 - loss: 0.3688 - acc: 0.9596 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.3467 - acc: 0.9597 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.3462 - acc: 0.9603 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/final</span><br></pre></td></tr></table></figure>
<h2 id="启动-VisualDL-查看训练过程可视化结果">启动 VisualDL 查看训练过程可视化结果</h2>
<p>启动步骤：</p>
<ul>
<li>1、切换到本界面左侧「可视化」</li>
<li>2、日志文件路径选择 ‘visualdl’</li>
<li>3、点击「启动 VisualDL」后点击「打开 VisualDL」，即可查看可视化结果：</li>
</ul>
<p>Accuracy 和 Loss 的实时变化趋势如下：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/03b59d954509455a44c4d6b4b6214b8d.png" alt="train_loss"><br>
<img src="https://img-blog.csdnimg.cn/img_convert/1bb4b6f02f714cfda3bd3b9e6827263c.png" alt="train_acc"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2f99481a79e83e90df9a91d3b8417046.png" alt="eval_loss"><br>
<img src="https://img-blog.csdnimg.cn/img_convert/f9347fec87e7dfb520e3e9c7c7153fd9.png" alt="eval_loss"></p>
<h2 id="评估">评估</h2>
<p>最终得到的评估准确率为 96%</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results = model.evaluate(dev_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finally test acc: %.5f&quot;</span> % results[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3675 - acc: 0.9602 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.3638 - acc: 0.9602 - 70ms/step</span><br><span class="line">step 30/84 - loss: 0.3522 - acc: 0.9599 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.3541 - acc: 0.9600 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.3643 - acc: 0.9587 - 63ms/step</span><br><span class="line">step 60/84 - loss: 0.3276 - acc: 0.9600 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.3688 - acc: 0.9596 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.3467 - acc: 0.9597 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.3462 - acc: 0.9603 - 59ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Finally test acc: 0.96027</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">model.save(<span class="string">&#x27;./finetuning/lstm/model&#x27;</span>, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1>预测</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model= LSTMModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab),</span><br><span class="line">        <span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br><span class="line"></span><br><span class="line">model.load(<span class="string">&#x27;./finetuning/model&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.prepare()</span><br><span class="line"></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="string">&#x27;negative&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;positive&#x27;</span>&#125;</span><br><span class="line">results = model.predict(test_loader, batch_size=<span class="number">128</span>)[<span class="number">0</span>]</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_probs <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># 映射分类label</span></span><br><span class="line">    idx = np.argmax(batch_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    idx = idx.tolist()</span><br><span class="line">    labels = [label_map[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    predictions.extend(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看预测数据前5个样例分类结果</span></span><br><span class="line"><span class="keyword">for</span> idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_ds.data[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Data: &#123;&#125; \t Label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data[<span class="number">0</span>], predictions[idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 42/42 [==============================] - ETA: 3s - 97ms/ste - ETA: 3s - 104ms/st - ETA: 3s - 108ms/st - ETA: 3s - 97ms/step - ETA: 2s - 89ms/ste - ETA: 2s - 85ms/ste - ETA: 2s - 81ms/ste - ETA: 2s - 78ms/ste - ETA: 1s - 76ms/ste - ETA: 1s - 74ms/ste - ETA: 1s - 73ms/ste - ETA: 1s - 72ms/ste - ETA: 1s - 71ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 69ms/ste - ETA: 0s - 69ms/ste - ETA: 0s - 68ms/ste - ETA: 0s - 67ms/ste - ETA: 0s - 64ms/ste - 62ms/step</span><br><span class="line">Predict samples: 5353</span><br><span class="line">Data: 楼面经理服务态度极差，等位和埋单都差，楼面小妹还挺好 	 Label: negative</span><br><span class="line">Data: 欺负北方人没吃过鲍鱼是怎么着？简直敷衍到可笑的程度，团购连青菜都是两人份？！难吃到死，菜色还特别可笑，什么时候粤菜的小菜改成拍黄瓜了？！把团购客人当傻子，可这满大厅的傻子谁还会再来？！ 	 Label: negative</span><br><span class="line">Data: 如果大家有时间而且不怕麻烦的话可以去这里试试，点一个饭等左2个钟，没错！是两个钟！期间催了n遍都说马上到，结果？呵呵。乳鸽的味道，太咸，可能不新鲜吧……要用重口味盖住异味。上菜超级慢！中途还搞什么表演，麻烦有人手的话就上菜啊，表什么演？！？！要大家饿着看表演吗？最后结账还算错单，我真心服了……有一种店叫不会有下次，大概就是指它吧 	 Label: negative</span><br><span class="line">Data: 偌大的一个大厅就一个人点菜，点菜速度超级慢，菜牌上多个菜停售，连续点了两个没标停售的菜也告知没有，粥上来是凉的，榴莲酥火大了，格格肉超级油腻而且咸?????? 	 Label: negative</span><br><span class="line">Data: 泥撕雞超級好吃！！！吃了一個再叫一個還想打包的節奏！ 	 Label: positive</span><br><span class="line">Data: 作为地道的广州人，从小就跟着家人在西关品尝各式美食，今日带着家中长辈来这个老字号泮溪酒家真实失望透顶，出品差、服务差、洗手间邋遢弥漫着浓郁尿骚味、丢广州人的脸、丢广州老字号的脸。 	 Label: negative</span><br><span class="line">Data: 辣味道很赞哦！猪肚鸡一直是我们的最爱，每次来都必点，服务很给力，环境很好，值得分享哦！西洋菜 	 Label: positive</span><br><span class="line">Data: 第一次吃到這麼脏的火鍋：吃着吃著吃出一條尾指粗的黑毛毛蟲——惡心！脏！！！第一次吃到這麼無誠信的火鍋服務：我們呼喚人員時，某女部長立即使服務員迅速取走蟲所在的碗，任我們多次叫「放下」論理，她們也置若罔聞轉身將蟲毁屍滅跡，還嘻皮笑臉辯稱只是把碗換走,態度行為惡劣——奸詐！毫無誠信！！爛！！！當然還有剛坐下時的情形：第一次吃到這樣的火鍋：所有肉食熟食都上桌了，鍋底遲遲沒上，足足等了半小時才姍姍來遲；---差！！第一次吃到這樣的火鍋：1元雞鍋、1碟6塊小牛肉、1碟小腐皮、1碟5塊裝的普通肥牛、1碟數片的細碎牛肚結帳便2百多元；---不值！！以下省略千字差評......白云路的稻香是最差、最失禮的稻香，天河城、華廈的都比它好上過萬倍！！白云路的稻香是史上最差的餐廳！！！ 	 Label: negative</span><br><span class="line">Data: 文昌鸡份量很少且很咸，其他菜味道很一般！服务态度差差差！还要10%的服务费、 	 Label: negative</span><br><span class="line">Data: 这个网站的评价真是越来越不可信了，搞不懂为什么这么多好评。真的是很一般，不要迷信什么哪里回来的大厨吧。环境和出品若是当作普通茶餐厅来看待就还说得过去，但是价格又不是茶餐厅的价格，这就很尴尬了。。服务也是有待提高。 	 Label: negative</span><br></pre></td></tr></table></figure>
<h2 id="修改seq2vec模型">修改<code>seq2vec</code>模型</h2>
<p><strong><code>seq2vec</code>模块</strong></p>
<ul>
<li>
<p>输入：文本序列的 Embedding Tensor，shape：(batch_size, num_token, emb_dim)</p>
</li>
<li>
<p>输出：文本语义表征 Enocded Texts Tensor，shape：(batch_sie,encoding_size)</p>
</li>
<li>
<p>提供了<code>BoWEncoder</code>，<code>CNNEncoder</code>，<code>GRUEncoder</code>，<code>LSTMEncoder</code>，<code>RNNEncoder</code>等模型</p>
<ul>
<li>
<p><code>BoWEncoder</code> 是将输入序列 Embedding Tensor 在 num_token 维度上叠加，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>CNNEncoder</code> 是将输入序列 Embedding Tensor 进行卷积操作，在对卷积结果进行 max_pooling，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>GRUEncoder</code> 是对输入序列 Embedding Tensor 进行 GRU 运算，在运算结果上进行 pooling 或者取最后一个 step 的隐表示，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>LSTMEncoder</code> 是对输入序列 Embedding Tensor 进行 LSTM 运算，在运算结果上进行 pooling 或者取最后一个 step 的隐表示，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>RNNEncoder</code> 是对输入序列 Embedding Tensor 进行 RNN 运算，在运算结果上进行 pooling 或者取最后一个 step 的隐表示，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
</ul>
</li>
<li>
<p><code>seq2vec</code>提供了许多语义表征方法，那么这些方法有什么特点呢？</p>
<ol>
<li><code>BoWEncoder</code>采用 Bag of Word Embedding 方法，其特点是简单。但其缺点是没有考虑文本的语境，所以对文本语义的表征不足以表意。</li>
<li><code>CNNEncoder</code>采用卷积操作，提取局部特征，其特点是可以共享权重。但其缺点同样只考虑了局部语义，上下文信息没有充分利用。</li>
</ol>
<center>
  <img src="https://ai-studio-static-online.cdn.bcebos.com/2b2498edd83e49d3b017c4a14e1be68506349249b8a24cdaa214755fb51eadcd" width="400" height="150" >
</center>
<center>
  图2：卷积示意图
</center>
</br>
<ol start="3">
<li><code>RNNEnocder</code>采用 RNN 方法，在计算下一个 token 语义信息时，利用上一个 token 语义信息作为其输入。但其缺点容易产生梯度消失和梯度爆炸。</li>
</ol>
  <p align="center">
  <img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fxilinx.eetrend.com%2Ffiles-eetrend-xilinx%2Farticle%2F201706%2F11511-30367-tu4rnn.jpg&refer=http%3A%2F%2Fxilinx.eetrend.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1615266778&t=23c2518ff8126315ddaa780303e79737" width = "40%" height = "20%"  hspace='10'/> 
  </p>
  <center>
    图3：RNN示意图
  </center>
  </br>
<ol start="4">
<li><code>LSTMEnocder</code>采用 LSTM 方法，LSTM 是 RNN 的一种变种。为了学到长期依赖关系，LSTM 中引入了门控机制来控制信息的累计速度，包括有选择地加入新的信息，并有选择地遗忘之前累计的信息。</li>
</ol>
<p align="center">
  <img src="https://ai-studio-static-online.cdn.bcebos.com/a5af1d93c69f422d963e094397a2f6ce978c30a26ab6480ab70d688dd1929de0" width = "50%" height = "30%"  hspace='10'/> 
</center>
<center>
  图4：LSTM示意图
</center>
</br>
<ol start="5">
<li><code>GRUEncoder</code>采用 GRU 方法，GRU 也是 RNN 的一种变种。一个 LSTM 单元有四个输入 ，因而参数是 RNN 的四倍，带来的结果是训练速度慢。GRU 对 LSTM 进行了简化，在不影响效果的前提下加快了训练速度。</li>
</ol>
</li>
</ul>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/fc848bc2cb494b40ae42af892b756f5888770320a1fa42348cec10d3df64ee2f" width = "40%" height = "25%"  hspace='10'/> 
  <br />
</p><br><center>图5：GRU示意图</center></br>
<p>关于 CNN、LSTM、GRU、RNN 等更多信息参考：</p>
<ul>
<li>Understanding LSTM Networks: <a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
<li>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</a></li>
<li>A Critical Review of Recurrent Neural Networks</li>
</ul>
<p>for Sequence Learning: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.00019">https://arxiv.org/pdf/1506.00019</a></p>
<ul>
<li>A Convolutional Neural Network for Modelling Sentences: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1404.2188">https://arxiv.org/abs/1404.2188</a></li>
</ul>
<h3 id="数据准备-2">数据准备</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddlenlp <span class="keyword">as</span> ppnlp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddlenlp内置数据集</span></span><br><span class="line"><span class="comment"># train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets([&#x27;train&#x27;, &#x27;dev&#x27;, &#x27;test&#x27;])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># label_list = train_ds.get_labels()</span></span><br><span class="line"><span class="comment"># print(label_list)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for sent, label in train_ds[:5]:</span></span><br><span class="line"><span class="comment">#     print (sent, label)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tmp = ppnlp.datasets.ChnSentiCorp(&#x27;train&#x27;)</span></span><br><span class="line"><span class="comment"># tmp1 = ppnlp.datasets.MapDatasetWrapper(tmp)</span></span><br><span class="line"><span class="comment"># print(type(tmp), type(tmp1))</span></span><br><span class="line"><span class="comment"># for sent, label in tmp1[:5]:</span></span><br><span class="line"><span class="comment">#     print(sent, label)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Pad, Stack, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> create_dataloader,convert_example</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reads data and generates mini-batches.</span></span><br><span class="line">trans_fn = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    vocab=vocab,</span><br><span class="line">    unk_token_id=vocab.get(<span class="string">&#x27;[UNK]&#x27;</span>, <span class="number">1</span>),</span><br><span class="line">    is_test=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的数据batch化处理，便于模型batch化运算。</span></span><br><span class="line"><span class="comment"># batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。</span></span><br><span class="line"><span class="comment"># 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.</span></span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=vocab[<span class="string">&#x27;[PAD]&#x27;</span>]),  <span class="comment"># input_ids</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>),  <span class="comment"># seq len</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br><span class="line">train_loader = create_dataloader(</span><br><span class="line">    train_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">dev_loader = create_dataloader(</span><br><span class="line">    dev_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">test_loader = create_dataloader(</span><br><span class="line">    test_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br></pre></td></tr></table></figure>
<h3 id="模型建立">模型建立</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GRUModel</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 num_classes,</span></span><br><span class="line"><span class="params">                 emb_dim=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 padding_idx=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 hidden_size=<span class="number">198</span>,</span></span><br><span class="line"><span class="params">                 direction=<span class="string">&#x27;forward&#x27;</span>,</span></span><br><span class="line"><span class="params">                 num_layers=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 pooling_type=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 fc_hidden_size=<span class="number">96</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 首先将输入word id 查表后映射成 word embedding</span></span><br><span class="line">        self.embedder = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size,</span><br><span class="line">            embedding_dim=emb_dim,</span><br><span class="line">            padding_idx=padding_idx)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将word embedding经过LSTMEncoder变换到文本语义表征空间中</span></span><br><span class="line">        self.gru_encoder = ppnlp.seq2vec.GRUEncoder(</span><br><span class="line">            emb_dim,</span><br><span class="line">            hidden_size,</span><br><span class="line">            num_layers=num_layers,</span><br><span class="line">            direction=direction,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            pooling_type=pooling_type)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size</span></span><br><span class="line">        self.fc = nn.Linear(self.gru_encoder.get_output_dim(), fc_hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后的分类器</span></span><br><span class="line">        self.output_layer = nn.Linear(fc_hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text, seq_len</span>):</span><br><span class="line">        <span class="comment"># text shape: (batch_size, num_tokens)</span></span><br><span class="line">        <span class="comment"># print(&#x27;input :&#x27;, text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># print(&#x27;after word-embeding:&#x27;, embedded_text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)</span></span><br><span class="line">        <span class="comment"># num_directions = 2 if direction is &#x27;bidirectional&#x27; else 1</span></span><br><span class="line">        text_repr = self.gru_encoder(embedded_text, sequence_length=seq_len)</span><br><span class="line">        <span class="comment"># print(&#x27;after lstm:&#x27;, text_repr.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, fc_hidden_size)</span></span><br><span class="line">        fc_out = paddle.tanh(self.fc(text_repr))</span><br><span class="line">        <span class="comment"># print(&#x27;after Linear classifier:&#x27;, fc_out.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_classes)</span></span><br><span class="line">        logits = self.output_layer(fc_out)</span><br><span class="line">        <span class="comment"># print(&#x27;output:&#x27;, logits.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># probs 分类概率值</span></span><br><span class="line">        probs = F.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;output probability:&#x27;, probs.shape)</span></span><br><span class="line">        <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure>
<h3 id="模型配置-3">模型配置</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">model= GRUModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab),</span><br><span class="line">        <span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br><span class="line"></span><br><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">        parameters=model.parameters(), learning_rate=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line">loss = paddle.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, loss, metric)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置visualdl路径</span></span><br><span class="line">log_dir = <span class="string">&#x27;./visualdl/gru_v1&#x27;</span></span><br><span class="line">callbacks = paddle.callbacks.VisualDL(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints/gru_v1&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br></pre></td><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line">step  10/125 - loss: 1.1046 - acc: 0.0000e+00 - 111ms/step</span><br><span class="line">step  20/125 - loss: 1.0817 - acc: 0.2332 - 99ms/step</span><br><span class="line">step  30/125 - loss: 1.0604 - acc: 0.3221 - 95ms/step</span><br><span class="line">step  40/125 - loss: 1.0446 - acc: 0.3602 - 94ms/step</span><br><span class="line">step  50/125 - loss: 1.0306 - acc: 0.3811 - 92ms/step</span><br><span class="line">step  60/125 - loss: 1.0166 - acc: 0.4025 - 91ms/step</span><br><span class="line">step  70/125 - loss: 1.0039 - acc: 0.4552 - 91ms/step</span><br><span class="line">step  80/125 - loss: 0.9935 - acc: 0.4743 - 91ms/step</span><br><span class="line">step  90/125 - loss: 0.9841 - acc: 0.5078 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.9752 - acc: 0.5260 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.9695 - acc: 0.5223 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.9620 - acc: 0.5324 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.9583 - acc: 0.5418 - 88ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_v1/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.9590 - acc: 0.7398 - 95ms/step</span><br><span class="line">step 20/84 - loss: 0.9589 - acc: 0.7324 - 75ms/step</span><br><span class="line">step 30/84 - loss: 0.9585 - acc: 0.7328 - 69ms/step</span><br><span class="line">step 40/84 - loss: 0.9579 - acc: 0.7295 - 67ms/step</span><br><span class="line">step 50/84 - loss: 0.9593 - acc: 0.7316 - 67ms/step</span><br><span class="line">step 60/84 - loss: 0.9556 - acc: 0.7346 - 67ms/step</span><br><span class="line">step 70/84 - loss: 0.9594 - acc: 0.7323 - 67ms/step</span><br><span class="line">step 80/84 - loss: 0.9589 - acc: 0.7323 - 66ms/step</span><br><span class="line">step 84/84 - loss: 0.9616 - acc: 0.7330 - 63ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 2/10</span><br><span class="line">step  10/125 - loss: 0.9529 - acc: 0.7086 - 114ms/step</span><br><span class="line">step  20/125 - loss: 0.9500 - acc: 0.7051 - 104ms/step</span><br><span class="line">step  30/125 - loss: 0.9377 - acc: 0.7341 - 98ms/step</span><br><span class="line">step  40/125 - loss: 0.9367 - acc: 0.7318 - 96ms/step</span><br><span class="line">step  50/125 - loss: 0.9213 - acc: 0.7439 - 94ms/step</span><br><span class="line">step  60/125 - loss: 0.8881 - acc: 0.7379 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.8537 - acc: 0.7412 - 92ms/step</span><br><span class="line">step  80/125 - loss: 0.8674 - acc: 0.7401 - 93ms/step</span><br><span class="line">step  90/125 - loss: 0.8386 - acc: 0.7376 - 92ms/step</span><br><span class="line">step 100/125 - loss: 0.8068 - acc: 0.7366 - 92ms/step</span><br><span class="line">step 110/125 - loss: 0.8452 - acc: 0.7364 - 92ms/step</span><br><span class="line">step 120/125 - loss: 0.7921 - acc: 0.7374 - 91ms/step</span><br><span class="line">step 125/125 - loss: 0.8001 - acc: 0.7378 - 89ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.7727 - acc: 0.7500 - 88ms/step</span><br><span class="line">step 20/84 - loss: 0.7982 - acc: 0.7555 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.7934 - acc: 0.7549 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.7760 - acc: 0.7557 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.7713 - acc: 0.7572 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.7942 - acc: 0.7568 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.8002 - acc: 0.7545 - 62ms/step</span><br><span class="line">step 80/84 - loss: 0.7784 - acc: 0.7554 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.8570 - acc: 0.7559 - 59ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 3/10</span><br><span class="line">step  10/125 - loss: 0.7971 - acc: 0.7773 - 109ms/step</span><br><span class="line">step  20/125 - loss: 0.7422 - acc: 0.7855 - 100ms/step</span><br><span class="line">step  30/125 - loss: 0.7125 - acc: 0.7951 - 97ms/step</span><br><span class="line">step  40/125 - loss: 0.7575 - acc: 0.8016 - 94ms/step</span><br><span class="line">step  50/125 - loss: 0.6859 - acc: 0.8089 - 93ms/step</span><br><span class="line">step  60/125 - loss: 0.6763 - acc: 0.8214 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.6569 - acc: 0.8319 - 94ms/step</span><br><span class="line">step  80/125 - loss: 0.6440 - acc: 0.8431 - 93ms/step</span><br><span class="line">step  90/125 - loss: 0.6573 - acc: 0.8521 - 95ms/step</span><br><span class="line">step 100/125 - loss: 0.6323 - acc: 0.8586 - 94ms/step</span><br><span class="line">step 110/125 - loss: 0.6311 - acc: 0.8650 - 94ms/step</span><br><span class="line">step 120/125 - loss: 0.6425 - acc: 0.8699 - 94ms/step</span><br><span class="line">step 125/125 - loss: 0.6360 - acc: 0.8723 - 92ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6429 - acc: 0.9258 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.6527 - acc: 0.9227 - 70ms/step</span><br><span class="line">step 30/84 - loss: 0.6373 - acc: 0.9237 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.6258 - acc: 0.9271 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6468 - acc: 0.9269 - 63ms/step</span><br><span class="line">step 60/84 - loss: 0.6249 - acc: 0.9279 - 63ms/step</span><br><span class="line">step 70/84 - loss: 0.6554 - acc: 0.9261 - 62ms/step</span><br><span class="line">step 80/84 - loss: 0.6408 - acc: 0.9262 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.6208 - acc: 0.9280 - 58ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 4/10</span><br><span class="line">step  10/125 - loss: 0.6157 - acc: 0.9445 - 107ms/step</span><br><span class="line">step  20/125 - loss: 0.6325 - acc: 0.9371 - 97ms/step</span><br><span class="line">step  30/125 - loss: 0.6596 - acc: 0.9365 - 95ms/step</span><br><span class="line">step  40/125 - loss: 0.6046 - acc: 0.9361 - 93ms/step</span><br><span class="line">step  50/125 - loss: 0.6530 - acc: 0.9320 - 92ms/step</span><br><span class="line">step  60/125 - loss: 0.6364 - acc: 0.9341 - 91ms/step</span><br><span class="line">step  70/125 - loss: 0.6242 - acc: 0.9348 - 90ms/step</span><br><span class="line">step  80/125 - loss: 0.6116 - acc: 0.9347 - 90ms/step</span><br><span class="line">step  90/125 - loss: 0.6371 - acc: 0.9350 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.6125 - acc: 0.9355 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5946 - acc: 0.9368 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.6274 - acc: 0.9363 - 89ms/step</span><br><span class="line">step 125/125 - loss: 0.5729 - acc: 0.9368 - 87ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6248 - acc: 0.9398 - 84ms/step</span><br><span class="line">step 20/84 - loss: 0.6306 - acc: 0.9383 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.6160 - acc: 0.9401 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.6045 - acc: 0.9424 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6228 - acc: 0.9414 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5969 - acc: 0.9419 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.6313 - acc: 0.9408 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.6134 - acc: 0.9413 - 59ms/step</span><br><span class="line">step 84/84 - loss: 0.6030 - acc: 0.9427 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 5/10</span><br><span class="line">step  10/125 - loss: 0.5897 - acc: 0.9445 - 107ms/step</span><br><span class="line">step  20/125 - loss: 0.6003 - acc: 0.9520 - 98ms/step</span><br><span class="line">step  30/125 - loss: 0.6177 - acc: 0.9516 - 96ms/step</span><br><span class="line">step  40/125 - loss: 0.6054 - acc: 0.9525 - 94ms/step</span><br><span class="line">step  50/125 - loss: 0.6303 - acc: 0.9537 - 93ms/step</span><br><span class="line">step  60/125 - loss: 0.6067 - acc: 0.9530 - 94ms/step</span><br><span class="line">step  70/125 - loss: 0.5856 - acc: 0.9540 - 93ms/step</span><br><span class="line">step  80/125 - loss: 0.5976 - acc: 0.9545 - 92ms/step</span><br><span class="line">step  90/125 - loss: 0.6220 - acc: 0.9537 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.6000 - acc: 0.9540 - 91ms/step</span><br><span class="line">step 110/125 - loss: 0.6048 - acc: 0.9522 - 91ms/step</span><br><span class="line">step 120/125 - loss: 0.6144 - acc: 0.9520 - 91ms/step</span><br><span class="line">step 125/125 - loss: 0.6159 - acc: 0.9520 - 89ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6338 - acc: 0.9406 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.6171 - acc: 0.9441 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.6064 - acc: 0.9461 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5934 - acc: 0.9490 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.6212 - acc: 0.9481 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.5881 - acc: 0.9484 - 64ms/step</span><br><span class="line">step 70/84 - loss: 0.6167 - acc: 0.9477 - 63ms/step</span><br><span class="line">step 80/84 - loss: 0.5918 - acc: 0.9479 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.6166 - acc: 0.9488 - 59ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 6/10</span><br><span class="line">step  10/125 - loss: 0.5790 - acc: 0.9539 - 111ms/step</span><br><span class="line">step  20/125 - loss: 0.5977 - acc: 0.9559 - 99ms/step</span><br><span class="line">step  30/125 - loss: 0.5986 - acc: 0.9557 - 96ms/step</span><br><span class="line">step  40/125 - loss: 0.5777 - acc: 0.9551 - 95ms/step</span><br><span class="line">step  50/125 - loss: 0.5906 - acc: 0.9563 - 93ms/step</span><br><span class="line">step  60/125 - loss: 0.5921 - acc: 0.9577 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.5816 - acc: 0.9589 - 92ms/step</span><br><span class="line">step  80/125 - loss: 0.6051 - acc: 0.9584 - 92ms/step</span><br><span class="line">step  90/125 - loss: 0.5874 - acc: 0.9579 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.5957 - acc: 0.9590 - 91ms/step</span><br><span class="line">step 110/125 - loss: 0.6152 - acc: 0.9592 - 91ms/step</span><br><span class="line">step 120/125 - loss: 0.5884 - acc: 0.9595 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.6076 - acc: 0.9595 - 88ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_v1/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6259 - acc: 0.9492 - 86ms/step</span><br><span class="line">step 20/84 - loss: 0.6103 - acc: 0.9500 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.5972 - acc: 0.9516 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.5912 - acc: 0.9543 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.6174 - acc: 0.9533 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5815 - acc: 0.9540 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.6090 - acc: 0.9535 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.5879 - acc: 0.9540 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.6208 - acc: 0.9546 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 7/10</span><br><span class="line">step  10/125 - loss: 0.5885 - acc: 0.9609 - 111ms/step</span><br><span class="line">step  20/125 - loss: 0.5848 - acc: 0.9637 - 98ms/step</span><br><span class="line">step  30/125 - loss: 0.5899 - acc: 0.9648 - 95ms/step</span><br><span class="line">step  40/125 - loss: 0.6268 - acc: 0.9602 - 93ms/step</span><br><span class="line">step  50/125 - loss: 0.5984 - acc: 0.9617 - 92ms/step</span><br><span class="line">step  60/125 - loss: 0.6060 - acc: 0.9624 - 91ms/step</span><br><span class="line">step  70/125 - loss: 0.6193 - acc: 0.9633 - 90ms/step</span><br><span class="line">step  80/125 - loss: 0.6107 - acc: 0.9638 - 90ms/step</span><br><span class="line">step  90/125 - loss: 0.6120 - acc: 0.9636 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.5850 - acc: 0.9638 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.6033 - acc: 0.9646 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.5901 - acc: 0.9643 - 89ms/step</span><br><span class="line">step 125/125 - loss: 0.5671 - acc: 0.9646 - 87ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6079 - acc: 0.9547 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.5994 - acc: 0.9566 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.5997 - acc: 0.9573 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5883 - acc: 0.9590 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.6115 - acc: 0.9583 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.5703 - acc: 0.9592 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.6082 - acc: 0.9585 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.5885 - acc: 0.9590 - 60ms/step</span><br><span class="line">step 84/84 - loss: 0.5901 - acc: 0.9596 - 58ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 8/10</span><br><span class="line">step  10/125 - loss: 0.6017 - acc: 0.9648 - 105ms/step</span><br><span class="line">step  20/125 - loss: 0.5801 - acc: 0.9695 - 97ms/step</span><br><span class="line">step  30/125 - loss: 0.5824 - acc: 0.9693 - 92ms/step</span><br><span class="line">step  40/125 - loss: 0.6109 - acc: 0.9689 - 91ms/step</span><br><span class="line">step  50/125 - loss: 0.5899 - acc: 0.9686 - 90ms/step</span><br><span class="line">step  60/125 - loss: 0.5632 - acc: 0.9691 - 89ms/step</span><br><span class="line">step  70/125 - loss: 0.6070 - acc: 0.9694 - 89ms/step</span><br><span class="line">step  80/125 - loss: 0.5853 - acc: 0.9693 - 89ms/step</span><br><span class="line">step  90/125 - loss: 0.6099 - acc: 0.9687 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.5829 - acc: 0.9694 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5884 - acc: 0.9694 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.6085 - acc: 0.9689 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.5566 - acc: 0.9688 - 88ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6086 - acc: 0.9563 - 89ms/step</span><br><span class="line">step 20/84 - loss: 0.5964 - acc: 0.9582 - 72ms/step</span><br><span class="line">step 30/84 - loss: 0.5983 - acc: 0.9591 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5864 - acc: 0.9605 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6144 - acc: 0.9600 - 63ms/step</span><br><span class="line">step 60/84 - loss: 0.5730 - acc: 0.9604 - 63ms/step</span><br><span class="line">step 70/84 - loss: 0.6029 - acc: 0.9597 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.5849 - acc: 0.9601 - 60ms/step</span><br><span class="line">step 84/84 - loss: 0.5951 - acc: 0.9606 - 58ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 9/10</span><br><span class="line">step  10/125 - loss: 0.5861 - acc: 0.9633 - 110ms/step</span><br><span class="line">step  20/125 - loss: 0.5853 - acc: 0.9652 - 100ms/step</span><br><span class="line">step  30/125 - loss: 0.5744 - acc: 0.9695 - 95ms/step</span><br><span class="line">step  40/125 - loss: 0.5724 - acc: 0.9689 - 93ms/step</span><br><span class="line">step  50/125 - loss: 0.5910 - acc: 0.9702 - 92ms/step</span><br><span class="line">step  60/125 - loss: 0.5868 - acc: 0.9701 - 91ms/step</span><br><span class="line">step  70/125 - loss: 0.5931 - acc: 0.9705 - 91ms/step</span><br><span class="line">step  80/125 - loss: 0.5752 - acc: 0.9715 - 91ms/step</span><br><span class="line">step  90/125 - loss: 0.5677 - acc: 0.9720 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.5638 - acc: 0.9712 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5650 - acc: 0.9715 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.5765 - acc: 0.9715 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.5677 - acc: 0.9713 - 88ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6051 - acc: 0.9578 - 89ms/step</span><br><span class="line">step 20/84 - loss: 0.5957 - acc: 0.9602 - 73ms/step</span><br><span class="line">step 30/84 - loss: 0.5952 - acc: 0.9607 - 68ms/step</span><br><span class="line">step 40/84 - loss: 0.5860 - acc: 0.9625 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.6130 - acc: 0.9616 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.5709 - acc: 0.9620 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.5985 - acc: 0.9615 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.5859 - acc: 0.9617 - 60ms/step</span><br><span class="line">step 84/84 - loss: 0.5923 - acc: 0.9622 - 57ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 10/10</span><br><span class="line">step  10/125 - loss: 0.5898 - acc: 0.9703 - 109ms/step</span><br><span class="line">step  20/125 - loss: 0.5808 - acc: 0.9699 - 96ms/step</span><br><span class="line">step  30/125 - loss: 0.5680 - acc: 0.9706 - 94ms/step</span><br><span class="line">step  40/125 - loss: 0.5849 - acc: 0.9723 - 94ms/step</span><br><span class="line">step  50/125 - loss: 0.5791 - acc: 0.9741 - 94ms/step</span><br><span class="line">step  60/125 - loss: 0.5722 - acc: 0.9742 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.5931 - acc: 0.9735 - 92ms/step</span><br><span class="line">step  80/125 - loss: 0.5773 - acc: 0.9725 - 91ms/step</span><br><span class="line">step  90/125 - loss: 0.5967 - acc: 0.9731 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.5766 - acc: 0.9735 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5773 - acc: 0.9739 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.5805 - acc: 0.9740 - 89ms/step</span><br><span class="line">step 125/125 - loss: 0.5547 - acc: 0.9741 - 88ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6034 - acc: 0.9563 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.5972 - acc: 0.9598 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.5895 - acc: 0.9604 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.5856 - acc: 0.9623 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.6117 - acc: 0.9614 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5724 - acc: 0.9618 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.5927 - acc: 0.9614 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.5870 - acc: 0.9614 - 59ms/step</span><br><span class="line">step 84/84 - loss: 0.5899 - acc: 0.9620 - 57ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_v1/final</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;finetuning/gru/model&#x27;</span>, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="VisualDL-可视化">VisualDL 可视化</h3>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2b39e7bf1322f63a5f0f64c1b107d8ec.png" alt=""></p>
<h3 id="模型评估-2">模型评估</h3>
<ul>
<li>
<p>调用<code>model.evaluate</code>一键评估模型</p>
</li>
<li>
<p>参数：</p>
</li>
</ul>
<!---->
<ul>
<li><code>eval_data</code> (<code>Dataset</code>|<code>DataLoader</code>) - 一个可迭代的数据源，推荐给定一个 <code>paddle.io.Dataset</code> 或 <code>paddle.io.Dataloader</code> 的实例。默认值：None。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results = model.evaluate(dev_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finally test acc: %.5f&quot;</span> % results[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6034 - acc: 0.9563 - 88ms/step</span><br><span class="line">step 20/84 - loss: 0.5972 - acc: 0.9598 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.5895 - acc: 0.9604 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5856 - acc: 0.9623 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6117 - acc: 0.9614 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5724 - acc: 0.9618 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.5927 - acc: 0.9614 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.5870 - acc: 0.9614 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.5899 - acc: 0.9620 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Finally test acc: 0.96196</span><br></pre></td></tr></table></figure>
<h3 id="模型">模型</h3>
<ul>
<li>
<p>调用<code>model.predict</code>进行预测。</p>
</li>
<li>
<p>参数</p>
</li>
</ul>
<!---->
<ul>
<li><code>test_data</code> (<code>Dataset</code>|<code>DataLoader</code>): 一个可迭代的数据源，推荐给定一个<code>paddle.io.Dataset</code> 或 <code>paddle.io.Dataloader</code> 的实例。默认值：None。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="string">&#x27;negative&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;positive&#x27;</span>&#125;</span><br><span class="line">results = model.predict(test_loader, batch_size=<span class="number">64</span>)[<span class="number">0</span>]</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_probs <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># 映射分类label</span></span><br><span class="line">    idx = np.argmax(batch_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    idx = idx.tolist()</span><br><span class="line">    labels = [label_map[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    predictions.extend(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看预测数据前5个样例分类结果</span></span><br><span class="line"><span class="keyword">for</span> idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_ds.data[:<span class="number">5</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Data: &#123;&#125; \t Label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data[<span class="number">0</span>], predictions[idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 42/42 [==============================] - ETA: 3s - 89ms/ste - ETA: 3s - 86ms/ste - ETA: 3s - 96ms/ste - ETA: 3s - 95ms/ste - ETA: 2s - 87ms/ste - ETA: 2s - 83ms/ste - ETA: 2s - 80ms/ste - ETA: 2s - 78ms/ste - ETA: 1s - 75ms/ste - ETA: 1s - 74ms/ste - ETA: 1s - 74ms/ste - ETA: 1s - 70ms/ste - ETA: 1s - 68ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 71ms/ste - ETA: 0s - 71ms/ste - ETA: 0s - 71ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 68ms/ste - ETA: 0s - 66ms/ste - 64ms/step</span><br><span class="line">Predict samples: 5353</span><br><span class="line">Data: 楼面经理服务态度极差，等位和埋单都差，楼面小妹还挺好 	 Label: negative</span><br><span class="line">Data: 欺负北方人没吃过鲍鱼是怎么着？简直敷衍到可笑的程度，团购连青菜都是两人份？！难吃到死，菜色还特别可笑，什么时候粤菜的小菜改成拍黄瓜了？！把团购客人当傻子，可这满大厅的傻子谁还会再来？！ 	 Label: negative</span><br><span class="line">Data: 如果大家有时间而且不怕麻烦的话可以去这里试试，点一个饭等左2个钟，没错！是两个钟！期间催了n遍都说马上到，结果？呵呵。乳鸽的味道，太咸，可能不新鲜吧……要用重口味盖住异味。上菜超级慢！中途还搞什么表演，麻烦有人手的话就上菜啊，表什么演？！？！要大家饿着看表演吗？最后结账还算错单，我真心服了……有一种店叫不会有下次，大概就是指它吧 	 Label: negative</span><br><span class="line">Data: 偌大的一个大厅就一个人点菜，点菜速度超级慢，菜牌上多个菜停售，连续点了两个没标停售的菜也告知没有，粥上来是凉的，榴莲酥火大了，格格肉超级油腻而且咸?????? 	 Label: negative</span><br><span class="line">Data: 泥撕雞超級好吃！！！吃了一個再叫一個還想打包的節奏！ 	 Label: positive</span><br></pre></td></tr></table></figure>
<h2 id="更换三分类数据集进行测试">更换三分类数据集进行测试</h2>
<p>三分类除了涉及到 positive 和 negative 两种情感外，还有一种 neural 情感，从原始数据集中可以提取到有语义转折的句子，“然而”，“但”都是关键词。从而可以得到 3 份不同语义的数据集。</p>
<h3 id="数据准备-3">数据准备</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Classifier3Dataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="built_in">super</span>(Classifier3Dataset, self).__init__()</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_labels</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;2&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">txt_to_list</span>(<span class="params">file_name</span>):</span><br><span class="line">    res_list = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(file_name):</span><br><span class="line">        res_list.append(line.strip().split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> res_list</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Pad, Stack, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> create_dataloader, convert_example</span><br><span class="line"></span><br><span class="line">trainlst = txt_to_list(<span class="string">&#x27;./my_data/train.txt&#x27;</span>)</span><br><span class="line">devlst = txt_to_list(<span class="string">&#x27;./my_data/dev.txt&#x27;</span>)</span><br><span class="line">testlst = txt_to_list(<span class="string">&#x27;./my_data/test.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过get_datasets()函数，将list数据转换为dataset。</span></span><br><span class="line"><span class="comment"># get_datasets()可接收[list]参数，或[str]参数，根据自定义数据集的写法自由选择。</span></span><br><span class="line"><span class="comment"># train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets([&#x27;train&#x27;, &#x27;dev&#x27;, &#x27;test&#x27;])</span></span><br><span class="line">train_ds, dev_ds, test_ds = Classifier3Dataset.get_datasets([trainlst, devlst, testlst])</span><br><span class="line"></span><br><span class="line">label_list = train_ds.get_labels()</span><br><span class="line"><span class="built_in">print</span>(label_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sent, label <span class="keyword">in</span> train_ds[:<span class="number">5</span>]:</span><br><span class="line">    <span class="built_in">print</span> (sent, label)</span><br><span class="line"></span><br><span class="line">tmp = ppnlp.datasets.ChnSentiCorp(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">tmp1 = ppnlp.datasets.MapDatasetWrapper(tmp)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tmp), <span class="built_in">type</span>(tmp1))</span><br><span class="line"><span class="keyword">for</span> sent, label <span class="keyword">in</span> tmp1[:<span class="number">5</span>]:</span><br><span class="line">    <span class="built_in">print</span>(sent, label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reads data and generates mini-batches.</span></span><br><span class="line">trans_fn = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    vocab=vocab,</span><br><span class="line">    unk_token_id=vocab.get(<span class="string">&#x27;[UNK]&#x27;</span>, <span class="number">1</span>),</span><br><span class="line">    is_test=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的数据batch化处理，便于模型batch化运算。</span></span><br><span class="line"><span class="comment"># batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。</span></span><br><span class="line"><span class="comment"># 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.</span></span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=vocab[<span class="string">&#x27;[PAD]&#x27;</span>]),  <span class="comment"># input_ids</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>),  <span class="comment"># seq len</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size改为256</span></span><br><span class="line">train_loader = create_dataloader(</span><br><span class="line">    train_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">dev_loader = create_dataloader(</span><br><span class="line">    dev_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    mode=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">test_loader = create_dataloader(</span><br><span class="line">    test_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    mode=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;]</span><br><span class="line">环境不错，叉烧包小孩爱吃，三色煎糕很一般 2</span><br><span class="line">刚来的时候让我们等位置，我们就在门口等了十分钟左右，没有见到有人离开，然后有工作人员让我们上来二楼，上来后看到有十来桌是没有人的，既然有位置，为什么非得让我们在门口等位！！！本以为有座位后就可以马上吃饭了，让人内心崩溃的是点菜就等了十分钟才有人有空过来理我们。上菜更是郁闷，端来了一锅猪肚鸡，但是没人开火，要点调味料也没人管，服务质量太差，之前来过一次觉得还可以，这次让我再也不想来这家店了！真心失望 0</span><br><span class="line">口味还可以服务真的差到爆啊我来过45次真的次次都只给差评东西确实不错但你看看你们的服务还收服务费我的天干蒸什么的60块比太古汇翠园还贵主要是没人收台没人倒水谁还要自己倒我的天给你服务费还什么都自己干我接受不了钱花了服务没有实在不行 0</span><br><span class="line">出品不错老字号就是好有山有水有树有鱼赞 1</span><br><span class="line">在江南大道这间～服务态度好差，食物出品平凡，应该唔会再去了 1</span><br><span class="line">&lt;class &#x27;paddlenlp.datasets.chnsenticorp.ChnSentiCorp&#x27;&gt; &lt;class &#x27;paddlenlp.datasets.dataset.MapDatasetWrapper&#x27;&gt;</span><br><span class="line">选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般 1</span><br><span class="line">15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错 1</span><br><span class="line">房间太小。其他的都一般。。。。。。。。。 0</span><br><span class="line">1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸，一摸一个印. 4.硬盘分区不好办. 0</span><br><span class="line">今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量商量,单独出个第6卷,让我们的孩子不会有所遗憾。 1</span><br></pre></td></tr></table></figure>
<blockquote>
<p>训练感觉有点过拟合 加大了<code>batch_size 128-&gt;256</code>, <code>hidden_size 96-&gt;128</code> 同时引入了<code>dropout=0.2</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">vocab = load_vocab(<span class="string">&#x27;./senta_word_dict.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model = GRUModel(</span><br><span class="line">        vocab_size=<span class="built_in">len</span>(vocab),</span><br><span class="line">        num_classes=<span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>],</span><br><span class="line">        dropout_rate=<span class="number">0.2</span>,</span><br><span class="line">        fc_hidden_size=<span class="number">128</span>) <span class="comment"># out -&gt; 128 -&gt; 3</span></span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br><span class="line"></span><br><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">        parameters=model.parameters(), learning_rate=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line">loss = paddle.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, loss, metric)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置visualdl路径</span></span><br><span class="line">log_dir = <span class="string">&#x27;./visualdl/gru_3&#x27;</span></span><br><span class="line">callbacks = paddle.callbacks.VisualDL(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints/gru_3&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line">step  10/126 - loss: 1.0978 - acc: 0.3398 - 208ms/step</span><br><span class="line">step  20/126 - loss: 1.0996 - acc: 0.3350 - 181ms/step</span><br><span class="line">step  30/126 - loss: 1.0976 - acc: 0.3408 - 173ms/step</span><br><span class="line">step  40/126 - loss: 1.0953 - acc: 0.3434 - 169ms/step</span><br><span class="line">step  50/126 - loss: 1.0956 - acc: 0.3451 - 165ms/step</span><br><span class="line">step  60/126 - loss: 1.0965 - acc: 0.3449 - 162ms/step</span><br><span class="line">step  70/126 - loss: 1.0983 - acc: 0.3444 - 161ms/step</span><br><span class="line">step  80/126 - loss: 1.0960 - acc: 0.3454 - 160ms/step</span><br><span class="line">step  90/126 - loss: 1.0955 - acc: 0.3456 - 159ms/step</span><br><span class="line">step 100/126 - loss: 1.0925 - acc: 0.3445 - 158ms/step</span><br><span class="line">step 110/126 - loss: 1.0915 - acc: 0.3480 - 158ms/step</span><br><span class="line">step 120/126 - loss: 1.0898 - acc: 0.3595 - 158ms/step</span><br><span class="line">step 126/126 - loss: 1.0895 - acc: 0.3675 - 154ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 1.0886 - acc: 0.5535 - 129ms/step</span><br><span class="line">step 16/16 - loss: 1.0892 - acc: 0.5592 - 117ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 2/10</span><br><span class="line">step  10/126 - loss: 1.0875 - acc: 0.5953 - 200ms/step</span><br><span class="line">step  20/126 - loss: 1.0836 - acc: 0.5965 - 177ms/step</span><br><span class="line">step  30/126 - loss: 1.0774 - acc: 0.5948 - 171ms/step</span><br><span class="line">step  40/126 - loss: 1.0721 - acc: 0.5929 - 166ms/step</span><br><span class="line">step  50/126 - loss: 1.0589 - acc: 0.5895 - 164ms/step</span><br><span class="line">step  60/126 - loss: 1.0572 - acc: 0.5824 - 161ms/step</span><br><span class="line">step  70/126 - loss: 1.0510 - acc: 0.5797 - 161ms/step</span><br><span class="line">step  80/126 - loss: 1.0356 - acc: 0.5745 - 160ms/step</span><br><span class="line">step  90/126 - loss: 0.9759 - acc: 0.5755 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.9620 - acc: 0.5741 - 160ms/step</span><br><span class="line">step 110/126 - loss: 0.9256 - acc: 0.5756 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.9095 - acc: 0.5785 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.8743 - acc: 0.5817 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.8846 - acc: 0.6320 - 138ms/step</span><br><span class="line">step 16/16 - loss: 0.9308 - acc: 0.6308 - 123ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 3/10</span><br><span class="line">step  10/126 - loss: 0.9291 - acc: 0.6621 - 197ms/step</span><br><span class="line">step  20/126 - loss: 0.8573 - acc: 0.6590 - 176ms/step</span><br><span class="line">step  30/126 - loss: 0.9236 - acc: 0.6594 - 168ms/step</span><br><span class="line">step  40/126 - loss: 0.8650 - acc: 0.6684 - 164ms/step</span><br><span class="line">step  50/126 - loss: 0.8187 - acc: 0.6738 - 162ms/step</span><br><span class="line">step  60/126 - loss: 0.8715 - acc: 0.6797 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.8505 - acc: 0.6854 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.8257 - acc: 0.6878 - 159ms/step</span><br><span class="line">step  90/126 - loss: 0.8558 - acc: 0.6905 - 159ms/step</span><br><span class="line">step 100/126 - loss: 0.8403 - acc: 0.6926 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7945 - acc: 0.6958 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.8609 - acc: 0.6980 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.8882 - acc: 0.6992 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.8116 - acc: 0.7141 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8394 - acc: 0.7193 - 119ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 4/10</span><br><span class="line">step  10/126 - loss: 0.7935 - acc: 0.7508 - 231ms/step</span><br><span class="line">step  20/126 - loss: 0.7982 - acc: 0.7494 - 190ms/step</span><br><span class="line">step  30/126 - loss: 0.8068 - acc: 0.7480 - 177ms/step</span><br><span class="line">step  40/126 - loss: 0.7855 - acc: 0.7446 - 172ms/step</span><br><span class="line">step  50/126 - loss: 0.8038 - acc: 0.7469 - 168ms/step</span><br><span class="line">step  60/126 - loss: 0.8076 - acc: 0.7500 - 165ms/step</span><br><span class="line">step  70/126 - loss: 0.7809 - acc: 0.7518 - 165ms/step</span><br><span class="line">step  80/126 - loss: 0.7585 - acc: 0.7547 - 163ms/step</span><br><span class="line">step  90/126 - loss: 0.7997 - acc: 0.7564 - 162ms/step</span><br><span class="line">step 100/126 - loss: 0.8227 - acc: 0.7561 - 161ms/step</span><br><span class="line">step 110/126 - loss: 0.7757 - acc: 0.7562 - 160ms/step</span><br><span class="line">step 120/126 - loss: 0.7927 - acc: 0.7562 - 160ms/step</span><br><span class="line">step 126/126 - loss: 0.7383 - acc: 0.7566 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7797 - acc: 0.7344 - 131ms/step</span><br><span class="line">step 16/16 - loss: 0.8309 - acc: 0.7424 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 5/10</span><br><span class="line">step  10/126 - loss: 0.7490 - acc: 0.7945 - 204ms/step</span><br><span class="line">step  20/126 - loss: 0.7892 - acc: 0.7848 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.7733 - acc: 0.7818 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7219 - acc: 0.7829 - 165ms/step</span><br><span class="line">step  50/126 - loss: 0.7361 - acc: 0.7833 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.7994 - acc: 0.7804 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7618 - acc: 0.7810 - 161ms/step</span><br><span class="line">step  80/126 - loss: 0.7607 - acc: 0.7832 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.7378 - acc: 0.7851 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.7430 - acc: 0.7852 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7676 - acc: 0.7856 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.7475 - acc: 0.7865 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.7938 - acc: 0.7866 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7679 - acc: 0.7441 - 132ms/step</span><br><span class="line">step 16/16 - loss: 0.8193 - acc: 0.7505 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 6/10</span><br><span class="line">step  10/126 - loss: 0.7762 - acc: 0.8020 - 200ms/step</span><br><span class="line">step  20/126 - loss: 0.7412 - acc: 0.8004 - 177ms/step</span><br><span class="line">step  30/126 - loss: 0.7627 - acc: 0.8049 - 169ms/step</span><br><span class="line">step  40/126 - loss: 0.7367 - acc: 0.8074 - 165ms/step</span><br><span class="line">step  50/126 - loss: 0.7610 - acc: 0.8068 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.7663 - acc: 0.8056 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7318 - acc: 0.8050 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.7516 - acc: 0.8081 - 159ms/step</span><br><span class="line">step  90/126 - loss: 0.7567 - acc: 0.8073 - 158ms/step</span><br><span class="line">step 100/126 - loss: 0.7430 - acc: 0.8080 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7549 - acc: 0.8069 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.7199 - acc: 0.8068 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.7319 - acc: 0.8067 - 155ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7777 - acc: 0.7492 - 131ms/step</span><br><span class="line">step 16/16 - loss: 0.8116 - acc: 0.7538 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 7/10</span><br><span class="line">step  10/126 - loss: 0.7169 - acc: 0.8000 - 195ms/step</span><br><span class="line">step  20/126 - loss: 0.7229 - acc: 0.8127 - 176ms/step</span><br><span class="line">step  30/126 - loss: 0.7203 - acc: 0.8156 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7103 - acc: 0.8191 - 166ms/step</span><br><span class="line">step  50/126 - loss: 0.6762 - acc: 0.8223 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.7651 - acc: 0.8215 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7337 - acc: 0.8228 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.7348 - acc: 0.8228 - 160ms/step</span><br><span class="line">step  90/126 - loss: 0.7023 - acc: 0.8231 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.7326 - acc: 0.8229 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7133 - acc: 0.8237 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.7220 - acc: 0.8237 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6864 - acc: 0.8238 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7588 - acc: 0.7543 - 133ms/step</span><br><span class="line">step 16/16 - loss: 0.8104 - acc: 0.7581 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 8/10</span><br><span class="line">step  10/126 - loss: 0.7200 - acc: 0.8258 - 196ms/step</span><br><span class="line">step  20/126 - loss: 0.7397 - acc: 0.8305 - 176ms/step</span><br><span class="line">step  30/126 - loss: 0.7124 - acc: 0.8366 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7328 - acc: 0.8372 - 167ms/step</span><br><span class="line">step  50/126 - loss: 0.7100 - acc: 0.8402 - 164ms/step</span><br><span class="line">step  60/126 - loss: 0.6964 - acc: 0.8393 - 162ms/step</span><br><span class="line">step  70/126 - loss: 0.7164 - acc: 0.8388 - 163ms/step</span><br><span class="line">step  80/126 - loss: 0.7102 - acc: 0.8390 - 165ms/step</span><br><span class="line">step  90/126 - loss: 0.7163 - acc: 0.8385 - 165ms/step</span><br><span class="line">step 100/126 - loss: 0.7125 - acc: 0.8386 - 165ms/step</span><br><span class="line">step 110/126 - loss: 0.7342 - acc: 0.8384 - 164ms/step</span><br><span class="line">step 120/126 - loss: 0.7394 - acc: 0.8379 - 163ms/step</span><br><span class="line">step 126/126 - loss: 0.6978 - acc: 0.8382 - 158ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7574 - acc: 0.7547 - 140ms/step</span><br><span class="line">step 16/16 - loss: 0.8107 - acc: 0.7558 - 127ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 9/10</span><br><span class="line">step  10/126 - loss: 0.6952 - acc: 0.8582 - 208ms/step</span><br><span class="line">step  20/126 - loss: 0.7105 - acc: 0.8520 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.7099 - acc: 0.8492 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7073 - acc: 0.8480 - 167ms/step</span><br><span class="line">step  50/126 - loss: 0.7057 - acc: 0.8480 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.6870 - acc: 0.8474 - 160ms/step</span><br><span class="line">step  70/126 - loss: 0.6980 - acc: 0.8481 - 159ms/step</span><br><span class="line">step  80/126 - loss: 0.6783 - acc: 0.8481 - 158ms/step</span><br><span class="line">step  90/126 - loss: 0.7179 - acc: 0.8467 - 158ms/step</span><br><span class="line">step 100/126 - loss: 0.6865 - acc: 0.8482 - 158ms/step</span><br><span class="line">step 110/126 - loss: 0.6836 - acc: 0.8488 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.7049 - acc: 0.8488 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.7239 - acc: 0.8485 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7657 - acc: 0.7578 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8159 - acc: 0.7608 - 121ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 10/10</span><br><span class="line">step  10/126 - loss: 0.6735 - acc: 0.8539 - 202ms/step</span><br><span class="line">step  20/126 - loss: 0.7280 - acc: 0.8547 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.7055 - acc: 0.8539 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7137 - acc: 0.8554 - 165ms/step</span><br><span class="line">step  50/126 - loss: 0.6825 - acc: 0.8568 - 162ms/step</span><br><span class="line">step  60/126 - loss: 0.7012 - acc: 0.8569 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7151 - acc: 0.8573 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.7342 - acc: 0.8571 - 158ms/step</span><br><span class="line">step  90/126 - loss: 0.6726 - acc: 0.8569 - 158ms/step</span><br><span class="line">step 100/126 - loss: 0.7126 - acc: 0.8577 - 157ms/step</span><br><span class="line">step 110/126 - loss: 0.6896 - acc: 0.8578 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.6932 - acc: 0.8581 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.7185 - acc: 0.8580 - 153ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7604 - acc: 0.7590 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8171 - acc: 0.7626 - 121ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/final</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于acc还在上升 再训练5个epoch</span></span><br><span class="line"></span><br><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">5</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints/gru_3&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/5</span><br><span class="line">step  10/126 - loss: 0.6639 - acc: 0.8621 - 205ms/step</span><br><span class="line">step  20/126 - loss: 0.6666 - acc: 0.8641 - 184ms/step</span><br><span class="line">step  30/126 - loss: 0.6868 - acc: 0.8629 - 178ms/step</span><br><span class="line">step  40/126 - loss: 0.6952 - acc: 0.8665 - 174ms/step</span><br><span class="line">step  50/126 - loss: 0.6786 - acc: 0.8670 - 172ms/step</span><br><span class="line">step  60/126 - loss: 0.6718 - acc: 0.8674 - 172ms/step</span><br><span class="line">step  70/126 - loss: 0.7139 - acc: 0.8652 - 170ms/step</span><br><span class="line">step  80/126 - loss: 0.7146 - acc: 0.8647 - 168ms/step</span><br><span class="line">step  90/126 - loss: 0.6890 - acc: 0.8660 - 167ms/step</span><br><span class="line">step 100/126 - loss: 0.7247 - acc: 0.8655 - 165ms/step</span><br><span class="line">step 110/126 - loss: 0.6748 - acc: 0.8660 - 164ms/step</span><br><span class="line">step 120/126 - loss: 0.6856 - acc: 0.8658 - 163ms/step</span><br><span class="line">step 126/126 - loss: 0.7343 - acc: 0.8657 - 159ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7668 - acc: 0.7523 - 130ms/step</span><br><span class="line">step 16/16 - loss: 0.8106 - acc: 0.7581 - 116ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 2/5</span><br><span class="line">step  10/126 - loss: 0.6910 - acc: 0.8793 - 201ms/step</span><br><span class="line">step  20/126 - loss: 0.6806 - acc: 0.8732 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.6818 - acc: 0.8702 - 168ms/step</span><br><span class="line">step  40/126 - loss: 0.7119 - acc: 0.8700 - 164ms/step</span><br><span class="line">step  50/126 - loss: 0.6921 - acc: 0.8701 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.6909 - acc: 0.8715 - 162ms/step</span><br><span class="line">step  70/126 - loss: 0.7123 - acc: 0.8719 - 162ms/step</span><br><span class="line">step  80/126 - loss: 0.6695 - acc: 0.8720 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.6982 - acc: 0.8723 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.7042 - acc: 0.8723 - 160ms/step</span><br><span class="line">step 110/126 - loss: 0.6810 - acc: 0.8721 - 160ms/step</span><br><span class="line">step 120/126 - loss: 0.7072 - acc: 0.8723 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6921 - acc: 0.8724 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7636 - acc: 0.7559 - 144ms/step</span><br><span class="line">step 16/16 - loss: 0.8205 - acc: 0.7601 - 127ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 3/5</span><br><span class="line">step  10/126 - loss: 0.6746 - acc: 0.8824 - 208ms/step</span><br><span class="line">step  20/126 - loss: 0.6689 - acc: 0.8826 - 183ms/step</span><br><span class="line">step  30/126 - loss: 0.6737 - acc: 0.8811 - 174ms/step</span><br><span class="line">step  40/126 - loss: 0.6970 - acc: 0.8804 - 169ms/step</span><br><span class="line">step  50/126 - loss: 0.6662 - acc: 0.8800 - 165ms/step</span><br><span class="line">step  60/126 - loss: 0.6893 - acc: 0.8801 - 163ms/step</span><br><span class="line">step  70/126 - loss: 0.6819 - acc: 0.8804 - 163ms/step</span><br><span class="line">step  80/126 - loss: 0.6664 - acc: 0.8803 - 162ms/step</span><br><span class="line">step  90/126 - loss: 0.6590 - acc: 0.8793 - 161ms/step</span><br><span class="line">step 100/126 - loss: 0.6681 - acc: 0.8790 - 161ms/step</span><br><span class="line">step 110/126 - loss: 0.6805 - acc: 0.8778 - 160ms/step</span><br><span class="line">step 120/126 - loss: 0.7028 - acc: 0.8781 - 160ms/step</span><br><span class="line">step 126/126 - loss: 0.6371 - acc: 0.8775 - 156ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7640 - acc: 0.7527 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8242 - acc: 0.7586 - 120ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 4/5</span><br><span class="line">step  10/126 - loss: 0.6708 - acc: 0.8879 - 200ms/step</span><br><span class="line">step  20/126 - loss: 0.6879 - acc: 0.8863 - 177ms/step</span><br><span class="line">step  30/126 - loss: 0.6686 - acc: 0.8828 - 168ms/step</span><br><span class="line">step  40/126 - loss: 0.6895 - acc: 0.8804 - 164ms/step</span><br><span class="line">step  50/126 - loss: 0.6907 - acc: 0.8794 - 162ms/step</span><br><span class="line">step  60/126 - loss: 0.6387 - acc: 0.8808 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.6723 - acc: 0.8815 - 162ms/step</span><br><span class="line">step  80/126 - loss: 0.6770 - acc: 0.8814 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.6581 - acc: 0.8829 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.6441 - acc: 0.8826 - 160ms/step</span><br><span class="line">step 110/126 - loss: 0.6712 - acc: 0.8829 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.6454 - acc: 0.8835 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6550 - acc: 0.8836 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7569 - acc: 0.7500 - 139ms/step</span><br><span class="line">step 16/16 - loss: 0.8168 - acc: 0.7573 - 123ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 5/5</span><br><span class="line">step  10/126 - loss: 0.6532 - acc: 0.8828 - 193ms/step</span><br><span class="line">step  20/126 - loss: 0.6534 - acc: 0.8863 - 175ms/step</span><br><span class="line">step  30/126 - loss: 0.6523 - acc: 0.8850 - 169ms/step</span><br><span class="line">step  40/126 - loss: 0.6625 - acc: 0.8868 - 166ms/step</span><br><span class="line">step  50/126 - loss: 0.6752 - acc: 0.8874 - 164ms/step</span><br><span class="line">step  60/126 - loss: 0.6603 - acc: 0.8874 - 163ms/step</span><br><span class="line">step  70/126 - loss: 0.6711 - acc: 0.8879 - 162ms/step</span><br><span class="line">step  80/126 - loss: 0.6867 - acc: 0.8875 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.6505 - acc: 0.8882 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.6640 - acc: 0.8889 - 158ms/step</span><br><span class="line">step 110/126 - loss: 0.6786 - acc: 0.8891 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.6796 - acc: 0.8883 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6795 - acc: 0.8881 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7610 - acc: 0.7520 - 136ms/step</span><br><span class="line">step 16/16 - loss: 0.8282 - acc: 0.7571 - 121ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/final</span><br></pre></td></tr></table></figure>
<h3 id="VisualDL-可视化-2">VisualDL 可视化</h3>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8e024860774ce180feb704bdfd839d0e.png" alt=""></p>
<h3 id="评估-2">评估</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results = model.evaluate(dev_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finally test acc: %.5f&quot;</span> % results[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7610 - acc: 0.7520 - 131ms/step</span><br><span class="line">step 16/16 - loss: 0.8282 - acc: 0.7571 - 117ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Finally test acc: 0.75706</span><br></pre></td></tr></table></figure>
<h3 id="预测">预测</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="string">&#x27;negative&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;positive&#x27;</span>, <span class="number">2</span>:<span class="string">&#x27;neural&#x27;</span>&#125;</span><br><span class="line">results = model.predict(test_loader, batch_size=<span class="number">64</span>)[<span class="number">0</span>]</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_probs <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># 映射分类label</span></span><br><span class="line">    idx = np.argmax(batch_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    idx = idx.tolist()</span><br><span class="line">    labels = [label_map[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    predictions.extend(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看预测数据前5个样例分类结果</span></span><br><span class="line"><span class="keyword">for</span> idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_ds.data[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Data: &#123;&#125; \t Label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data[<span class="number">0</span>], predictions[idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 16/16 [==============================] - ETA: 2s - 191ms/st - ETA: 1s - 162ms/st - ETA: 1s - 155ms/st - ETA: 1s - 146ms/st - ETA: 0s - 144ms/st - ETA: 0s - 141ms/st - ETA: 0s - 138ms/st - 128ms/step</span><br><span class="line">Predict samples: 3986</span><br><span class="line">Data: 出品与环境都算可以吧，服务亦过得去。 	 Label: positive</span><br><span class="line">Data: 菜真是一般般滴，除了贵没啥优点… 	 Label: negative</span><br><span class="line">Data: 第一次是朋友约的喝茶，两层楼都是坐满满客人，我们一行四个人，东西味道很正宗的广式茶点，最爱红米肠，还有乳鸽...网上团购真心觉得划算，在惠福路靠近北京路步行街总觉得东西会很贵，买单时让我感觉很意外四个人才一百多两百，旁边的一些客人也在说好划算哦，因为味道正宗，服务好，性价比高，这就是它为什么现在很多人选择它的原因，朋友家人聚餐的又一个不错的选择... 	 Label: positive</span><br><span class="line">Data: 抱住期待来…有d失望咯…点左bb猪，乳鸽松，上汤豆苗，薄撑，燕窝鹧鸪粥bb猪一般，无好好吃乳鸽松其实不错，不过实在太咸啦！！！！而且好多味精！吃完点饮水都唔够！上汤豆苗d豆苗唔知系乜豆苗，d汤底几好，不过都系咸左小小薄撑边系得得地啊，系唔多得！皮不够烟韧，又唔够脆，特別系在上边的，馅糖太多，花生系咸的！！！！！燕窝粥我无食…环境ok，就系有点嘈。服务一般，不太值10%。总结一句无乜动力令我再帮衬，实在太咸太甜啦！！！！！我宜家仲想买支水一口气喝掉半瓶！！！！！ 	 Label: positive</span><br><span class="line">Data: 点了一个小时的单，还没有上，上了的东西还是生的，跟服务员理论还一脸臭相。差！ 	 Label: negative</span><br><span class="line">Data: 服务贴心。不过鲍鱼没有想象中的那么大，团购划算不少！每人一份感觉挺丰盛的。已经团了两次了，自己觉得好还带家人去尝试。 	 Label: neural</span><br><span class="line">Data: 环境可以就是吃饭时间人比较多，出品比较精致分量男生可能少点 	 Label: positive</span><br><span class="line">Data: 差评！非繁忙时段，两位不可以坐卡座，卡座还有一大片是空着的，二人桌只剩一张而且对着门口不想坐，看到别人两位可以坐卡座，就我们两位不可以坐！差评！果断走！不是只有你一间吃饭的！ 	 Label: negative</span><br><span class="line">Data: 好久都没去过广州塔和珠江边附近走走了，又刚好有个灯光节，在加上抽奖抽中了穿粤传奇的【4D魔幻灯光秀】，就必须要过去看看咯，感受一下灯光节的夜景??不知不觉看完4D魔幻灯光秀都快到九点了，就不如来个宵夜吧。走着走着看到了广州塔下面，有一件叫做赏点点心喝茶的好地方。哈哈…于是我们两个人决定去那吃宵夜咯，增肥的节奏哇！一进去很有广州喝茶的感觉，连装修风格也很像之前茶酒的感觉，好喜欢这种风格哦，不错不错。我们一坐下就看了菜单，看到价格都不是很贵，还是挺实惠的。早上是十一点前埋单，还有得打折呢。我们两个人一共点了五样食物。说真的，他家的出品还真的不错，和我经常去的“点都德”相比来说，他家的份量多点，味道也好点，有一些价格上还便宜先。不管白天还是晚上，喝完茶以后，走走珠江边，看看广州塔，还真是个不错的选择。 	 Label: neural</span><br><span class="line">Data: 晚市点了一条鱼，一份翡翠饺子，一个咖喱牛筋。饺子很一般很一般，鱼很难吃，牛筋少且难吃。两个人花了178没一道菜及格的也是醉了，因为太难吃了到最后菜都没吃完，不知道是不是我们太***丝了到这里都要点上百的菜。反正除了环境好，感觉就只剩招牌了。 	 Label: negative</span><br></pre></td></tr></table></figure>
<h2 id="预训练模型">预训练模型</h2>
<p>近年来随着深度学习的发展，模型参数数量飞速增长，为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分 NLP 任务来说，构建大规模的标注数据集成本过高，非常困难，特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 能够习得通用的语言表示，将预训练模型 Fine-tune 到下游任务，能够获得出色的表现。另外，预训练模型能够避免从零开始训练模型。</p>
<p>参考<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1294333">如何通过预训练模型 Fine-tune 下游任务</a></p>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e" width="60%" height="50%"> <br />
</p>
<br><center>图2：预训练模型一览，图片来源：https://github.com/thunlp/PLMpapers</center></br>
<h3 id="PaddleNLP一键加载预训练模型"><code>PaddleNLP</code>一键加载预训练模型</h3>
<p>情感分析本质是一个文本分类任务，PaddleNLP 对于各种预训练模型已经内置了对于下游任务-文本分类的 Fine-tune 网络。以下教程 ERNIE 为例，介绍如何将预训练模型 Fine-tune 完成文本分类任务。</p>
<ul>
<li>
<p><code>paddlenlp.transformers.ErnieModel()</code></p>
<p>一行代码即可加载预训练模型 ERNIE。</p>
</li>
<li>
<p><code>paddlenlp.transformers.ErnieForSequenceClassification()</code></p>
<p>一行代码即可加载预训练模型 ERNIE 用于文本分类任务的 Fine-tune 网络。<br>
其在 ERNIE 模型后拼接上一个全连接网络（Full Connected）进行分类。</p>
</li>
<li>
<p><code>paddlenlp.transformers.ErnieForSequenceClassification.from_pretrained()</code></p>
<p>只需指定想要使用的模型名称和文本分类的类别数即可完成网络定义。</p>
<p>PaddleNLP 不仅支持 ERNIE 预训练模型，还支持 BERT、RoBERTa、Electra 等预训练模型。</p>
</li>
</ul>
<h3 id="调用ppnlp-transformers-ErnieTokenizer进行数据处理">调用<code>ppnlp.transformers.ErnieTokenizer</code>进行数据处理</h3>
<p>预训练模型 ERNIE 对中文数据的处理是以字为单位。PaddleNLP 对于各种预训练模型已经内置了相应的 tokenizer。指定想要使用的模型名字即可加载对应的 tokenizer。</p>
<p>tokenizer 作用为将原始输入文本转化成模型 model 可以接受的输入数据形式。</p>
<p align="center">
<img src="https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png" hspace='10'/> <br />
</p>
<p align="center">
<img src="https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png" hspace='10'/> <br />
</p>
<br><center>ERNIE模型框架示意图</center></br
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用ernie预训练模型</span></span><br><span class="line"><span class="comment"># ernie</span></span><br><span class="line">model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(<span class="string">&#x27;ernie-1.0&#x27;</span>, num_classes=<span class="number">2</span>)</span><br><span class="line">tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(<span class="string">&#x27;ernie-1.0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ernie-tiny</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.ErnieForSequenceClassification.rom_pretrained(&#x27;ernie-tiny&#x27;,num_classes=2))</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.ErnieTinyTokenizer.from_pretrained(&#x27;ernie-tiny&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用bert预训练模型</span></span><br><span class="line"><span class="comment"># bert-base-chinese</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(&#x27;bert-base-chinese&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(&#x27;bert-base-chinese&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bert-wwm-chinese</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(&#x27;bert-wwm-chinese&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(&#x27;bert-wwm-chinese&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bert-wwm-ext-chinese</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(&#x27;bert-wwm-ext-chinese&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(&#x27;bert-wwm-ext-chinese&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用roberta预训练模型</span></span><br><span class="line"><span class="comment"># roberta-wwm-ext</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(&#x27;roberta-wwm-ext&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># roberta-wwm-ext</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;)</span></span><br><span class="line"></span><br><span class="line">ForSequenceClassification.from_pretrained(<span class="string">&#x27;roberta-wwm-ext&#x27;</span>, num_class=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># roberta-wwm-ext</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2021-02-07 16:58:12,351] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams</span><br><span class="line">[2021-02-07 16:58:14,238] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt</span><br></pre></td></tr></table></figure>
<p>github 源码：<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples/text_classification/pretrained_models">https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples/text_classification/pretrained_models</a></p>
<h1>PaddleNLP 更多项目</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1283423">瞧瞧怎么使用 PaddleNLP 内置数据集-基于 seq2vec 的情感分析</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1294333">如何通过预训练模型 Fine-tune 下游任务</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1317771">使用 BiGRU-CRF 模型完成快递单信息抽取</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1329361">使用预训练模型 ERNIE 优化快递单信息抽取</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1321118">使用 Seq2Seq 模型完成自动对联</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1339888">使用预训练模型 ERNIE-GEN 实现智能写诗</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1290873">使用 TCN 网络完成新冠疫情病例数预测</a></li>
<li><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/projectdetail/1339612">使用预训练模型完成阅读理解</a></li>
</ul>
<h1>加入交流群，一起学习吧</h1>
<p>现在就加入 PaddleNLP 的 QQ 技术交流群，一起交流 NLP 技术吧！</p>
<img src="https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394" width="200" height="250" >
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://example.com">YUAN Tingyi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/06/01/234b53c9dbdf4d4595e8b9f095173105/">http://example.com/2022/06/01/234b53c9dbdf4d4595e8b9f095173105/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/6.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/06/01/1a54430645484c2dbe7ec9d538ede221/"><img class="prev-cover" src="/img/1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">paddle2.0高层API实现ResNet50(十二生肖分类实战)</div></div></a></div><div class="next-post pull-right"><a href="/2022/06/01/63e0d330af4f4b7c818e86e056b93ab8/"><img class="next-cover" src="/img/6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">paddle2.0高层API实现人脸关键点检测(人脸关键点检测综述_自定义网络_paddleHub_趣味ps)</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">YUAN Tingyi</div><div class="author-info__description">XianrenYty</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XianrenYty"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">问题定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">环境介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PaddleNLP-%E5%92%8C-Paddle-%E6%A1%86%E6%9E%B6%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%9F"><span class="toc-number">3.</span> <span class="toc-text">PaddleNLP 和 Paddle 框架是什么关系？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%A3%9E%E6%A1%A8%E5%AE%8C%E6%88%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%80%9A%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">4.</span> <span class="toc-text">使用飞桨完成深度学习任务的通用流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">数据集和数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.</span> <span class="toc-text">自定义数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E6%9F%A5%E7%9C%8B"><span class="toc-number">2.</span> <span class="toc-text">训练数据查看</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">数据处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E9%80%A0-dataloder"><span class="toc-number">4.</span> <span class="toc-text">构造 dataloder</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">模型搭建</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">模型配置和训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE-2"><span class="toc-number">1.</span> <span class="toc-text">模型配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.</span> <span class="toc-text">模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8-VisualDL-%E6%9F%A5%E7%9C%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BB%93%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">启动 VisualDL 查看训练过程可视化结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">4.</span> <span class="toc-text">评估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">预测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9seq2vec%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">修改seq2vec模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-2"><span class="toc-number">1.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">模型建立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE-3"><span class="toc-number">1.3.</span> <span class="toc-text">模型配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VisualDL-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.4.</span> <span class="toc-text">VisualDL 可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-2"><span class="toc-number">1.5.</span> <span class="toc-text">模型评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.6.</span> <span class="toc-text">模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E6%8D%A2%E4%B8%89%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95"><span class="toc-number">2.</span> <span class="toc-text">更换三分类数据集进行测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-3"><span class="toc-number">2.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VisualDL-%E5%8F%AF%E8%A7%86%E5%8C%96-2"><span class="toc-number">2.2.</span> <span class="toc-text">VisualDL 可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0-2"><span class="toc-number">2.3.</span> <span class="toc-text">评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">2.4.</span> <span class="toc-text">预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PaddleNLP%E4%B8%80%E9%94%AE%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">PaddleNLP一键加载预训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E7%94%A8ppnlp-transformers-ErnieTokenizer%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">调用ppnlp.transformers.ErnieTokenizer进行数据处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">PaddleNLP 更多项目</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number"></span> <span class="toc-text">加入交流群，一起学习吧</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/fd61228310d24c999c625f49b503f7ce/" title="train_test_split 参数详解"><img src="/img/8.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="train_test_split 参数详解"/></a><div class="content"><a class="title" href="/2022/06/01/fd61228310d24c999c625f49b503f7ce/" title="train_test_split 参数详解">train_test_split 参数详解</a><time datetime="2022-06-01T12:52:00.000Z" title="Created 2022-06-01 20:52:00">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/224bdd4444be4fe0b7826959bc6b40a4/" title="StandardScaler(sklearn)参数详解"><img src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="StandardScaler(sklearn)参数详解"/></a><div class="content"><a class="title" href="/2022/06/01/224bdd4444be4fe0b7826959bc6b40a4/" title="StandardScaler(sklearn)参数详解">StandardScaler(sklearn)参数详解</a><time datetime="2022-06-01T12:51:39.966Z" title="Created 2022-06-01 20:51:39">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/2fd9e712ce6b4a49a24775586e47e44b/" title="八大排序算法(Python实现)"><img src="/img/6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="八大排序算法(Python实现)"/></a><div class="content"><a class="title" href="/2022/06/01/2fd9e712ce6b4a49a24775586e47e44b/" title="八大排序算法(Python实现)">八大排序算法(Python实现)</a><time datetime="2022-06-01T12:51:26.086Z" title="Created 2022-06-01 20:51:26">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/9d8547900901416593ba964e6757c6c3/" title="ResNet"><img src="/img/2.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ResNet"/></a><div class="content"><a class="title" href="/2022/06/01/9d8547900901416593ba964e6757c6c3/" title="ResNet">ResNet</a><time datetime="2022-06-01T12:50:56.055Z" title="Created 2022-06-01 20:50:56">2022-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/01/b6a6cbce3d7f45a8b2511edf86fb5a24/" title="Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）"><img src="/img/8.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）"/></a><div class="content"><a class="title" href="/2022/06/01/b6a6cbce3d7f45a8b2511edf86fb5a24/" title="Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）">Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）</a><time datetime="2022-06-01T12:48:23.363Z" title="Created 2022-06-01 20:48:23">2022-06-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By YUAN Tingyi</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://example.com/2022/06/01/234b53c9dbdf4d4595e8b9f095173105/'
    this.page.identifier = '2022/06/01/234b53c9dbdf4d4595e8b9f095173105/'
    this.page.title = 'paddle2.0高层API实现自定义数据集文本分类中的情感分析任务'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Valine' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>