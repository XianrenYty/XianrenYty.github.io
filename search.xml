<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>网络层其他协议</title>
    <url>/2022/05/31/11e941d5517240bd959c71ad81279c28/</url>
    <content><![CDATA[<p>[toc]</p>
<p>网络层不仅有 IP 协议，还有其它如 ARP、ICMP、IGMP、RARP 等其它协议，这一节我们将对这些协议做介绍。<br>
<img src="/resource/6879294ab7484faf885e5dabf521f667.png" alt="2faba7050549ac71713adddd875cde16.png"></p>
<h2 id="ARP-Address-Resolution-Protocol-地址解析协议">ARP(Address Resolution Protocol)地址解析协议</h2>
<h3 id="功能">功能</h3>
<p>当主机通过数据链路发送数据的时候，IP 数据报会先被封装为一个数据帧，而 MAC 地址会被添加到数据帧的报头（链路层介绍时已讲过）。</p>
<p>ARP 便是在这个过程中通过目标主机的 IP 地址，查询目标主机的 MAC 地址。</p>
<h3 id="原理">原理</h3>
<p>在你的电脑和路由器中都有一个 ARP 缓存表，其中保存的是近期(20 分钟)与自己有过通信的主机的 IP 地址与 MAC 地址的对应关系。</p>
<p>ARP 缓存表使用过程：</p>
<ul>
<li>当主机要发送一个 IP 数据报的时候，会首先查询一下自己的 ARP 缓存表；</li>
<li>如果在 ARP 缓存表中找到对应的 MAC 地址，则将 IP 数据报封装为数据帧，把 MAC 地址放在帧首部，发送数据帧；</li>
<li>如果查询的 IP－MAC 值对不存在，那么主机就向网络中广播发送一个 ARP 请求数据帧，ARP 请求中包含待查询 IP 地址；</li>
<li>网络内所有收到 ARP 请求的主机查询自己的 IP 地址，如果发现自己符合条件，就回复一个 ARP 应答数据帧，其中包含自己的 MAC 地址；</li>
<li>收到 ARP 应答后，主机将其 IP - MAC 对应信息存入自己的 ARP 缓存，然后再据此封装 IP 数据报，再发送数据帧。</li>
</ul>
<h3 id="ARP-代理">ARP 代理</h3>
<p>如果 ARP 请求是从一个网络上的主机发往另一个网络上的主机，那么连接这两个网络的路由器就可以回答该 ARP 请求，这个过程称作代理 ARP（Proxy ARP）。</p>
<p>当连接这两个网络的路由器收到该 ARP 请求时，它会发现自己有通向目的主机的路径，随后它会将自己（路由器）的 MAC 地址回复给源主机。源主机会认为路由器的 MAC 地址就是目的主机的 MAC 地址，而对于随后发来的数据帧，路由器会转发到它后面真实 MAC 地址的目的主机。</p>
<p>两个物理网络之间的路由器可以使这两个网络彼此透明化，在这种情况下，只要路由器设置成一个 ARP 代理，以响应一个网络到另一个网络主机的 ARP 请求，两个物理网络就可以使用相同的网络号。</p>
<h3 id="ARP-欺骗">ARP 欺骗</h3>
<p>从 ARP 代理的原理可以看出来：IP - MAC 的对应信息很容易被伪造！黑客可以伪造 ARP 应答数据帧而欺骗 ARP 请求者，从而达到截获数据的目的。</p>
<h2 id="RARP-Reverse-Address-Resolution-Protocol-逆向地址解析协议">RARP(Reverse Address Resolution Protocol)逆向地址解析协议</h2>
<p>听名字就知道，RARP 与 ARP 是相反的关系，用于将 MAC 地址转换为 IP 地址。对应于 ARP，RARP 请求以广播方式传送，而 RARP 应答一般是单播传送的。</p>
<p>某些设备，比如无盘机在启动时可能不知道自己的 IP 地址，它们可以将自己的 MAC 地址使用 RARP 请求广播出去，RARP 服务器就会响应并回复无盘机的 IP 地址。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RARP 在目前的应用中已极少被使用，不再赘述了。</span><br></pre></td></tr></table></figure>
<h2 id="ICMP-Internet-Control-Message-Protocol-控制报文协议">ICMP(Internet Control Message Protocol)控制报文协议</h2>
<p>通信过程中发生各种问题时，ICMP 将问题反馈，通过这些信息，管理者可以对所发生的问题作出诊断，然后采取适当的措施去解决它。</p>
<p>ICMP 报文由 8 位错误类型, 8 位条件代码和 16 位校验和组成，被封装在一个 IP 数据报中：</p>
<p><img src="/resource/5ee19c00ab97416584b913a01176c4d2.png" alt="7b79665560df207e3488241a8da6f657.png"></p>
<p>报文的类型字段可以有 15 个不同的值，以便描述特定类型的 ICMP 报文，代码字段的值进一步描述不同的条件，各报文类型描述可参考 <a href="https://baike.baidu.com/item/ICMP#7">ICMP_百度百科</a></p>
<p>也有一些出现差错而不产生 ICMP 报文的情况，比如：</p>
<ul>
<li>ICMP 差错报文；</li>
<li>目的地址是广播或多播地址；</li>
<li>作为链路层广播的数据报；</li>
<li>不是 IP 分片的第一片；</li>
<li>源地址不是单个主机的数据报（源不能为零地址、环回地址、广播多播地址）。</li>
</ul>
<h2 id="ping-程序">ping 程序</h2>
<p><code>ping</code> 程序和 <code>traceroute</code> 程序是两个常见的基于 <strong>ICMP 协议</strong>的工具。</p>
<h3 id="ping-简介">ping 简介</h3>
<p>ping 程序是对两台主机之间连通性进行测试的基本工具，它只是利用 <strong>ICMP 回显请求</strong>和<strong>回显应答报文</strong>，而不用经过传输层（TCP/UDP）。</p>
<p>ping 程序通过在 ICMP 报文数据中存放发送请求的时间值来计算往返时间，当应答返回时，用当前时间减去存放在 ICMP 报文中的时间值，即是<strong>往返时间</strong>。</p>
<p>ping 程序使用方法为 <code>ping IP 地址</code>，ping 命令还可以加上参数，实现更多的功能：</p>
<ul>
<li>-n 只输出数值；</li>
<li>-q 不显示任何传送封包的信息，只显示最后的结果；</li>
<li>-r 忽略普通的 Routing Table，直接将数据包送到远端主机上，通常是查看本机的网络接口是否有问题；</li>
<li>-R 记录路由过程；</li>
<li>-v 详细显示指令的执行过程；</li>
<li>-c 数目：在发送指定数目的包后停止；</li>
<li>-i 秒数：设定间隔几秒传送一个网络封包给一台机器，预设值是一秒送一次；</li>
<li>-t 存活数值：设置存活数值 TTL 的大小。</li>
</ul>
<p>下面使用 ping 向 <code>mirrors.cloud.aliyuncs.com</code> 发送 <code>3</code> 个数据包。<br>
<img src="/resource/9ac85004fc074d9198a05774cfda6f4a.png" alt="273d3bd6cb3bc57a84811f27d829c522.png"></p>
<p>下面我们用 tcpdump 命令查看 ping 命令包结构。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo tcpdump -nnvXSs 0 -c2 icmp</span><br></pre></td></tr></table></figure>
<p>此时还没有数据包信息，我们需要新开一个终端，执行 <code>ping -c1 mirrors.cloud.aliyuncs.com</code> 就能看到以下输出：<br>
<img src="/resource/9907b826e6c5467db597713362d3ceca.png" alt="5e5441e2a8d2b22af2b92f18c80618ad.png"></p>
<p>第一个数据报就是 <strong>ICMP 回显请求报文</strong>，是主机 192.168.42.3 发送给 100.100.2.148 的，第二个数据报是 <strong>ICMP 回显应答报文</strong>，是 100.100.2.148 发给本机 192.168.42.3 的。</p>
<h3 id="TTL-值">TTL 值</h3>
<p>TTL 是 Time To Live 的缩写，该字段指定 IP 包被路由器丢弃之前允许通过的最大网段数量。可以去回顾一下上节的 IP 报文结构图。</p>
<p>TTL 是 IPv4 包头的一个 8 bit 字段，它的作用是限制 IP 数据包在计算机网络中的存在时间，即 IP 数据包在计算机网络中可以转发的最大条数。</p>
<p>假如没有 TTL 字段，网络中的 IP 包将越来越多造成网络阻塞，TTL 避免 IP 包在网络中的无限循环和收发，节省了网络资源，并能使 IP 包的发送者能收到告警消息。</p>
<h3 id="Ping-命令判断操作系统">Ping 命令判断操作系统</h3>
<p>ping 命令会返回一个 TTL 值，我们可以使用它来判断目标的操作系统类型。 常见操作系统缺省 TTL 值如下:</p>
<ul>
<li>UNIX TTL: 255；</li>
<li>Linux TTL: 64；</li>
<li>WINDOWS 95/98 TTL: 32；</li>
<li>Windows NT 4.0/2000/XP/2003/7/8/10 TTL：128。</li>
</ul>
<h2 id="traceroute-程序">traceroute 程序</h2>
<p><code>traceroute</code> 程序是用来侦测主机到目的主机之间所经路由情况的重要工具。刚才 ping 程序中讲过，带 <code>-R</code> 参数的 ping 命令也可以记录路由过程，但是因为 IP 数据报头的长度限制(最多能保存 9 个 IP 地址)，ping 不能完全的记录下所经过的路由器，<code>traceroute</code> 正好就填补了这个缺憾。</p>
<p><strong>traceroute 工作原理</strong></p>
<ol>
<li>它发送一份 TTL 为 1 的 IP 数据报给目的主机，经过第一个路由器时，TTL 值被减为 0，则第一个路由器丢弃该数据报，并返回一份超时 ICMP 报文，于此得到了路径中第一个路由器的地址；</li>
<li>然后再发送一份 TTL 值为 2 的数据报，便可得到第二个路由器的地址；</li>
<li>以此类推，一直到到达目的主机为止，这样便记录下了路径上所有的路由 IP。</li>
</ol>
<h2 id="IGMP-Internet-Group-Management-Protocol-组管理协议">IGMP(Internet Group Management Protocol)组管理协议</h2>
<p>IGMP 是用于管理多播组成员的一种协议，它的作用在于：让其它所有需要知道自己处于哪个多播组的主机和路由器知道自己的状态。只要某一个多播组还有一台主机，多播路由器就会把数据传输出去，这样接受方就会通过网卡过滤功能来得到自己想要的数据。</p>
<p>为了知道多播组的信息，多播路由器需要定时的发送 IGMP 查询，各个多播组里面的主机要根据查询来回复自己的状态。路由器来决定有几个多播组，自己要对某一个多播组发送什么样的数据。</p>
]]></content>
  </entry>
  <entry>
    <title>应用层协议</title>
    <url>/2022/05/31/16f0aa85af874c64a3a8f176c81d2eac/</url>
    <content><![CDATA[<p>[toc]</p>
<p><strong>应用层协议</strong>（application layer protocol）定义了运行在不同端系统上的应用程序进程如何相互传递报文。接下来我们就来学习应用层上的一些协议。</p>
<p>在传输层之上，便是<strong>应用层</strong>。传输层的 UDP 报文和 TCP 报文段的数据部分就是应用层交付的数据。</p>
<p>不同类型的网络应用有不同的通信规则，因此应用层协议是多种多样的，比如 DNS、FTP、Telnet、SMTP、HTTP、RIP、NFS 等协议都是用于解决其各自的一类问题。</p>
<h2 id="DNS-协议">DNS 协议</h2>
<p>DNS（Domain Name Service 域名服务）协议基于 UDP，使用<strong>端口号 53</strong>。</p>
<p>由数字组成的 IP 地址很难记忆，所以我们上网使用网站 IP 地址的别名——<strong>域名</strong>。实际使用中，域名与 IP 地址是对应的，这种对应关系保存在 DNS 服务器之中。</p>
<p>在浏览器中输入一个域名后，会有 DNS 服务器将域名解析为对应的 IP 地址。注意这和网络层的 ARP 协议的不同之处：DNS 提供的是域名与 IP 地址的对应关系，而 ARP 提供的是 IP 地址和 MAC 地址的对应关系。</p>
<h3 id="DNS-服务器">DNS 服务器</h3>
<p>DNS 服务器是个分层次的系统：</p>
<ul>
<li><strong>根 DNS 服务器</strong> ：全世界共有 13 台根域名服务器，编号 A 到 M，其中大部分位于美国。</li>
<li><strong>顶级(TLD)DNS 服务器</strong> ：负责如 com、org、edu 等顶级域名和所有国家的顶级域名（如 cn 、uk 、jp）。</li>
<li><strong>权威 DNS 服务器</strong> ：大型组织、大学、企业的域名解析服务。</li>
<li><strong>本地 DNS 服务器</strong> ：通常与我们主机最近的 DNS 服务器。</li>
</ul>
<p>而域名解析的过程，有<strong>迭代</strong>查询和<strong>递归</strong>查询两种方式：<br>
<img src="/resource/9c0faa36afe34603910c7963e8db4100.png" alt="16d01638e91a67e6b316020c5962ea85.png"></p>
<h3 id="host-命令">host 命令</h3>
<p>在 linux 系统中，可以用 host 命令进行 DNS 查询，查看一个指定域名的 IP，比如要查询 <a href="http://mirrors.aliyuncs.com">mirrors.aliyuncs.com</a> 的 IP 地址：<br>
<img src="/resource/6be41f6db0874057b093dce2bea7fb9d.png" alt="3d73dec68b83b38102ab9d66df73f8fc.png"></p>
<h3 id="DNS-报文">DNS 报文</h3>
<p>主机向 DNS 服务器发出的查询叫做<strong>DNS 报文</strong>，问答报文的内容，都是 IP 和域名的对应信息，问题中包含域名，类型，类信息，回答中包含指针，类型，类，TTL，长度，IP 地址信息。</p>
<h3 id="DNS-缓存和-hosts-文件">DNS 缓存和 hosts 文件</h3>
<p>之前 DNS 解析查询过程的图中，共发出了 8 份 DNS 报文，这是非常消耗时间的，所以实际应用上使用 DNS 缓存 ：当一个 DNS 服务器接收到一个 DNS 回答后，会将其信息缓存一段时间，当再有一个对相同域名的查询时，便可直接回复。</p>
<p>通过 DNS 缓存，其实很多查询都只需要本地 DNS 服务器便可完成。</p>
<h3 id="FTP-协议">FTP 协议</h3>
<p>FTP（File Transfer Protocol 文件传输协议）基于 TCP，使用端口号 <strong>20（数据）</strong> 和 <strong>21（控制）</strong>。</p>
<p>它的主要功能是减少或消除在不同操作系统下处理文件的不兼容性，以达到便捷高效的文件传输效果。</p>
<ul>
<li>FTP 只提供文件传输的基本服务，它采用 客户端—服务器 的方式，一个 FTP 服务器可同时为多个客户端提供服务。</li>
<li>在进行文件传输时，FTP 的客户端和服务器之间会建立两个 TCP 连接：21 号端口建立<strong>控制连接</strong>，20 号端口建立<strong>数据连接</strong>。</li>
<li>FTP 的传输有两种方式：ASCII 传输模式和二进制数据传输模式。</li>
</ul>
<p><strong>示例：</strong><br>
前面三帧是客户端与服务器的三次握手，连接服务端的 ftp 端口（21），客户端 ip 是 192.168.0.114，服务端 ip 是 192.168.0.193：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">01:24:40.499548 IP 192.168.0.114.1137 &gt; 192.168.0.193.ftp: Flags [S], <span class="built_in">seq</span> 3753095934, win 16384, options [mss 1460,nop,nop,sackOK], length 0</span><br><span class="line">01:24:40.501867 IP 192.168.0.193.ftp &gt; 192.168.0.114.1137: Flags [S.], <span class="built_in">seq</span> 3334930753, ack 3753095935, win 16384, options [mss 1452,nop,nop,sackOK], length 0</span><br><span class="line">01:24:40.501886 IP 192.168.0.114.1137 &gt; 192.168.0.193.ftp: Flags [.], ack 1, win 17424, length 0</span><br></pre></td></tr></table></figure>
<p>第四帧是服务器向客户端发送相关信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">01:24:40.503947 IP 192.168.0.193.ftp &gt; 192.168.0.114.1137: Flags [P.], <span class="built_in">seq</span> 1:31, ack 1, win 65535, length 30</span><br></pre></td></tr></table></figure>
<p>此时控制连接建立完成。再下面的部分就是客户端向服务器端发送请求，然后服务器端响应客户端的过程，比如登录，输出所在路径等操作。</p>
<p>在后面的输出中，还能找到客户端与服务器端建立数据连接时三次握手的过程。</p>
<p>我们这里看到服务器端开放端口号不是 20 是因为 FTP 的工作模式是<strong>被动模式（PASV）</strong>，被动模式中，服务端会创建一个新的随机的非特权端口 P（P&gt; = 1023）与客户端建立数据通道连接。</p>
<h2 id="HTTP-协议">HTTP 协议</h2>
<p><strong>HTTP (HyperText Transfer Protocol 超文本传输协议)</strong> 基于 TCP，使用<strong>端口号 80</strong> 或 <strong>8080</strong>。</p>
<p>每当你在浏览器里输入一个网址或点击一个链接时，浏览器就通过 HTTP 协议将网页信息从服务器提取再显示出来，这是现在使用频率最大的应用层协议。</p>
<p>这个原理很简单：</p>
<ul>
<li>点击一个链接后，浏览器向服务器发起 TCP 连接；</li>
<li>连接建立后浏览器发送 HTTP 请求报文，然后服务器回复响应报文；</li>
<li>浏览器将收到的响应报文内容显示在网页上；</li>
<li>报文收发结束，关闭 TCP 连接。</li>
</ul>
<p>HTTP 报文会被传输层封装为 TCP 报文段，然后再被 IP 层封装为 IP 数据报。</p>
<p>HTTP 请求报文结构：<br>
<img src="/resource/5f40f38b659043fcbb466cee15fc7c0c.png" alt="bdae0479dd5227b75b91b2b3375c4446.png"></p>
<p>HTTP 响应报文结构：<br>
<img src="/resource/dce51e6786e34e1d9dfd12f1926305af.png" alt="4964e04f07cee4310f5b5b79682b94d5.png"></p>
<p>可见报文分为 3 部分：</p>
<ul>
<li>开始行：用于区分是请求报文还是响应报文，请求报文中开始行叫做请求行，而响应报文中，开始行叫做状态行。在开始行的三个字段之间都用空格分开，结尾处 CRLF 表示回车和换行。</li>
<li>首部行：用于说明浏览器、服务器或报文主体的一些信息。</li>
<li>实体主体：请求报文中通常不用实体主体。</li>
</ul>
<p>请求报文的方法字段是对所请求对象进行的操作，而响应报文的状态码是一个 3 位数字，分为 5 类 33 种：</p>
<ul>
<li>1xx：通知信息，如收到或正在处理。</li>
<li>2xx：成功接收。</li>
<li>3xx：重定向。</li>
<li>4xx：客户的差错，如 404 表示网页未找到。</li>
<li>5xx：服务器的差错，如常见的 502 Bad Gateway。</li>
</ul>
<h2 id="Telnet-协议">Telnet 协议</h2>
<h3 id="Telnet-简介">Telnet 简介</h3>
<p>Telnet 协议是 TCP/IP 协议族中的一员，是 Internet 远程登录服务的标准协议和主要方式，它基于 TCP 协议，使用<strong>端口 23</strong>。</p>
<p>终端使用者在本地电脑上使用 telnet 程序，用它连接到服务器，终端使用者可以在 telnet 程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样。</p>
<h3 id="Telnet-工作过程">Telnet 工作过程</h3>
<p>使用 Telnet 协议进行远程登录时必须满足以下条件:</p>
<ul>
<li>在本地计算机上必须装有包含 Telnet 协议的客户程序；</li>
<li>必须知道远程主机的 IP 地址或域名；</li>
<li>必须知道登录标识与口令。</li>
</ul>
<p>Telnet 远程登录服务分为以下 4 个过程:</p>
<ol>
<li>本地与远程主机建立连接。该过程实际上是建立一个 TCP 连接，用户必须知道远程主机的 IP 地址或域名；</li>
<li>将本地终端上输入的用户名和口令及以后输入的任何命令或字符以 NVT ( Net Virtual Terminal ) 格式传送到远程主机。该过程实际上是从本地主机向远程主机发送一个 IP 数据包；</li>
<li>将远程主机输出的 NVT 格式的数据转化为本地所接受的格式送回本地终端，包括输入命令回显和命令执行结果；</li>
<li>最后，本地终端对远程主机进行撤消连接。该过程是撤销一个 TCP 连接。</li>
</ol>
<h3 id="Telnet-测试主机端口">Telnet 测试主机端口</h3>
<p>telnet 可以测试目标机器的 TCP 端口是否开放。</p>
<p>例如 <code>telnet IP地址 3389</code> 是用来测试目标机器的 3389 端口是否开放，如果连接失败，可能是以下原因：</p>
<ul>
<li>防火墙屏蔽</li>
<li>目标机器没有启用相关远程桌面服务（windows）</li>
<li>修改了默认占用 3389 端口。</li>
</ul>
<h2 id="TFTP-协议">TFTP 协议</h2>
<h3 id="TFTP-简介">TFTP 简介</h3>
<p>TFTP（ Trivial File Transfer Protocol ）是 TCP/IP 协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务，它基于 UDP 协议，使用<strong>端口 69</strong> 。</p>
<p>此协议设计的时候是进行小文件传输的，与 FTP 相比少了许多功能，它只能从文件服务器上<strong>获得或写入文件</strong>，不能列出目录，不进行认证。</p>
<p>TFTP 也有着它自身的优点：</p>
<ul>
<li>TFTP 可用于 UDP 环境；比如当需要将程序或者文件同时向许多机器下载时就往往需要使用到 TFTP 协议。</li>
<li>TFTP 代码所占的内存较小，这对于小型计算机或者某些特殊用途的设备来说是很重要的，TFTP 具有更多的灵活性，也减少了开销。</li>
</ul>
<h2 id="SMTP-协议和-POP3-协议">SMTP 协议和 POP3 协议</h2>
<h3 id="SMTP-简介">SMTP 简介</h3>
<p>SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议，它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式,它使用 TCP 协议，使用<strong>端口 25</strong>。</p>
<p>SMTP 存在两个端：</p>
<ul>
<li>在发信人的邮件服务器上执行的客户端；</li>
<li>在收信人的邮件服务器上执行的服务器端。</li>
</ul>
<p>SMTP 的客户端和服务器端同时运行在每个邮件服务器上。当一个邮件服务器在向其它邮件服务器发送邮件消息时，它是作为 SMTP 客户在运行。</p>
<h3 id="SMTP-的连接和发送过程">SMTP 的连接和发送过程</h3>
<ol>
<li>建立 TCP 连接</li>
<li>客户端向服务器发送 HELO 命令以标识发件人自己的身份，然后客户端发送 MAIL 命令</li>
<li>服务器端以 OK 作为响应，表示准备接收</li>
<li>客户端发送 RCPT 命令</li>
<li>服务器端表示是否愿意为收件人接收邮件</li>
<li>协商结束，发送邮件，用命令 DATA 发送输入内容</li>
<li>结束此次发送，用 QUIT 命令退出</li>
</ol>
<p><a href="https://blog.csdn.net/kerry0071/article/details/28604267">SMTP 协议详解及工作过程</a></p>
<h3 id="POP3-简介">POP3 简介</h3>
<p>POP3（Post Office Protocol Version 3 ）即<strong>邮局协议版本 3</strong>，是 TCP/IP 协议族中的一员 ，主要用于支持使用客户端远程管理在服务器上的电子邮件，使用 <strong>TCP 协议</strong>，使用<strong>端口 110</strong> 。</p>
<p>POP3 邮件服务器大都可以“只下载邮件，服务器端并不删除”，也就是改进的 POP3 协议。</p>
<h3 id="POP3-工作过程">POP3 工作过程</h3>
<ol>
<li>用户运行用户代理（如 Foxmail, Outlook Express）</li>
<li>用户代理（以下简称客户端）与邮件服务器（以下简称服务器端）的 110 端口建立 TCP 连接</li>
<li>客户端向服务器端发出各种命令，来请求各种服务（如查询邮箱信息，下载某封邮件等）</li>
<li>服务端解析用户的命令，做出相应动作并返回给客户端一个响应</li>
<li>上述的两个步骤交替进行，直到接收完所有邮件转到下一步，或两者的连接被意外中断而直接退出</li>
<li>用户代理解析从服务器端获得的邮件，以适当地形式（如可读）的形式呈现给用户</li>
</ol>
<p><a href="https://www.cnblogs.com/foxmin/archive/2011/10/16/2214425.html">邮件服务器之 POP3 协议分析</a></p>
<h3 id="POP3-和-SMTP-协同工作">POP3 和 SMTP 协同工作</h3>
<p>一封邮件的发送过程：<br>
<img src="/resource/7d6f373ae0ea4da18cb2f68ca9165813.png" alt="69cfcabd31be6ac6bb7e2c9ea638ae62.png"></p>
<ol>
<li>通过 smtp 协议连接到 smtp 服务器，然后发一封邮件给 sohu 的 smtp 服务器；</li>
<li>通过 smtp 协议将邮件转投给 sina 的 smtp 服务器（邮件发送服务器）；</li>
<li>将接收到的邮件存储到 <a href="mailto:gacl@sina.com">gacl@sina.com</a> 这个邮件账号分配的存储空间中；</li>
<li>通过 POP3 协议连接到 POP3 服务器收取邮件；</li>
<li>从 <a href="mailto:gacl@sina.com">gacl@sina.com</a> 账号的存储空间当中取出邮件；</li>
<li>POP3 服务器将取出来的邮件回送给 <a href="mailto:gacl@sina.com">gacl@sina.com</a> 账户。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>paddle2.0高层API实现ResNet50(十二生肖分类实战)</title>
    <url>/2022/06/01/1a54430645484c2dbe7ec9d538ede221/</url>
    <content><![CDATA[<p>[toc]</p>
<p>『深度学习 7 日打卡营·快速入门特辑』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/67c2cc646d638522a2c7a8b82bd0bf73.png" alt=""></p>
<h1>① 问题定义</h1>
<p>十二生肖分类的本质是图像分类任务，我们采用 CNN 网络结构进行相关实践。</p>
<h1>② 数据准备</h1>
<h2 id="2-1-解压缩数据集">2.1 解压缩数据集</h2>
<p>我们将网上获取的数据集以压缩包的方式上传到 aistudio 数据集中，并加载到我们的项目内。</p>
<p>在使用之前我们进行数据集压缩包的一个解压。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!unzip -q -o data/data68755/signs.<span class="built_in">zip</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-数据标注">2.2 数据标注</h2>
<p>我们先看一下解压缩后的数据集长成什么样子。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── <span class="built_in">test</span></span><br><span class="line">│   ├── dog</span><br><span class="line">│   ├── dragon</span><br><span class="line">│   ├── goat</span><br><span class="line">│   ├── horse</span><br><span class="line">│   ├── monkey</span><br><span class="line">│   ├── ox</span><br><span class="line">│   ├── pig</span><br><span class="line">│   ├── rabbit</span><br><span class="line">│   ├── ratt</span><br><span class="line">│   ├── rooster</span><br><span class="line">│   ├── snake</span><br><span class="line">│   └── tiger</span><br><span class="line">├── train</span><br><span class="line">│   ├── dog</span><br><span class="line">│   ├── dragon</span><br><span class="line">│   ├── goat</span><br><span class="line">│   ├── horse</span><br><span class="line">│   ├── monkey</span><br><span class="line">│   ├── ox</span><br><span class="line">│   ├── pig</span><br><span class="line">│   ├── rabbit</span><br><span class="line">│   ├── ratt</span><br><span class="line">│   ├── rooster</span><br><span class="line">│   ├── snake</span><br><span class="line">│   └── tiger</span><br><span class="line">└── valid</span><br><span class="line">    ├── dog</span><br><span class="line">    ├── dragon</span><br><span class="line">    ├── goat</span><br><span class="line">    ├── horse</span><br><span class="line">    ├── monkey</span><br><span class="line">    ├── ox</span><br><span class="line">    ├── pig</span><br><span class="line">    ├── rabbit</span><br><span class="line">    ├── ratt</span><br><span class="line">    ├── rooster</span><br><span class="line">    ├── snake</span><br><span class="line">    └── tiger</span><br></pre></td></tr></table></figure>
<p>数据集分为 train、valid、test 三个文件夹，每个文件夹内包含 12 个分类文件夹，每个分类文件夹内是具体的样本图片。</p>
<p>我们对这些样本进行一个标注处理，最终生成 train.txt/valid.txt/test.txt 三个数据标注文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># %cd work</span></span><br><span class="line">!ls</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1512224.ipynb  config.py  data	dataset.py  __MACOSX  __pycache__  signs  work</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集根目录</span></span><br><span class="line">DATA_ROOT = <span class="string">&#x27;signs&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标签List</span></span><br><span class="line">LABEL_MAP = get(<span class="string">&#x27;LABEL_MAP&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标注生成函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_annotation</span>(<span class="params">mode</span>):</span><br><span class="line">    <span class="comment"># 建立标注文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;&#123;&#125;/&#123;&#125;.txt&#x27;</span>.<span class="built_in">format</span>(DATA_ROOT, mode), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 对应每个用途的数据文件夹，train/valid/test</span></span><br><span class="line">        train_dir = <span class="string">&#x27;&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(DATA_ROOT, mode)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历文件夹，获取里面的分类文件夹</span></span><br><span class="line">        <span class="keyword">for</span> path <span class="keyword">in</span> os.listdir(train_dir):</span><br><span class="line">            <span class="comment"># 标签对应的数字索引，实际标注的时候直接使用数字索引</span></span><br><span class="line">            label_index = LABEL_MAP.index(path)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 图像样本所在的路径</span></span><br><span class="line">            image_path = <span class="string">&#x27;&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_dir, path)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 遍历所有图像</span></span><br><span class="line">            <span class="keyword">for</span> image <span class="keyword">in</span> os.listdir(image_path):</span><br><span class="line">                <span class="comment"># 图像完整路径和名称</span></span><br><span class="line">                image_file = <span class="string">&#x27;&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(image_path, image)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="comment"># 验证图片格式是否ok</span></span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(image_file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f_img:</span><br><span class="line">                        image = Image.<span class="built_in">open</span>(io.BytesIO(f_img.read()))</span><br><span class="line">                        image.load()</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> image.mode == <span class="string">&#x27;RGB&#x27;</span>:</span><br><span class="line">                            f.write(<span class="string">&#x27;&#123;&#125;\t&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(image_file, label_index))</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">generate_annotation(<span class="string">&#x27;train&#x27;</span>)  <span class="comment"># 生成训练集标注文件</span></span><br><span class="line">generate_annotation(<span class="string">&#x27;valid&#x27;</span>)  <span class="comment"># 生成验证集标注文件</span></span><br><span class="line">generate_annotation(<span class="string">&#x27;test&#x27;</span>)   <span class="comment"># 生成测试集标注文件</span></span><br></pre></td></tr></table></figure>
<h2 id="2-3-数据集定义">2.3 数据集定义</h2>
<p>接下来我们使用标注好的文件进行数据集类的定义，方便后续模型训练使用。</p>
<h3 id="2-3-1-导入相关库">2.3.1 导入相关库</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">paddle.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;2.0.0&#x27;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-2-导入数据集的定义实现">2.3.2 导入数据集的定义实现</h3>
<p>我们数据集的代码实现是在 <a href="http://dataset.py">dataset.py</a> 中。</p>
<p>数据增强<code>data_augumentation</code>为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    self.transforms = T.Compose([</span><br><span class="line">    T.RandomResizedCrop(IMAGE_SIZE),    # 随机裁剪大小</span><br><span class="line">    T.RandomHorizontalFlip(0.5),        # 随机水平翻转</span><br><span class="line">    T.ToTensor(),                       # 数据的格式转换和标准化 HWC =&gt; CHW</span><br><span class="line">    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 图像归一化</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> ZodiacDataset</span><br></pre></td></tr></table></figure>
<h3 id="2-3-3-实例化数据集类">2.3.3 实例化数据集类</h3>
<p>根据所使用的数据集需求实例化数据集类，并查看总样本量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dataset = ZodiacDataset(mode=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">valid_dataset = ZodiacDataset(mode=<span class="string">&#x27;valid&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据集：&#123;&#125;张；验证数据集：&#123;&#125;张&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_dataset), <span class="built_in">len</span>(valid_dataset)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练数据集：7096张；验证数据集：639张</span><br></pre></td></tr></table></figure>
<h1>③ 模型选择和开发</h1>
<h2 id="3-1-网络构建">3.1 网络构建</h2>
<p>本次我们使用 ResNet50 网络来完成我们的案例实践。</p>
<p><strong>1）ResNet 系列网络</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/9fa9fbaf2943b69e4de49a2c1de97acb.png" alt=""></p>
<p><strong>2）ResNet50 结构</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/a62bf34558789921ed0297926d15c2d3.png" alt=""></p>
<p><strong>3）残差区块</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/9b8b57b52b9aa0e3588efad736ad6f3b.png" alt=""></p>
<p><strong>4）ResNet 其他版本</strong><br>
<img src="https://img-blog.csdnimg.cn/img_convert/2385de78c06da2f1410f5a17686c8458.png" alt=""><br>
<img src="https://img-blog.csdnimg.cn/img_convert/7dbf5144f44583fe0f785b8014fb01ea.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 请补齐模型实例化代码</span></span><br><span class="line"></span><br><span class="line">network = paddle.vision.models.resnet50(num_classes=get(<span class="string">&#x27;num_classes&#x27;</span>), pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100%|██████████| 151272/151272 [00:03&lt;00:00, 41104.37it/s]</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1263: UserWarning: Skip loading for fc.weight. fc.weight receives a shape [2048, 1000], but the expected shape is [2048, 12].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1263: UserWarning: Skip loading for fc.bias. fc.bias receives a shape [1000], but the expected shape is [12].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br></pre></td></tr></table></figure>
<p><strong>模型可视化</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = paddle.Model(network)</span><br><span class="line">model.summary((-<span class="number">1</span>, ) + <span class="built_in">tuple</span>(get(<span class="string">&#x27;image_shape&#x27;</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">   Layer (type)         Input Shape          Output Shape         Param #</span><br><span class="line">===============================================================================</span><br><span class="line">     Conv2D-1        [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408</span><br><span class="line">   BatchNorm2D-1    [[1, 64, 112, 112]]   [1, 64, 112, 112]         256</span><br><span class="line">      ReLU-1        [[1, 64, 112, 112]]   [1, 64, 112, 112]          0</span><br><span class="line">    MaxPool2D-1     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0</span><br><span class="line">     Conv2D-3        [[1, 64, 56, 56]]     [1, 64, 56, 56]         4,096</span><br><span class="line">   BatchNorm2D-3     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-2         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">     Conv2D-4        [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">   BatchNorm2D-4     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">     Conv2D-5        [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">   BatchNorm2D-5     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">     Conv2D-2        [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">   BatchNorm2D-2     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line"> BottleneckBlock-1   [[1, 64, 56, 56]]     [1, 256, 56, 56]          0</span><br><span class="line">     Conv2D-6        [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384</span><br><span class="line">   BatchNorm2D-6     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-3         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">     Conv2D-7        [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">   BatchNorm2D-7     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">     Conv2D-8        [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">   BatchNorm2D-8     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line"> BottleneckBlock-2   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">     Conv2D-9        [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384</span><br><span class="line">   BatchNorm2D-9     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-4         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">     Conv2D-10       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-10     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">     Conv2D-11       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-11     [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line"> BottleneckBlock-3   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">     Conv2D-13       [[1, 256, 56, 56]]    [1, 128, 56, 56]       32,768</span><br><span class="line">  BatchNorm2D-13     [[1, 128, 56, 56]]    [1, 128, 56, 56]         512</span><br><span class="line">      ReLU-5         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-14       [[1, 128, 56, 56]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-14     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">     Conv2D-15       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-15     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">     Conv2D-12       [[1, 256, 56, 56]]    [1, 512, 28, 28]       131,072</span><br><span class="line">  BatchNorm2D-12     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line"> BottleneckBlock-4   [[1, 256, 56, 56]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-16       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-16     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-6         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-17       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-17     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">     Conv2D-18       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-18     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line"> BottleneckBlock-5   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-19       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-19     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-7         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-20       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-20     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">     Conv2D-21       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-21     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line"> BottleneckBlock-6   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-22       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-22     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-8         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-23       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-23     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">     Conv2D-24       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-24     [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line"> BottleneckBlock-7   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">     Conv2D-26       [[1, 512, 28, 28]]    [1, 256, 28, 28]       131,072</span><br><span class="line">  BatchNorm2D-26     [[1, 256, 28, 28]]    [1, 256, 28, 28]        1,024</span><br><span class="line">      ReLU-9        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-27       [[1, 256, 28, 28]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-27     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">     Conv2D-28       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-28    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">     Conv2D-25       [[1, 512, 28, 28]]   [1, 1024, 14, 14]       524,288</span><br><span class="line">  BatchNorm2D-25    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line"> BottleneckBlock-8   [[1, 512, 28, 28]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-29      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-29     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-10       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-30       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-30     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">     Conv2D-31       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-31    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line"> BottleneckBlock-9  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-32      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-32     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-11       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-33       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-33     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">     Conv2D-34       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-34    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-10  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-35      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-35     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-12       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-36       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-36     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">     Conv2D-37       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-37    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-11  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-38      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-38     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-13       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-39       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-39     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">     Conv2D-40       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-40    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-12  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-41      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-41     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-14       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-42       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-42     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">     Conv2D-43       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-43    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-13  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">     Conv2D-45      [[1, 1024, 14, 14]]    [1, 512, 14, 14]       524,288</span><br><span class="line">  BatchNorm2D-45     [[1, 512, 14, 14]]    [1, 512, 14, 14]        2,048</span><br><span class="line">      ReLU-15        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">     Conv2D-46       [[1, 512, 14, 14]]     [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-46      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">     Conv2D-47        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-47     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">     Conv2D-44      [[1, 1024, 14, 14]]    [1, 2048, 7, 7]       2,097,152</span><br><span class="line">  BatchNorm2D-44     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-14  [[1, 1024, 14, 14]]    [1, 2048, 7, 7]           0</span><br><span class="line">     Conv2D-48       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-48      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">      ReLU-16        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">     Conv2D-49        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-49      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">     Conv2D-50        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-50     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-15   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">     Conv2D-51       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-51      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">      ReLU-17        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">     Conv2D-52        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-52      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">     Conv2D-53        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-53     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-16   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">AdaptiveAvgPool2D-1  [[1, 2048, 7, 7]]     [1, 2048, 1, 1]           0</span><br><span class="line">     Linear-1           [[1, 2048]]            [1, 12]            24,588</span><br><span class="line">===============================================================================</span><br><span class="line">Total params: 23,585,740</span><br><span class="line">Trainable params: 23,479,500</span><br><span class="line">Non-trainable params: 106,240</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.57</span><br><span class="line">Forward/backward pass size (MB): 261.48</span><br><span class="line">Params size (MB): 89.97</span><br><span class="line">Estimated Total Size (MB): 352.02</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&#x27;total_params&#x27;: 23585740, &#x27;trainable_params&#x27;: 23479500&#125;</span><br></pre></td></tr></table></figure>
<h2 id="超参数配置">超参数配置</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CONFIG = &#123;</span><br><span class="line">&#x27;model_save_dir&#x27;: &quot;./output/zodiac&quot;,</span><br><span class="line">&#x27;num_classes&#x27;: 12,</span><br><span class="line">&#x27;total_images&#x27;: 7096,</span><br><span class="line">&#x27;epochs&#x27;: 20,</span><br><span class="line">&#x27;batch_size&#x27;: 32,</span><br><span class="line">&#x27;image_shape&#x27;: [3, 224, 224],</span><br><span class="line">&#x27;LEARNING_RATE&#x27;: &#123;</span><br><span class="line">    &#x27;params&#x27;: &#123;</span><br><span class="line">        &#x27;lr&#x27;: 0.00375</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line">&#x27;OPTIMIZER&#x27;: &#123;</span><br><span class="line">    &#x27;params&#x27;: &#123;</span><br><span class="line">        &#x27;momentum&#x27;: 0.9</span><br><span class="line">    &#125;,</span><br><span class="line">    &#x27;regularizer&#x27;: &#123;</span><br><span class="line">        &#x27;function&#x27;: &#x27;L2&#x27;,</span><br><span class="line">        &#x27;factor&#x27;: 0.000001</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line">&#x27;LABEL_MAP&#x27;: [</span><br><span class="line">    &quot;ratt&quot;,</span><br><span class="line">    &quot;ox&quot;,</span><br><span class="line">    &quot;tiger&quot;,</span><br><span class="line">    &quot;rabbit&quot;,</span><br><span class="line">    &quot;dragon&quot;,</span><br><span class="line">    &quot;snake&quot;,</span><br><span class="line">    &quot;horse&quot;,</span><br><span class="line">    &quot;goat&quot;,</span><br><span class="line">    &quot;monkey&quot;,</span><br><span class="line">    &quot;rooster&quot;,</span><br><span class="line">    &quot;dog&quot;,</span><br><span class="line">    &quot;pig&quot;,</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>④ 模型训练和优化</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">EPOCHS = get(<span class="string">&#x27;epochs&#x27;</span>)</span><br><span class="line">BATCH_SIZE = get(<span class="string">&#x27;batch_size&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请补齐模型训练过程代码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_optim</span>(<span class="params">parameters</span>):</span><br><span class="line">    step_each_epoch = get(<span class="string">&#x27;total_images&#x27;</span>) // get(<span class="string">&#x27;batch_size&#x27;</span>)</span><br><span class="line">    lr = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=get(<span class="string">&#x27;LEARNING_RATE.params.lr&#x27;</span>),</span><br><span class="line">                                                  T_max=step_each_epoch * EPOCHS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> paddle.optimizer.Momentum(learning_rate=lr,</span><br><span class="line">                                     parameters=parameters,</span><br><span class="line">                                     weight_decay=paddle.regularizer.L2Decay(get(<span class="string">&#x27;OPTIMIZER.regularizer.factor&#x27;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练配置</span></span><br><span class="line">model.prepare(create_optim(network.parameters()),  <span class="comment"># 优化器</span></span><br><span class="line">              paddle.nn.CrossEntropyLoss(),        <span class="comment"># 损失函数</span></span><br><span class="line">              paddle.metric.Accuracy(topk=(<span class="number">1</span>, <span class="number">5</span>))) <span class="comment"># 评估指标</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练可视化VisualDL工具的回调函数</span></span><br><span class="line">visualdl = paddle.callbacks.VisualDL(log_dir=<span class="string">&#x27;visualdl_log&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动模型全流程训练</span></span><br><span class="line">model.fit(train_dataset,            <span class="comment"># 训练数据集</span></span><br><span class="line">          valid_dataset,            <span class="comment"># 评估数据集</span></span><br><span class="line">          epochs=EPOCHS,            <span class="comment"># 总的训练轮次</span></span><br><span class="line">          batch_size=BATCH_SIZE,    <span class="comment"># 批次计算的样本量大小</span></span><br><span class="line">          shuffle=<span class="literal">True</span>,             <span class="comment"># 是否打乱样本集</span></span><br><span class="line">          verbose=<span class="number">1</span>,                <span class="comment"># 日志展示格式</span></span><br><span class="line">          save_dir=<span class="string">&#x27;./chk_points/&#x27;</span>, <span class="comment"># 分阶段的训练模型存储路径</span></span><br><span class="line">          callbacks=[visualdl])     <span class="comment"># 回调函数使用</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/20</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  return (isinstance(seq, collections.Sequence) and</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:636: UserWarning: When training, we now always track global mean and variance.</span><br><span class="line">  &quot;When training, we now always track global mean and variance.&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">step 222/222 [==============================] - loss: 0.5499 - acc_top1: 0.7851 - acc_top5: 0.9548 - 734ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3935 - acc_top1: 0.9092 - acc_top5: 0.9922 - 812ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 2/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.3459 - acc_top1: 0.8519 - acc_top5: 0.9790 - 732ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/1</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.7412 - acc_top1: 0.8983 - acc_top5: 0.9890 - 833ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 3/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.4244 - acc_top1: 0.8671 - acc_top5: 0.9817 - 728ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/2</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.5254 - acc_top1: 0.9218 - acc_top5: 0.9984 - 819ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 4/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.2774 - acc_top1: 0.8878 - acc_top5: 0.9858 - 738ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/3</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3364 - acc_top1: 0.9280 - acc_top5: 0.9969 - 823ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 5/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.2692 - acc_top1: 0.8922 - acc_top5: 0.9884 - 728ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/4</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.2550 - acc_top1: 0.9311 - acc_top5: 0.9969 - 804ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 6/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.5775 - acc_top1: 0.9121 - acc_top5: 0.9894 - 727ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.5690 - acc_top1: 0.9531 - acc_top5: 0.9969 - 821ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 7/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.2791 - acc_top1: 0.9136 - acc_top5: 0.9897 - 731ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/6</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.5091 - acc_top1: 0.9609 - acc_top5: 0.9969 - 820ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 8/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.1944 - acc_top1: 0.9253 - acc_top5: 0.9920 - 734ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/7</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.2852 - acc_top1: 0.9531 - acc_top5: 0.9953 - 821ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 9/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.3202 - acc_top1: 0.9287 - acc_top5: 0.9927 - 735ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/8</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.4450 - acc_top1: 0.9531 - acc_top5: 0.9969 - 824ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 10/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.1250 - acc_top1: 0.9359 - acc_top5: 0.9925 - 745ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/9</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.4062 - acc_top1: 0.9609 - acc_top5: 0.9953 - 844ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 11/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.0311 - acc_top1: 0.9366 - acc_top5: 0.9917 - 737ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/10</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1610 - acc_top1: 0.9577 - acc_top5: 1.0000 - 831ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 12/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.2705 - acc_top1: 0.9479 - acc_top5: 0.9938 - 726ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/11</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1312 - acc_top1: 0.9577 - acc_top5: 1.0000 - 811ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 13/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.1966 - acc_top1: 0.9548 - acc_top5: 0.9942 - 729ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/12</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3287 - acc_top1: 0.9562 - acc_top5: 1.0000 - 833ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 14/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.2755 - acc_top1: 0.9563 - acc_top5: 0.9956 - 740ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/13</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1597 - acc_top1: 0.9546 - acc_top5: 0.9969 - 811ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 15/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.1340 - acc_top1: 0.9589 - acc_top5: 0.9945 - 741ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/14</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3858 - acc_top1: 0.9531 - acc_top5: 0.9937 - 828ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 16/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.0603 - acc_top1: 0.9600 - acc_top5: 0.9975 - 741ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/15</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1964 - acc_top1: 0.9656 - acc_top5: 0.9984 - 826ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 17/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.0447 - acc_top1: 0.9581 - acc_top5: 0.9955 - 740ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/16</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1808 - acc_top1: 0.9703 - acc_top5: 0.9969 - 818ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 18/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.5067 - acc_top1: 0.9625 - acc_top5: 0.9956 - 741ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/17</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.2021 - acc_top1: 0.9656 - acc_top5: 0.9984 - 833ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 19/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.0673 - acc_top1: 0.9593 - acc_top5: 0.9946 - 738ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/18</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.5409 - acc_top1: 0.9577 - acc_top5: 0.9969 - 836ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 20/20</span><br><span class="line">step 222/222 [==============================] - loss: 0.1440 - acc_top1: 0.9596 - acc_top5: 0.9965 - 742ms/step</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/19</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.2487 - acc_top1: 0.9640 - acc_top5: 0.9984 - 809ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">save checkpoint at /home/aistudio/chk_points/final</span><br></pre></td></tr></table></figure>
<h2 id="VisualDL-训练过程可视化展示">VisualDL 训练过程可视化展示</h2>
<p><code>visualdl = paddle.callbacks.VisualDL(log_dir='VisualDL_log')</code></p>
<h3 id="模型存储">模型存储</h3>
<p>将我们训练得到的模型进行保存，以便后续评估和测试使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(get(<span class="string">&#x27;model_save_dir&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h1>⑤ 模型评估和测试</h1>
<h2 id="5-1-批量预测测试">5.1 批量预测测试</h2>
<h3 id="5-1-1-测试数据集">5.1.1 测试数据集</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_dataset = ZodiacDataset(mode=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据集样本量：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(predict_dataset)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">测试数据集样本量：646</span><br></pre></td></tr></table></figure>
<h3 id="5-1-2-执行预测">5.1.2 执行预测</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.static <span class="keyword">import</span> InputSpec</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请补充网络结构</span></span><br><span class="line">network = paddle.vision.models.resnet50(num_classes=get(<span class="string">&#x27;num_classes&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型封装</span></span><br><span class="line">model_2 = paddle.Model(network, inputs=[InputSpec(shape=[-<span class="number">1</span>] + get(<span class="string">&#x27;image_shape&#x27;</span>), dtype=<span class="string">&#x27;float32&#x27;</span>, name=<span class="string">&#x27;image&#x27;</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请补充模型文件加载代码</span></span><br><span class="line"><span class="comment"># 训练好的模型加载</span></span><br><span class="line">model_2.load(get(<span class="string">&#x27;model_save_dir&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型配置</span></span><br><span class="line">model_2.prepare()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行预测</span></span><br><span class="line">result = model_2.predict(predict_dataset)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 646/646 [==============================] - 33ms/step</span><br><span class="line">Predict samples: 646</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 样本映射</span></span><br><span class="line">LABEL_MAP = get(<span class="string">&#x27;LABEL_MAP&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机取样本展示</span></span><br><span class="line">indexs = np.random.randint(<span class="number">1</span>, <span class="number">646</span>, size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> indexs:</span><br><span class="line">    predict_label = np.argmax(result[<span class="number">0</span>][idx])</span><br><span class="line">    real_label = predict_dataset[idx][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;样本ID：&#123;&#125;, 真实标签：&#123;&#125;, 预测值：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(idx, LABEL_MAP[real_label], LABEL_MAP[predict_label]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">样本ID：393, 真实标签：pig, 预测值：pig</span><br><span class="line">样本ID：107, 真实标签：monkey, 预测值：monkey</span><br><span class="line">样本ID：466, 真实标签：tiger, 预测值：tiger</span><br><span class="line">样本ID：279, 真实标签：horse, 预测值：horse</span><br><span class="line">样本ID：425, 真实标签：pig, 预测值：pig</span><br><span class="line">样本ID：127, 真实标签：monkey, 预测值：monkey</span><br><span class="line">样本ID：432, 真实标签：tiger, 预测值：tiger</span><br><span class="line">样本ID：377, 真实标签：pig, 预测值：pig</span><br><span class="line">样本ID：99, 真实标签：dragon, 预测值：dragon</span><br><span class="line">样本ID：322, 真实标签：dog, 预测值：dog</span><br><span class="line">样本ID：460, 真实标签：tiger, 预测值：tiger</span><br><span class="line">样本ID：554, 真实标签：ox, 预测值：ox</span><br><span class="line">样本ID：77, 真实标签：dragon, 预测值：dragon</span><br><span class="line">样本ID：335, 真实标签：dog, 预测值：dog</span><br><span class="line">样本ID：398, 真实标签：pig, 预测值：pig</span><br><span class="line">样本ID：176, 真实标签：snake, 预测值：snake</span><br><span class="line">样本ID：207, 真实标签：snake, 预测值：snake</span><br><span class="line">样本ID：29, 真实标签：rooster, 预测值：rooster</span><br><span class="line">样本ID：476, 真实标签：tiger, 预测值：tiger</span><br><span class="line">样本ID：424, 真实标签：pig, 预测值：pig</span><br></pre></td></tr></table></figure>
<h1>⑥ 模型部署</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_2.save(<span class="string">&#x27;infer/zodiac&#x27;</span>, training=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/math_op_patch.py:298: UserWarning: /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/vision/models/resnet.py:145</span><br><span class="line">The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.</span><br><span class="line">  op_type, op_type, EXPRESSION_MAP[method_name]))</span><br></pre></td></tr></table></figure>
<h2 id="MobileNet-V2-测试">MobileNet_V2 测试</h2>
<p>由于<code>ResNet</code>模型和计算量都比较大，在 AI_Studio 的 GPU 下跑 20 个 Epoch 还是跑了很久，这里测试一下用<code>MobileNet</code>来进行训练，测试运算效率和准确性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">network = paddle.vision.models.mobilenet_v2(num_classes=get(<span class="string">&#x27;num_classes&#x27;</span>), pretrained=<span class="literal">True</span>)</span><br><span class="line">model_lite = paddle.Model(network)</span><br><span class="line">model_lite.summary((-<span class="number">1</span>, ) + <span class="built_in">tuple</span>(get(<span class="string">&#x27;image_shape&#x27;</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100%|██████████| 20795/20795 [00:00&lt;00:00, 44614.48it/s]</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1263: UserWarning: Skip loading for classifier.1.weight. classifier.1.weight receives a shape [1280, 1000], but the expected shape is [1280, 12].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1263: UserWarning: Skip loading for classifier.1.bias. classifier.1.bias receives a shape [1000], but the expected shape is [12].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">   Layer (type)         Input Shape          Output Shape         Param #</span><br><span class="line">===============================================================================</span><br><span class="line">    Conv2D-160       [[1, 3, 224, 224]]   [1, 32, 112, 112]         864</span><br><span class="line">  BatchNorm2D-107   [[1, 32, 112, 112]]   [1, 32, 112, 112]         128</span><br><span class="line">      ReLU6-1       [[1, 32, 112, 112]]   [1, 32, 112, 112]          0</span><br><span class="line">    Conv2D-161      [[1, 32, 112, 112]]   [1, 32, 112, 112]         288</span><br><span class="line">  BatchNorm2D-108   [[1, 32, 112, 112]]   [1, 32, 112, 112]         128</span><br><span class="line">      ReLU6-2       [[1, 32, 112, 112]]   [1, 32, 112, 112]          0</span><br><span class="line">    Conv2D-162      [[1, 32, 112, 112]]   [1, 16, 112, 112]         512</span><br><span class="line">  BatchNorm2D-109   [[1, 16, 112, 112]]   [1, 16, 112, 112]         64</span><br><span class="line">InvertedResidual-1  [[1, 32, 112, 112]]   [1, 16, 112, 112]          0</span><br><span class="line">    Conv2D-163      [[1, 16, 112, 112]]   [1, 96, 112, 112]        1,536</span><br><span class="line">  BatchNorm2D-110   [[1, 96, 112, 112]]   [1, 96, 112, 112]         384</span><br><span class="line">      ReLU6-3       [[1, 96, 112, 112]]   [1, 96, 112, 112]          0</span><br><span class="line">    Conv2D-164      [[1, 96, 112, 112]]    [1, 96, 56, 56]          864</span><br><span class="line">  BatchNorm2D-111    [[1, 96, 56, 56]]     [1, 96, 56, 56]          384</span><br><span class="line">      ReLU6-4        [[1, 96, 56, 56]]     [1, 96, 56, 56]           0</span><br><span class="line">    Conv2D-165       [[1, 96, 56, 56]]     [1, 24, 56, 56]         2,304</span><br><span class="line">  BatchNorm2D-112    [[1, 24, 56, 56]]     [1, 24, 56, 56]          96</span><br><span class="line">InvertedResidual-2  [[1, 16, 112, 112]]    [1, 24, 56, 56]           0</span><br><span class="line">    Conv2D-166       [[1, 24, 56, 56]]     [1, 144, 56, 56]        3,456</span><br><span class="line">  BatchNorm2D-113    [[1, 144, 56, 56]]    [1, 144, 56, 56]         576</span><br><span class="line">      ReLU6-5        [[1, 144, 56, 56]]    [1, 144, 56, 56]          0</span><br><span class="line">    Conv2D-167       [[1, 144, 56, 56]]    [1, 144, 56, 56]        1,296</span><br><span class="line">  BatchNorm2D-114    [[1, 144, 56, 56]]    [1, 144, 56, 56]         576</span><br><span class="line">      ReLU6-6        [[1, 144, 56, 56]]    [1, 144, 56, 56]          0</span><br><span class="line">    Conv2D-168       [[1, 144, 56, 56]]    [1, 24, 56, 56]         3,456</span><br><span class="line">  BatchNorm2D-115    [[1, 24, 56, 56]]     [1, 24, 56, 56]          96</span><br><span class="line">InvertedResidual-3   [[1, 24, 56, 56]]     [1, 24, 56, 56]           0</span><br><span class="line">    Conv2D-169       [[1, 24, 56, 56]]     [1, 144, 56, 56]        3,456</span><br><span class="line">  BatchNorm2D-116    [[1, 144, 56, 56]]    [1, 144, 56, 56]         576</span><br><span class="line">      ReLU6-7        [[1, 144, 56, 56]]    [1, 144, 56, 56]          0</span><br><span class="line">    Conv2D-170       [[1, 144, 56, 56]]    [1, 144, 28, 28]        1,296</span><br><span class="line">  BatchNorm2D-117    [[1, 144, 28, 28]]    [1, 144, 28, 28]         576</span><br><span class="line">      ReLU6-8        [[1, 144, 28, 28]]    [1, 144, 28, 28]          0</span><br><span class="line">    Conv2D-171       [[1, 144, 28, 28]]    [1, 32, 28, 28]         4,608</span><br><span class="line">  BatchNorm2D-118    [[1, 32, 28, 28]]     [1, 32, 28, 28]          128</span><br><span class="line">InvertedResidual-4   [[1, 24, 56, 56]]     [1, 32, 28, 28]           0</span><br><span class="line">    Conv2D-172       [[1, 32, 28, 28]]     [1, 192, 28, 28]        6,144</span><br><span class="line">  BatchNorm2D-119    [[1, 192, 28, 28]]    [1, 192, 28, 28]         768</span><br><span class="line">      ReLU6-9        [[1, 192, 28, 28]]    [1, 192, 28, 28]          0</span><br><span class="line">    Conv2D-173       [[1, 192, 28, 28]]    [1, 192, 28, 28]        1,728</span><br><span class="line">  BatchNorm2D-120    [[1, 192, 28, 28]]    [1, 192, 28, 28]         768</span><br><span class="line">     ReLU6-10        [[1, 192, 28, 28]]    [1, 192, 28, 28]          0</span><br><span class="line">    Conv2D-174       [[1, 192, 28, 28]]    [1, 32, 28, 28]         6,144</span><br><span class="line">  BatchNorm2D-121    [[1, 32, 28, 28]]     [1, 32, 28, 28]          128</span><br><span class="line">InvertedResidual-5   [[1, 32, 28, 28]]     [1, 32, 28, 28]           0</span><br><span class="line">    Conv2D-175       [[1, 32, 28, 28]]     [1, 192, 28, 28]        6,144</span><br><span class="line">  BatchNorm2D-122    [[1, 192, 28, 28]]    [1, 192, 28, 28]         768</span><br><span class="line">     ReLU6-11        [[1, 192, 28, 28]]    [1, 192, 28, 28]          0</span><br><span class="line">    Conv2D-176       [[1, 192, 28, 28]]    [1, 192, 28, 28]        1,728</span><br><span class="line">  BatchNorm2D-123    [[1, 192, 28, 28]]    [1, 192, 28, 28]         768</span><br><span class="line">     ReLU6-12        [[1, 192, 28, 28]]    [1, 192, 28, 28]          0</span><br><span class="line">    Conv2D-177       [[1, 192, 28, 28]]    [1, 32, 28, 28]         6,144</span><br><span class="line">  BatchNorm2D-124    [[1, 32, 28, 28]]     [1, 32, 28, 28]          128</span><br><span class="line">InvertedResidual-6   [[1, 32, 28, 28]]     [1, 32, 28, 28]           0</span><br><span class="line">    Conv2D-178       [[1, 32, 28, 28]]     [1, 192, 28, 28]        6,144</span><br><span class="line">  BatchNorm2D-125    [[1, 192, 28, 28]]    [1, 192, 28, 28]         768</span><br><span class="line">     ReLU6-13        [[1, 192, 28, 28]]    [1, 192, 28, 28]          0</span><br><span class="line">    Conv2D-179       [[1, 192, 28, 28]]    [1, 192, 14, 14]        1,728</span><br><span class="line">  BatchNorm2D-126    [[1, 192, 14, 14]]    [1, 192, 14, 14]         768</span><br><span class="line">     ReLU6-14        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0</span><br><span class="line">    Conv2D-180       [[1, 192, 14, 14]]    [1, 64, 14, 14]        12,288</span><br><span class="line">  BatchNorm2D-127    [[1, 64, 14, 14]]     [1, 64, 14, 14]          256</span><br><span class="line">InvertedResidual-7   [[1, 32, 28, 28]]     [1, 64, 14, 14]           0</span><br><span class="line">    Conv2D-181       [[1, 64, 14, 14]]     [1, 384, 14, 14]       24,576</span><br><span class="line">  BatchNorm2D-128    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-15        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-182       [[1, 384, 14, 14]]    [1, 384, 14, 14]        3,456</span><br><span class="line">  BatchNorm2D-129    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-16        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-183       [[1, 384, 14, 14]]    [1, 64, 14, 14]        24,576</span><br><span class="line">  BatchNorm2D-130    [[1, 64, 14, 14]]     [1, 64, 14, 14]          256</span><br><span class="line">InvertedResidual-8   [[1, 64, 14, 14]]     [1, 64, 14, 14]           0</span><br><span class="line">    Conv2D-184       [[1, 64, 14, 14]]     [1, 384, 14, 14]       24,576</span><br><span class="line">  BatchNorm2D-131    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-17        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-185       [[1, 384, 14, 14]]    [1, 384, 14, 14]        3,456</span><br><span class="line">  BatchNorm2D-132    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-18        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-186       [[1, 384, 14, 14]]    [1, 64, 14, 14]        24,576</span><br><span class="line">  BatchNorm2D-133    [[1, 64, 14, 14]]     [1, 64, 14, 14]          256</span><br><span class="line">InvertedResidual-9   [[1, 64, 14, 14]]     [1, 64, 14, 14]           0</span><br><span class="line">    Conv2D-187       [[1, 64, 14, 14]]     [1, 384, 14, 14]       24,576</span><br><span class="line">  BatchNorm2D-134    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-19        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-188       [[1, 384, 14, 14]]    [1, 384, 14, 14]        3,456</span><br><span class="line">  BatchNorm2D-135    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-20        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-189       [[1, 384, 14, 14]]    [1, 64, 14, 14]        24,576</span><br><span class="line">  BatchNorm2D-136    [[1, 64, 14, 14]]     [1, 64, 14, 14]          256</span><br><span class="line">InvertedResidual-10  [[1, 64, 14, 14]]     [1, 64, 14, 14]           0</span><br><span class="line">    Conv2D-190       [[1, 64, 14, 14]]     [1, 384, 14, 14]       24,576</span><br><span class="line">  BatchNorm2D-137    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-21        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-191       [[1, 384, 14, 14]]    [1, 384, 14, 14]        3,456</span><br><span class="line">  BatchNorm2D-138    [[1, 384, 14, 14]]    [1, 384, 14, 14]        1,536</span><br><span class="line">     ReLU6-22        [[1, 384, 14, 14]]    [1, 384, 14, 14]          0</span><br><span class="line">    Conv2D-192       [[1, 384, 14, 14]]    [1, 96, 14, 14]        36,864</span><br><span class="line">  BatchNorm2D-139    [[1, 96, 14, 14]]     [1, 96, 14, 14]          384</span><br><span class="line">InvertedResidual-11  [[1, 64, 14, 14]]     [1, 96, 14, 14]           0</span><br><span class="line">    Conv2D-193       [[1, 96, 14, 14]]     [1, 576, 14, 14]       55,296</span><br><span class="line">  BatchNorm2D-140    [[1, 576, 14, 14]]    [1, 576, 14, 14]        2,304</span><br><span class="line">     ReLU6-23        [[1, 576, 14, 14]]    [1, 576, 14, 14]          0</span><br><span class="line">    Conv2D-194       [[1, 576, 14, 14]]    [1, 576, 14, 14]        5,184</span><br><span class="line">  BatchNorm2D-141    [[1, 576, 14, 14]]    [1, 576, 14, 14]        2,304</span><br><span class="line">     ReLU6-24        [[1, 576, 14, 14]]    [1, 576, 14, 14]          0</span><br><span class="line">    Conv2D-195       [[1, 576, 14, 14]]    [1, 96, 14, 14]        55,296</span><br><span class="line">  BatchNorm2D-142    [[1, 96, 14, 14]]     [1, 96, 14, 14]          384</span><br><span class="line">InvertedResidual-12  [[1, 96, 14, 14]]     [1, 96, 14, 14]           0</span><br><span class="line">    Conv2D-196       [[1, 96, 14, 14]]     [1, 576, 14, 14]       55,296</span><br><span class="line">  BatchNorm2D-143    [[1, 576, 14, 14]]    [1, 576, 14, 14]        2,304</span><br><span class="line">     ReLU6-25        [[1, 576, 14, 14]]    [1, 576, 14, 14]          0</span><br><span class="line">    Conv2D-197       [[1, 576, 14, 14]]    [1, 576, 14, 14]        5,184</span><br><span class="line">  BatchNorm2D-144    [[1, 576, 14, 14]]    [1, 576, 14, 14]        2,304</span><br><span class="line">     ReLU6-26        [[1, 576, 14, 14]]    [1, 576, 14, 14]          0</span><br><span class="line">    Conv2D-198       [[1, 576, 14, 14]]    [1, 96, 14, 14]        55,296</span><br><span class="line">  BatchNorm2D-145    [[1, 96, 14, 14]]     [1, 96, 14, 14]          384</span><br><span class="line">InvertedResidual-13  [[1, 96, 14, 14]]     [1, 96, 14, 14]           0</span><br><span class="line">    Conv2D-199       [[1, 96, 14, 14]]     [1, 576, 14, 14]       55,296</span><br><span class="line">  BatchNorm2D-146    [[1, 576, 14, 14]]    [1, 576, 14, 14]        2,304</span><br><span class="line">     ReLU6-27        [[1, 576, 14, 14]]    [1, 576, 14, 14]          0</span><br><span class="line">    Conv2D-200       [[1, 576, 14, 14]]     [1, 576, 7, 7]         5,184</span><br><span class="line">  BatchNorm2D-147     [[1, 576, 7, 7]]      [1, 576, 7, 7]         2,304</span><br><span class="line">     ReLU6-28         [[1, 576, 7, 7]]      [1, 576, 7, 7]           0</span><br><span class="line">    Conv2D-201        [[1, 576, 7, 7]]      [1, 160, 7, 7]        92,160</span><br><span class="line">  BatchNorm2D-148     [[1, 160, 7, 7]]      [1, 160, 7, 7]          640</span><br><span class="line">InvertedResidual-14  [[1, 96, 14, 14]]      [1, 160, 7, 7]           0</span><br><span class="line">    Conv2D-202        [[1, 160, 7, 7]]      [1, 960, 7, 7]        153,600</span><br><span class="line">  BatchNorm2D-149     [[1, 960, 7, 7]]      [1, 960, 7, 7]         3,840</span><br><span class="line">     ReLU6-29         [[1, 960, 7, 7]]      [1, 960, 7, 7]           0</span><br><span class="line">    Conv2D-203        [[1, 960, 7, 7]]      [1, 960, 7, 7]         8,640</span><br><span class="line">  BatchNorm2D-150     [[1, 960, 7, 7]]      [1, 960, 7, 7]         3,840</span><br><span class="line">     ReLU6-30         [[1, 960, 7, 7]]      [1, 960, 7, 7]           0</span><br><span class="line">    Conv2D-204        [[1, 960, 7, 7]]      [1, 160, 7, 7]        153,600</span><br><span class="line">  BatchNorm2D-151     [[1, 160, 7, 7]]      [1, 160, 7, 7]          640</span><br><span class="line">InvertedResidual-15   [[1, 160, 7, 7]]      [1, 160, 7, 7]           0</span><br><span class="line">    Conv2D-205        [[1, 160, 7, 7]]      [1, 960, 7, 7]        153,600</span><br><span class="line">  BatchNorm2D-152     [[1, 960, 7, 7]]      [1, 960, 7, 7]         3,840</span><br><span class="line">     ReLU6-31         [[1, 960, 7, 7]]      [1, 960, 7, 7]           0</span><br><span class="line">    Conv2D-206        [[1, 960, 7, 7]]      [1, 960, 7, 7]         8,640</span><br><span class="line">  BatchNorm2D-153     [[1, 960, 7, 7]]      [1, 960, 7, 7]         3,840</span><br><span class="line">     ReLU6-32         [[1, 960, 7, 7]]      [1, 960, 7, 7]           0</span><br><span class="line">    Conv2D-207        [[1, 960, 7, 7]]      [1, 160, 7, 7]        153,600</span><br><span class="line">  BatchNorm2D-154     [[1, 160, 7, 7]]      [1, 160, 7, 7]          640</span><br><span class="line">InvertedResidual-16   [[1, 160, 7, 7]]      [1, 160, 7, 7]           0</span><br><span class="line">    Conv2D-208        [[1, 160, 7, 7]]      [1, 960, 7, 7]        153,600</span><br><span class="line">  BatchNorm2D-155     [[1, 960, 7, 7]]      [1, 960, 7, 7]         3,840</span><br><span class="line">     ReLU6-33         [[1, 960, 7, 7]]      [1, 960, 7, 7]           0</span><br><span class="line">    Conv2D-209        [[1, 960, 7, 7]]      [1, 960, 7, 7]         8,640</span><br><span class="line">  BatchNorm2D-156     [[1, 960, 7, 7]]      [1, 960, 7, 7]         3,840</span><br><span class="line">     ReLU6-34         [[1, 960, 7, 7]]      [1, 960, 7, 7]           0</span><br><span class="line">    Conv2D-210        [[1, 960, 7, 7]]      [1, 320, 7, 7]        307,200</span><br><span class="line">  BatchNorm2D-157     [[1, 320, 7, 7]]      [1, 320, 7, 7]         1,280</span><br><span class="line">InvertedResidual-17   [[1, 160, 7, 7]]      [1, 320, 7, 7]           0</span><br><span class="line">    Conv2D-211        [[1, 320, 7, 7]]     [1, 1280, 7, 7]        409,600</span><br><span class="line">  BatchNorm2D-158    [[1, 1280, 7, 7]]     [1, 1280, 7, 7]         5,120</span><br><span class="line">     ReLU6-35        [[1, 1280, 7, 7]]     [1, 1280, 7, 7]           0</span><br><span class="line">AdaptiveAvgPool2D-3  [[1, 1280, 7, 7]]     [1, 1280, 1, 1]           0</span><br><span class="line">     Dropout-1          [[1, 1280]]           [1, 1280]              0</span><br><span class="line">     Linear-4           [[1, 1280]]            [1, 12]            15,372</span><br><span class="line">===============================================================================</span><br><span class="line">Total params: 2,273,356</span><br><span class="line">Trainable params: 2,205,132</span><br><span class="line">Non-trainable params: 68,224</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.57</span><br><span class="line">Forward/backward pass size (MB): 152.87</span><br><span class="line">Params size (MB): 8.67</span><br><span class="line">Estimated Total Size (MB): 162.12</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&#x27;total_params&#x27;: 2273356, &#x27;trainable_params&#x27;: 2205132&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">EPOCHS = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请补齐模型训练过程代码</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练配置</span></span><br><span class="line">model_lite.prepare(paddle.optimizer.Adam(learning_rate=<span class="number">0.0001</span>, parameters=model_lite.parameters()),  <span class="comment"># 优化器</span></span><br><span class="line">              paddle.nn.CrossEntropyLoss(),        <span class="comment"># 损失函数</span></span><br><span class="line">              paddle.metric.Accuracy(topk=(<span class="number">1</span>, <span class="number">5</span>))) <span class="comment"># 评估指标</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练可视化VisualDL工具的回调函数</span></span><br><span class="line">visualdl = paddle.callbacks.VisualDL(log_dir=<span class="string">&#x27;./mobilenet/visualdl_log&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动模型全流程训练</span></span><br><span class="line">model_lite.fit( train_dataset,            <span class="comment"># 训练数据集</span></span><br><span class="line">                valid_dataset,            <span class="comment"># 评估数据集</span></span><br><span class="line">                epochs=EPOCHS,            <span class="comment"># 总的训练轮次</span></span><br><span class="line">                batch_size=BATCH_SIZE,    <span class="comment"># 批次计算的样本量大小</span></span><br><span class="line">                shuffle=<span class="literal">True</span>,             <span class="comment"># 是否打乱样本集</span></span><br><span class="line">                verbose=<span class="number">1</span>,                <span class="comment"># 日志展示格式</span></span><br><span class="line">                save_dir=<span class="string">&#x27;./mobilenet/chk_points/&#x27;</span>, <span class="comment"># 分阶段的训练模型存储路径</span></span><br><span class="line">                callbacks=[visualdl])     <span class="comment"># 回调函数使用</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:636: UserWarning: When training, we now always track global mean and variance.</span><br><span class="line">  &quot;When training, we now always track global mean and variance.&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">step 222/222 [==============================] - loss: 0.2848 - acc_top1: 0.8271 - acc_top5: 0.9762 - 690ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.5077 - acc_top1: 0.9124 - acc_top5: 0.9906 - 803ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 2/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.2549 - acc_top1: 0.8541 - acc_top5: 0.9800 - 689ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/1</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3955 - acc_top1: 0.9218 - acc_top5: 0.9937 - 813ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 3/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.3388 - acc_top1: 0.8660 - acc_top5: 0.9842 - 693ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/2</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.2251 - acc_top1: 0.9390 - acc_top5: 0.9937 - 817ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 4/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.6369 - acc_top1: 0.8773 - acc_top5: 0.9845 - 688ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/3</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3293 - acc_top1: 0.9311 - acc_top5: 0.9937 - 813ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 5/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.4066 - acc_top1: 0.8908 - acc_top5: 0.9872 - 696ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/4</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.2782 - acc_top1: 0.9343 - acc_top5: 0.9937 - 808ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 6/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.3100 - acc_top1: 0.8961 - acc_top5: 0.9876 - 686ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.4607 - acc_top1: 0.9390 - acc_top5: 0.9937 - 810ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 7/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.1829 - acc_top1: 0.9016 - acc_top5: 0.9887 - 688ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/6</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.3421 - acc_top1: 0.9264 - acc_top5: 0.9969 - 815ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 8/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.4560 - acc_top1: 0.8932 - acc_top5: 0.9870 - 688ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/7</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1464 - acc_top1: 0.9343 - acc_top5: 0.9922 - 830ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 9/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.4075 - acc_top1: 0.9040 - acc_top5: 0.9873 - 686ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/8</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.5118 - acc_top1: 0.9468 - acc_top5: 0.9953 - 820ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">Epoch 10/10</span><br><span class="line">step 222/222 [==============================] - loss: 0.0882 - acc_top1: 0.9094 - acc_top5: 0.9866 - 686ms/step</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/9</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 20/20 [==============================] - loss: 0.1651 - acc_top1: 0.9468 - acc_top5: 0.9953 - 821ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">save checkpoint at /home/aistudio/mobilenet/chk_points/final</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">KeyError                                  Traceback (most recent call last)</span><br><span class="line"></span><br><span class="line">&lt;ipython-input-49-266b4a21aaf0&gt; in &lt;module&gt;</span><br><span class="line">     23                 callbacks=[visualdl])     # 回调函数使用</span><br><span class="line">     24</span><br><span class="line">---&gt; 25 model.save(get(&#x27;./mobilenet/model&#x27;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~/config.py in get(full_path)</span><br><span class="line">     43             config = CONFIG</span><br><span class="line">     44</span><br><span class="line">---&gt; 45         config = config[name]</span><br><span class="line">     46</span><br><span class="line">     47     return config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">KeyError: &#x27;&#x27;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;./mobilenet/model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型评估</span></span><br><span class="line">val = model_lite.evaluate(valid_dataset, verbose=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(val)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 639/639 [==============================] - loss: 1.0871e-04 - acc_top1: 0.9374 - acc_top5: 0.9890 - 46ms/step</span><br><span class="line">Eval samples: 639</span><br><span class="line">&#123;&#x27;loss&#x27;: [0.000108712964], &#x27;acc_top1&#x27;: 0.9374021909233177, &#x27;acc_top5&#x27;: 0.9890453834115805&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 执行预测</span></span><br><span class="line">pred = model_lite.predict(predict_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本映射</span></span><br><span class="line">LABEL_MAP = get(<span class="string">&#x27;LABEL_MAP&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机取样本展示</span></span><br><span class="line">indexs = np.random.randint(<span class="number">1</span>, <span class="number">646</span>, size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> indexs:</span><br><span class="line">    predict_label = np.argmax(pred[<span class="number">0</span>][idx])</span><br><span class="line">    real_label = predict_dataset[idx][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    real_label = predict_dataset[idx][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;样本ID：&#123;&#125;, 真实标签：&#123;&#125;, 预测值：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(idx, LABEL_MAP[real_label], LABEL_MAP[predict_label]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 646/646 [==============================] - 42ms/step</span><br><span class="line">Predict samples: 646</span><br><span class="line">样本ID：611, 真实标签：ratt, 预测值：dragon</span><br><span class="line">样本ID：42, 真实标签：rooster, 预测值：dragon</span><br><span class="line">样本ID：11, 真实标签：rooster, 预测值：rooster</span><br><span class="line">样本ID：625, 真实标签：ratt, 预测值：ratt</span><br><span class="line">样本ID：263, 真实标签：goat, 预测值：goat</span><br><span class="line">样本ID：635, 真实标签：ratt, 预测值：ratt</span><br><span class="line">样本ID：78, 真实标签：dragon, 预测值：dragon</span><br><span class="line">样本ID：567, 真实标签：ox, 预测值：ox</span><br><span class="line">样本ID：280, 真实标签：horse, 预测值：horse</span><br><span class="line">样本ID：406, 真实标签：pig, 预测值：pig</span><br><span class="line">样本ID：167, 真实标签：snake, 预测值：snake</span><br><span class="line">样本ID：19, 真实标签：rooster, 预测值：rooster</span><br><span class="line">样本ID：41, 真实标签：rooster, 预测值：rooster</span><br><span class="line">样本ID：406, 真实标签：pig, 预测值：pig</span><br><span class="line">样本ID：497, 真实标签：rabbit, 预测值：rabbit</span><br><span class="line">样本ID：71, 真实标签：dragon, 预测值：dragon</span><br><span class="line">样本ID：300, 真实标签：horse, 预测值：horse</span><br><span class="line">样本ID：247, 真实标签：goat, 预测值：goat</span><br><span class="line">样本ID：450, 真实标签：tiger, 预测值：tiger</span><br><span class="line">样本ID：82, 真实标签：dragon, 预测值：dragon</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>StandardScaler(sklearn)参数详解</title>
    <url>/2022/06/01/224bdd4444be4fe0b7826959bc6b40a4/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="为什么要归一化">为什么要归一化</h2>
<ul>
<li>
<p>归一化后加快了梯度下降求最优解的速度：</p>
<p>如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p>
</li>
<li>
<p>归一化有可能提高精度：</p>
<p>一些分类器需要计算样本之间的距离（如欧氏距离），例如 KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>StandardScaler()<br>
标准化数据，保证每个维度数据方差为 1.均值为 0。使得据测结果不会被某些维度过大的特征值而主导。\</p>
<p>$$<br>
x^* = \frac{x - \mu}{\sigma}<br>
$$</p>
<ul>
<li>
<p>fit</p>
<p>用于计算训练数据的均值和方差， 后面就会用均值和方差来转换训练数据</p>
</li>
<li>
<p>transform</p>
<p>很显然，它只是进行转换，只是把训练数据转换成标准的正态分布</p>
</li>
<li>
<p>fit_transform</p>
<p>不仅计算训练数据的均值和方差，还会基于计算出来的均值和方差来转换训练数据，从而把数据转换成标准的正态分布</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;use sklearn:\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 注：shape of data: [n_samples, n_features]</span></span><br><span class="line">data = np.random.randn(<span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">ss = StandardScaler()</span><br><span class="line">ss.fit(data)</span><br><span class="line">trans_data = ss.transform(data)</span><br><span class="line"><span class="comment"># ss.fit_transform(data)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;original data: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;transformed data: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(trans_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;scaler info: scaler.mean_: &#123;&#125;, scaler.var_: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ss.mean_, ss.var_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;use numpy by self&#x27;</span>)</span><br><span class="line">mean = np.mean(data, axis=<span class="number">0</span>)</span><br><span class="line">std = np.std(data, axis=<span class="number">0</span>)</span><br><span class="line">var = std * std</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;mean: &#123;&#125;, std: &#123;&#125;, var: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(mean, std, var))</span><br><span class="line"><span class="comment"># numpy 的广播功能</span></span><br><span class="line">another_trans_data = data - mean</span><br><span class="line"><span class="comment"># 注：是除以标准差</span></span><br><span class="line">another_trans_data = another_trans_data / std</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;another_trans_data: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(another_trans_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">use sklearn:</span><br><span class="line"></span><br><span class="line">original data:</span><br><span class="line">[[-1.0856306   0.99734545  0.2829785  -1.50629471]</span><br><span class="line"> [-0.57860025  1.65143654 -2.42667924 -0.42891263]</span><br><span class="line"> [ 1.26593626 -0.8667404  -0.67888615 -0.09470897]</span><br><span class="line"> [ 1.49138963 -0.638902   -0.44398196 -0.43435128]</span><br><span class="line"> [ 2.20593008  2.18678609  1.0040539   0.3861864 ]</span><br><span class="line"> [ 0.73736858  1.49073203 -0.93583387  1.17582904]</span><br><span class="line"> [-1.25388067 -0.6377515   0.9071052  -1.4286807 ]</span><br><span class="line"> [-0.14006872 -0.8617549  -0.25561937 -2.79858911]</span><br><span class="line"> [-1.7715331  -0.69987723  0.92746243 -0.17363568]</span><br><span class="line"> [ 0.00284592  0.68822271 -0.87953634  0.28362732]]</span><br><span class="line">transformed data:</span><br><span class="line">[[-0.94511643  0.58665507  0.5223171  -0.93064483]</span><br><span class="line"> [-0.53659117  1.16247784 -2.13366794  0.06768082]</span><br><span class="line"> [ 0.9495916  -1.05437488 -0.42049501  0.3773612 ]</span><br><span class="line"> [ 1.13124423 -0.85379954 -0.19024378  0.06264126]</span><br><span class="line"> [ 1.70696485  1.63376764  1.22910949  0.8229693 ]</span><br><span class="line"> [ 0.52371324  1.02100318 -0.67235312  1.55466934]</span><br><span class="line"> [-1.08067913 -0.85278672  1.13408114 -0.858726  ]</span><br><span class="line"> [-0.18325687 -1.04998594 -0.00561227 -2.1281129 ]</span><br><span class="line"> [-1.49776284 -0.9074785   1.15403514  0.30422599]</span><br><span class="line"> [-0.06810748  0.31452186 -0.61717074  0.72793583]]</span><br><span class="line">scaler info: scaler.mean_: [ 0.08737571  0.33094968 -0.24989369 -0.50195303], scaler.var_: [1.54038781 1.29032409 1.04082479 1.16464894]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">use numpy by self</span><br><span class="line">mean: [ 0.08737571  0.33094968 -0.24989369 -0.50195303], std: [1.24112361 1.13592433 1.02020821 1.07918902], var: [1.54038781 1.29032409 1.04082479 1.16464894]</span><br><span class="line">another_trans_data:</span><br><span class="line">[[-0.94511643  0.58665507  0.5223171  -0.93064483]</span><br><span class="line"> [-0.53659117  1.16247784 -2.13366794  0.06768082]</span><br><span class="line"> [ 0.9495916  -1.05437488 -0.42049501  0.3773612 ]</span><br><span class="line"> [ 1.13124423 -0.85379954 -0.19024378  0.06264126]</span><br><span class="line"> [ 1.70696485  1.63376764  1.22910949  0.8229693 ]</span><br><span class="line"> [ 0.52371324  1.02100318 -0.67235312  1.55466934]</span><br><span class="line"> [-1.08067913 -0.85278672  1.13408114 -0.858726  ]</span><br><span class="line"> [-0.18325687 -1.04998594 -0.00561227 -2.1281129 ]</span><br><span class="line"> [-1.49776284 -0.9074785   1.15403514  0.30422599]</span><br><span class="line"> [-0.06810748  0.31452186 -0.61717074  0.72793583]]</span><br></pre></td></tr></table></figure>
<p>fit_transform 方法是 fit 和 transform 的结合，fit_transform(X_train) 意思是找出 X_train 的和，并应用在 X_train 上。</p>
<p>这时对于 X_test，我们就可以直接使用 transform 方法。因为此时 StandardScaler 已经保存了 X_train 的和。</p>
<h2 id="几种归一化的区别">几种归一化的区别</h2>
<ul>
<li>Zero-mean normalization</li>
</ul>
<p>公式：$ X=(x-\mu)/\sigma $</p>
<p>这就是均值方差归一化，这样处理后的数据将符合标准正太分布，常用在一些通过距离得出相似度的聚类算法中，比如 K-means。</p>
<ul>
<li>Min-max normalization</li>
</ul>
<p>公式： $ X = (x-X_{min})/(x-X_{max}) $</p>
<p>min-max 归一化的手段是一种线性的归一化方法，它的特点是不会对数据分布产生影响。不过如果你的数据的最大最小值不是稳定的话，你的结果可能因此变得不稳定。min-max 归一化在图像处理上非常常用，因为大部分的像素值范围是 [0, 255]。</p>
<ul>
<li>Non-linear normaliztions</li>
</ul>
<p>非线性的归一化函数包含 log，exp，arctan, sigmoid 等等。用非线性归一化的函数取决于你的输入数据范围以及你期望的输出范围。比如 log() 函数在 [0, 1] 区间上有很强的区分度，arctan() 可以接收任意实数病转化到</p>
<p>区间，sigmoid 接收任意实数并映射到 (0, 1)。</p>
<ul>
<li>Length-one normalization</li>
</ul>
<p>公式：$X = \frac{x}{\left|x\right|} $</p>
<p>将特征转为单位向量的形式，可以剔除特征的强度的影响。这种处理用在不考虑向量大小而需要考虑向量方向的问题中，比如在一些文本情感的分类中，我们可能并不需要知道情感表达的强弱，而只要知道情感的类型，比如开心，生气等等。</p>
]]></content>
  </entry>
  <entry>
    <title>paddle2.0实现DNN（minst数据集）</title>
    <url>/2022/06/01/24aa2ecdd35b4df9b7f071254f4fb025/</url>
    <content><![CDATA[<p>[toc]</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/36f90c3a34e7678ec3333021264e1515.png" alt=""></p>
<p>实践总体过程和步骤如下图：<br>
<img src="https://img-blog.csdnimg.cn/img_convert/f521a48e6c9675424e764d25601daf6a.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle.fluid.dygraph <span class="keyword">import</span> Linear</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Python-依赖库">Python 依赖库</h2>
<p>numpy----------&gt;python 第三方库，用于进行科学计算</p>
<p>PIL------------&gt; Python Image Library,python 第三方图像处理库</p>
<p>matplotlib-----&gt;python 的绘图库 pyplot:matplotlib 的绘图框架</p>
<p>os-------------&gt;提供了丰富的方法来处理文件和目录</p>
<h2 id="数据准备">数据准备</h2>
<h3 id="数据集介绍">数据集介绍</h3>
<p>MNIST 数据集包含 60000 个训练集和 10000 测试数据集。分为图片和标签，图片是 28*28 的像素矩阵，标签为 0~9 共 10 个数字。</p>
<h3 id="train-reader-和-test-reader">train_reader 和 test_reader</h3>
<p>paddle.dataset.mnist.train()和 test()分别用于获取 mnist 训练集和测试集</p>
<p>使用 paddle.io.DataLoader()进行 batch 训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!mkdir -p /home/aistudio/.cache/paddle/dataset/mnist/</span><br><span class="line">!cp -r /home/aistudio/data/data65/*  /home/aistudio/.cache/paddle/dataset/mnist/</span><br><span class="line">!ls /home/aistudio/.cache/paddle/dataset/mnist/</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz</span><br><span class="line">t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BUF_SIZE = <span class="number">512</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line"><span class="comment">#用于训练的数据提供器，每次从缓存的数据项中随机读取批次大小的数据</span></span><br><span class="line">train_reader = paddle.batch(</span><br><span class="line">    paddle.reader.shuffle(paddle.dataset.mnist.train(),</span><br><span class="line">                          buf_size=BUF_SIZE),</span><br><span class="line">    batch_size=BATCH_SIZE)</span><br><span class="line"><span class="comment">#用于训练的数据提供器，每次从缓存的数据项中随机读取批次大小的数据</span></span><br><span class="line">test_reader = paddle.batch(</span><br><span class="line">    paddle.reader.shuffle(paddle.dataset.mnist.test(),</span><br><span class="line">                          buf_size=BUF_SIZE),</span><br><span class="line">    batch_size=BATCH_SIZE)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用于打印，查看mnist数据</span></span><br><span class="line">train_data = paddle.dataset.mnist.train();</span><br><span class="line">sampledata = <span class="built_in">next</span>(train_data())</span><br><span class="line"><span class="built_in">print</span>(sampledata)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(array([-1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -0.9764706 , -0.85882354, -0.85882354,</span><br><span class="line">       -0.85882354, -0.01176471,  0.06666672,  0.37254906, -0.79607844,</span><br><span class="line">        0.30196083,  1.        ,  0.9372549 , -0.00392157, -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -0.7647059 , -0.7176471 , -0.26274508,  0.20784318,</span><br><span class="line">        0.33333337,  0.9843137 ,  0.9843137 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.9843137 ,  0.7647059 ,  0.34901965,  0.9843137 ,  0.8980392 ,</span><br><span class="line">        0.5294118 , -0.4980392 , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -0.6156863 ,  0.8666667 ,</span><br><span class="line">        0.9843137 ,  0.9843137 ,  0.9843137 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.9843137 ,  0.9843137 ,  0.9843137 ,  0.96862745, -0.27058822,</span><br><span class="line">       -0.35686272, -0.35686272, -0.56078434, -0.69411767, -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -0.85882354,  0.7176471 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.9843137 ,  0.9843137 ,  0.9843137 ,  0.5529412 ,  0.427451  ,</span><br><span class="line">        0.9372549 ,  0.8901961 , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -0.372549  ,  0.22352946, -0.1607843 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.60784316, -0.9137255 , -1.        , -0.6627451 ,  0.20784318,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -0.8901961 ,</span><br><span class="line">       -0.99215686,  0.20784318,  0.9843137 , -0.29411763, -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        ,  0.09019613,</span><br><span class="line">        0.9843137 ,  0.4901961 , -0.9843137 , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -0.9137255 ,  0.4901961 ,  0.9843137 ,</span><br><span class="line">       -0.45098037, -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -0.7254902 ,  0.8901961 ,  0.7647059 ,  0.254902  ,</span><br><span class="line">       -0.15294117, -0.99215686, -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -0.36470586,  0.88235295,  0.9843137 ,  0.9843137 , -0.06666666,</span><br><span class="line">       -0.8039216 , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -0.64705884,</span><br><span class="line">        0.45882356,  0.9843137 ,  0.9843137 ,  0.17647064, -0.7882353 ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -0.8745098 , -0.27058822,</span><br><span class="line">        0.9764706 ,  0.9843137 ,  0.4666667 , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        ,  0.9529412 ,  0.9843137 ,</span><br><span class="line">        0.9529412 , -0.4980392 , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -0.6392157 ,  0.0196079 ,</span><br><span class="line">        0.43529415,  0.9843137 ,  0.9843137 ,  0.62352943, -0.9843137 ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -0.69411767,</span><br><span class="line">        0.16078436,  0.79607844,  0.9843137 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.9607843 ,  0.427451  , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -0.8117647 , -0.10588235,  0.73333335,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.9843137 ,  0.9843137 ,  0.5764706 , -0.38823527, -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -0.81960785, -0.4823529 ,  0.67058825,  0.9843137 ,</span><br><span class="line">        0.9843137 ,  0.9843137 ,  0.9843137 ,  0.5529412 , -0.36470586,</span><br><span class="line">       -0.9843137 , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -0.85882354,  0.3411765 ,  0.7176471 ,</span><br><span class="line">        0.9843137 ,  0.9843137 ,  0.9843137 ,  0.9843137 ,  0.5294118 ,</span><br><span class="line">       -0.372549  , -0.92941177, -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -0.5686275 ,  0.34901965,</span><br><span class="line">        0.77254903,  0.9843137 ,  0.9843137 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.9137255 ,  0.04313731, -0.9137255 , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        ,  0.06666672,  0.9843137 ,  0.9843137 ,  0.9843137 ,</span><br><span class="line">        0.6627451 ,  0.05882359,  0.03529418, -0.8745098 , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        , -1.        ,</span><br><span class="line">       -1.        , -1.        , -1.        , -1.        ], dtype=float32), 5)</span><br></pre></td></tr></table></figure>
<p>可以看出 数值为-1 表示灰度为 0，其余数值范围为[-1, 1]对应灰度 0~255</p>
<h2 id="网络配置">网络配置</h2>
<p>以下的代码判断就是定义一个简单的多层感知器，一共有三层，两个大小为 100 的隐层和一个大小为 10 的输出层，因为 MNIST 数据集是手写 0 到 9 的灰度图像，类别有 10 个，所以最后的输出大小是 10。最后输出层的激活函数是 Softmax，所以最后的输出层相当于一个分类器。加上一个输入层的话，多层感知器的结构是：输入层–&gt;&gt;隐层–&gt;&gt;隐层–&gt;&gt;输出层。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/934c7620b7133db2eee0d0a9ef90185b.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义多层感知器</span></span><br><span class="line"><span class="comment"># 动态图定义多层感知器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">multilayer_perceptron</span>(paddle.fluid.dygraph.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(multilayer_perceptron,self).__init__()</span><br><span class="line">        self.fc1 = Linear(input_dim=<span class="number">28</span>*<span class="number">28</span>, output_dim=<span class="number">100</span>, act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.fc2 = Linear(input_dim=<span class="number">100</span>, output_dim=<span class="number">100</span>, act=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.fc3 = Linear(input_dim=<span class="number">100</span>, output_dim=<span class="number">10</span>,act=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_</span>):</span><br><span class="line">        x = paddle.fluid.layers.reshape(input_, [input_.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        y = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示模型训练曲线</span></span><br><span class="line">all_train_iter=<span class="number">0</span></span><br><span class="line">all_train_iters=[]</span><br><span class="line">all_train_costs=[]</span><br><span class="line">all_train_accs=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_train_process</span>(<span class="params">title,iters,costs,accs,label_cost,lable_acc</span>):</span><br><span class="line">    plt.title(title, fontsize=<span class="number">24</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;cost/acc&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.plot(iters, costs,color=<span class="string">&#x27;red&#x27;</span>,label=label_cost)</span><br><span class="line">    plt.plot(iters, accs,color=<span class="string">&#x27;green&#x27;</span>,label=lable_acc)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_process</span>(<span class="params">title,color,iters,data,label</span>):</span><br><span class="line">    plt.title(title, fontsize=<span class="number">24</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(label, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.plot(iters, data,color=color,label=label)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">训练并保存模型</span></span><br><span class="line"><span class="string">训练需要有一个训练程序和一些必要参数，并构建了一个获取训练过程中测试误差的函数。必要参数有executor,program,reader,feeder,fetch_list。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 用动态图进行训练</span></span><br><span class="line">all_train_iter=<span class="number">0</span></span><br><span class="line">all_train_iters=[]</span><br><span class="line">all_train_costs=[]</span><br><span class="line">all_train_accs=[]</span><br><span class="line"></span><br><span class="line">best_test_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> paddle.fluid.dygraph.guard():</span><br><span class="line">    model = multilayer_perceptron() <span class="comment"># 模型实例化</span></span><br><span class="line">    model.train() <span class="comment"># 训练模式</span></span><br><span class="line">    <span class="comment"># ExponentialDecay?</span></span><br><span class="line">    opt = paddle.fluid.optimizer.Adam(learning_rate=paddle.fluid.dygraph.ExponentialDecay(</span><br><span class="line">              learning_rate=<span class="number">0.001</span>,</span><br><span class="line">              decay_steps=<span class="number">4000</span>,</span><br><span class="line">              decay_rate=<span class="number">0.1</span>,</span><br><span class="line">              staircase=<span class="literal">True</span>), parameter_list=model.parameters())</span><br><span class="line"></span><br><span class="line">    epochs_num = <span class="number">10</span> <span class="comment">#迭代次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> pass_num <span class="keyword">in</span> <span class="built_in">range</span>(epochs_num):</span><br><span class="line">        lr = opt.current_step_lr()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;learning-rate:&quot;</span>, lr)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch_id,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_reader()):</span><br><span class="line">            images = np.array([x[<span class="number">0</span>].reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data],np.float32)</span><br><span class="line"></span><br><span class="line">            labels = np.array([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data]).astype(<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">            labels = labels[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">            image = paddle.fluid.dygraph.to_variable(images)</span><br><span class="line">            label = paddle.fluid.dygraph.to_variable(labels)</span><br><span class="line">            predict = model(image)<span class="comment">#预测</span></span><br><span class="line">            <span class="comment">#print(predict)</span></span><br><span class="line">            loss = paddle.fluid.layers.cross_entropy(predict,label)</span><br><span class="line">            avg_loss = paddle.fluid.layers.mean(loss)<span class="comment">#获取loss值</span></span><br><span class="line"></span><br><span class="line">            acc = paddle.fluid.layers.accuracy(predict,label)<span class="comment">#计算精度</span></span><br><span class="line">            avg_loss.backward()</span><br><span class="line">            opt.minimize(avg_loss)</span><br><span class="line">            model.clear_gradients()</span><br><span class="line"></span><br><span class="line">            all_train_iter = all_train_iter + <span class="number">256</span></span><br><span class="line">            all_train_iters.append(all_train_iter)</span><br><span class="line">            all_train_costs.append(loss.numpy()[<span class="number">0</span>])</span><br><span class="line">            all_train_accs.append(acc.numpy()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_id!=<span class="number">0</span> <span class="keyword">and</span> batch_id%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;epoch:&#123;&#125;, batch_id:&#123;&#125;, train_loss:&#123;&#125;, train_acc:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(pass_num+<span class="number">1</span>, batch_id, avg_loss.numpy(), acc.numpy()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> paddle.fluid.dygraph.guard():</span><br><span class="line">            accs = []</span><br><span class="line">            model.<span class="built_in">eval</span>()<span class="comment">#评估模式</span></span><br><span class="line">            <span class="keyword">for</span> batch_id,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_reader()):<span class="comment">#测试集</span></span><br><span class="line">                images = np.array([x[<span class="number">0</span>].reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data],np.float32)</span><br><span class="line">                labels = np.array([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> data]).astype(<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">                labels = labels[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">                image = paddle.fluid.dygraph.to_variable(images)</span><br><span class="line">                label = paddle.fluid.dygraph.to_variable(labels)</span><br><span class="line"></span><br><span class="line">                predict = model(image)<span class="comment">#预测</span></span><br><span class="line">                acc = paddle.fluid.layers.accuracy(predict,label)</span><br><span class="line">                accs.append(acc.numpy()[<span class="number">0</span>])</span><br><span class="line">                avg_acc = np.mean(accs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> avg_acc &gt;= best_test_acc:</span><br><span class="line">                best_test_acc = avg_acc</span><br><span class="line">                <span class="keyword">if</span> pass_num &gt; <span class="number">10</span>:</span><br><span class="line">                    paddle.fluid.save_dygraph(model.state_dict(), <span class="string">&#x27;./work/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(pass_num))<span class="comment">#保存模型</span></span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Test:%d, Accuracy:%0.5f, Best: %0.5f&#x27;</span>%  (pass_num, avg_acc, best_test_acc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    paddle.fluid.save_dygraph(model.state_dict(),<span class="string">&#x27;./work/fashion_mnist_epoch&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epochs_num))<span class="comment">#保存模型</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练模型保存完成！&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;best_test_acc&quot;</span>, best_test_acc)</span><br><span class="line">draw_train_process(<span class="string">&quot;training&quot;</span>,all_train_iters,all_train_costs,all_train_accs,<span class="string">&quot;trainning cost&quot;</span>,<span class="string">&quot;trainning acc&quot;</span>)</span><br><span class="line">draw_process(<span class="string">&quot;trainning loss&quot;</span>,<span class="string">&quot;red&quot;</span>,all_train_iters,all_train_costs,<span class="string">&quot;trainning loss&quot;</span>)</span><br><span class="line">draw_process(<span class="string">&quot;trainning acc&quot;</span>,<span class="string">&quot;green&quot;</span>,all_train_iters,all_train_accs,<span class="string">&quot;trainning acc&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:1, batch_id:50, train_loss:[0.33342597], train_acc:[0.8984375]</span><br><span class="line">epoch:1, batch_id:100, train_loss:[0.6477896], train_acc:[0.78125]</span><br><span class="line">epoch:1, batch_id:150, train_loss:[0.38204402], train_acc:[0.9140625]</span><br><span class="line">epoch:1, batch_id:200, train_loss:[0.29537392], train_acc:[0.90625]</span><br><span class="line">epoch:1, batch_id:250, train_loss:[0.29159826], train_acc:[0.9140625]</span><br><span class="line">epoch:1, batch_id:300, train_loss:[0.39459157], train_acc:[0.8671875]</span><br><span class="line">epoch:1, batch_id:350, train_loss:[0.25907594], train_acc:[0.9296875]</span><br><span class="line">epoch:1, batch_id:400, train_loss:[0.31777298], train_acc:[0.90625]</span><br><span class="line">epoch:1, batch_id:450, train_loss:[0.16258541], train_acc:[0.9375]</span><br><span class="line">Test:0, Accuracy:0.92524, Best: 0.92524</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:2, batch_id:50, train_loss:[0.14996889], train_acc:[0.9453125]</span><br><span class="line">epoch:2, batch_id:100, train_loss:[0.2086468], train_acc:[0.9375]</span><br><span class="line">epoch:2, batch_id:150, train_loss:[0.13732132], train_acc:[0.953125]</span><br><span class="line">epoch:2, batch_id:200, train_loss:[0.20005819], train_acc:[0.9375]</span><br><span class="line">epoch:2, batch_id:250, train_loss:[0.22621125], train_acc:[0.921875]</span><br><span class="line">epoch:2, batch_id:300, train_loss:[0.23624715], train_acc:[0.9375]</span><br><span class="line">epoch:2, batch_id:350, train_loss:[0.22858979], train_acc:[0.921875]</span><br><span class="line">epoch:2, batch_id:400, train_loss:[0.15868747], train_acc:[0.9453125]</span><br><span class="line">epoch:2, batch_id:450, train_loss:[0.17579108], train_acc:[0.96875]</span><br><span class="line">Test:1, Accuracy:0.95431, Best: 0.95431</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:3, batch_id:50, train_loss:[0.09384024], train_acc:[0.9765625]</span><br><span class="line">epoch:3, batch_id:100, train_loss:[0.14337152], train_acc:[0.953125]</span><br><span class="line">epoch:3, batch_id:150, train_loss:[0.09826898], train_acc:[0.96875]</span><br><span class="line">epoch:3, batch_id:200, train_loss:[0.12162703], train_acc:[0.953125]</span><br><span class="line">epoch:3, batch_id:250, train_loss:[0.16990048], train_acc:[0.9375]</span><br><span class="line">epoch:3, batch_id:300, train_loss:[0.11993235], train_acc:[0.9765625]</span><br><span class="line">epoch:3, batch_id:350, train_loss:[0.04041685], train_acc:[0.9921875]</span><br><span class="line">epoch:3, batch_id:400, train_loss:[0.10029075], train_acc:[0.9765625]</span><br><span class="line">epoch:3, batch_id:450, train_loss:[0.20086782], train_acc:[0.9453125]</span><br><span class="line">Test:2, Accuracy:0.96034, Best: 0.96034</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:4, batch_id:50, train_loss:[0.10540008], train_acc:[0.96875]</span><br><span class="line">epoch:4, batch_id:100, train_loss:[0.06458011], train_acc:[0.96875]</span><br><span class="line">epoch:4, batch_id:150, train_loss:[0.0674578], train_acc:[0.96875]</span><br><span class="line">epoch:4, batch_id:200, train_loss:[0.09675008], train_acc:[0.9609375]</span><br><span class="line">epoch:4, batch_id:250, train_loss:[0.15608555], train_acc:[0.9609375]</span><br><span class="line">epoch:4, batch_id:300, train_loss:[0.09341267], train_acc:[0.9609375]</span><br><span class="line">epoch:4, batch_id:350, train_loss:[0.1041307], train_acc:[0.9609375]</span><br><span class="line">epoch:4, batch_id:400, train_loss:[0.07487246], train_acc:[0.9765625]</span><br><span class="line">epoch:4, batch_id:450, train_loss:[0.15261263], train_acc:[0.96875]</span><br><span class="line">Test:3, Accuracy:0.96351, Best: 0.96351</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:5, batch_id:50, train_loss:[0.07081573], train_acc:[0.984375]</span><br><span class="line">epoch:5, batch_id:100, train_loss:[0.12329036], train_acc:[0.9453125]</span><br><span class="line">epoch:5, batch_id:150, train_loss:[0.11128808], train_acc:[0.96875]</span><br><span class="line">epoch:5, batch_id:200, train_loss:[0.03693299], train_acc:[0.9921875]</span><br><span class="line">epoch:5, batch_id:250, train_loss:[0.06550381], train_acc:[0.9609375]</span><br><span class="line">epoch:5, batch_id:300, train_loss:[0.11091305], train_acc:[0.96875]</span><br><span class="line">epoch:5, batch_id:350, train_loss:[0.05953867], train_acc:[0.9921875]</span><br><span class="line">epoch:5, batch_id:400, train_loss:[0.05256216], train_acc:[0.984375]</span><br><span class="line">epoch:5, batch_id:450, train_loss:[0.04102388], train_acc:[0.984375]</span><br><span class="line">Test:4, Accuracy:0.96381, Best: 0.96381</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:6, batch_id:50, train_loss:[0.08369304], train_acc:[0.96875]</span><br><span class="line">epoch:6, batch_id:100, train_loss:[0.09292502], train_acc:[0.9609375]</span><br><span class="line">epoch:6, batch_id:150, train_loss:[0.13268939], train_acc:[0.9609375]</span><br><span class="line">epoch:6, batch_id:200, train_loss:[0.08329619], train_acc:[0.96875]</span><br><span class="line">epoch:6, batch_id:250, train_loss:[0.11900125], train_acc:[0.96875]</span><br><span class="line">epoch:6, batch_id:300, train_loss:[0.08534286], train_acc:[0.953125]</span><br><span class="line">epoch:6, batch_id:350, train_loss:[0.11742742], train_acc:[0.953125]</span><br><span class="line">epoch:6, batch_id:400, train_loss:[0.09688846], train_acc:[0.9765625]</span><br><span class="line">epoch:6, batch_id:450, train_loss:[0.02995617], train_acc:[1.]</span><br><span class="line">Test:5, Accuracy:0.96173, Best: 0.96381</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:7, batch_id:50, train_loss:[0.05730037], train_acc:[0.96875]</span><br><span class="line">epoch:7, batch_id:100, train_loss:[0.02739977], train_acc:[0.9921875]</span><br><span class="line">epoch:7, batch_id:150, train_loss:[0.04557585], train_acc:[0.9765625]</span><br><span class="line">epoch:7, batch_id:200, train_loss:[0.05771943], train_acc:[0.9765625]</span><br><span class="line">epoch:7, batch_id:250, train_loss:[0.06323972], train_acc:[0.9609375]</span><br><span class="line">epoch:7, batch_id:300, train_loss:[0.0729816], train_acc:[0.9765625]</span><br><span class="line">epoch:7, batch_id:350, train_loss:[0.03425251], train_acc:[0.9921875]</span><br><span class="line">epoch:7, batch_id:400, train_loss:[0.13220268], train_acc:[0.9609375]</span><br><span class="line">epoch:7, batch_id:450, train_loss:[0.0768251], train_acc:[0.96875]</span><br><span class="line">Test:6, Accuracy:0.96529, Best: 0.96529</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:8, batch_id:50, train_loss:[0.02684894], train_acc:[0.9921875]</span><br><span class="line">epoch:8, batch_id:100, train_loss:[0.05457066], train_acc:[0.9921875]</span><br><span class="line">epoch:8, batch_id:150, train_loss:[0.06887776], train_acc:[0.9765625]</span><br><span class="line">epoch:8, batch_id:200, train_loss:[0.01996839], train_acc:[1.]</span><br><span class="line">epoch:8, batch_id:250, train_loss:[0.07040852], train_acc:[0.96875]</span><br><span class="line">epoch:8, batch_id:300, train_loss:[0.02762877], train_acc:[0.9921875]</span><br><span class="line">epoch:8, batch_id:350, train_loss:[0.0307516], train_acc:[0.9921875]</span><br><span class="line">epoch:8, batch_id:400, train_loss:[0.12568305], train_acc:[0.9609375]</span><br><span class="line">epoch:8, batch_id:450, train_loss:[0.03238961], train_acc:[0.9921875]</span><br><span class="line">Test:7, Accuracy:0.96232, Best: 0.96529</span><br><span class="line">learning-rate: 0.001</span><br><span class="line">epoch:9, batch_id:50, train_loss:[0.04035459], train_acc:[0.984375]</span><br><span class="line">epoch:9, batch_id:100, train_loss:[0.04379664], train_acc:[0.9921875]</span><br><span class="line">epoch:9, batch_id:150, train_loss:[0.0402751], train_acc:[0.9921875]</span><br><span class="line">epoch:9, batch_id:200, train_loss:[0.03802398], train_acc:[0.984375]</span><br><span class="line">epoch:9, batch_id:250, train_loss:[0.09821159], train_acc:[0.953125]</span><br><span class="line">epoch:9, batch_id:300, train_loss:[0.03633454], train_acc:[0.9921875]</span><br><span class="line">epoch:9, batch_id:350, train_loss:[0.065966], train_acc:[0.9609375]</span><br><span class="line">epoch:9, batch_id:400, train_loss:[0.1054427], train_acc:[0.984375]</span><br><span class="line">epoch:9, batch_id:450, train_loss:[0.08116379], train_acc:[0.9765625]</span><br><span class="line">Test:8, Accuracy:0.97943, Best: 0.97943</span><br><span class="line">learning-rate: 0.000100000005</span><br><span class="line">epoch:10, batch_id:50, train_loss:[0.02536881], train_acc:[0.9921875]</span><br><span class="line">epoch:10, batch_id:100, train_loss:[0.01205996], train_acc:[1.]</span><br><span class="line">epoch:10, batch_id:150, train_loss:[0.05764459], train_acc:[0.9765625]</span><br><span class="line">epoch:10, batch_id:200, train_loss:[0.04137428], train_acc:[0.984375]</span><br><span class="line">epoch:10, batch_id:250, train_loss:[0.05747751], train_acc:[0.9609375]</span><br><span class="line">epoch:10, batch_id:300, train_loss:[0.05138961], train_acc:[0.984375]</span><br><span class="line">epoch:10, batch_id:350, train_loss:[0.02714467], train_acc:[0.984375]</span><br><span class="line">epoch:10, batch_id:400, train_loss:[0.08042958], train_acc:[0.984375]</span><br><span class="line">epoch:10, batch_id:450, train_loss:[0.02294997], train_acc:[0.9921875]</span><br><span class="line">Test:9, Accuracy:0.97973, Best: 0.97973</span><br><span class="line">训练模型保存完成！</span><br><span class="line">best_test_acc 0.979727</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210118120522887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210118120527322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210118120531208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="模型预测">模型预测</h2>
<h3 id="图片预处理">图片预处理</h3>
<p>在预测之前，要对图像进行预处理。</p>
<p>首先进行灰度化，然后压缩图像大小为 28*28，接着将图像转换成一维向量，最后再对一维向量进行归一化处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">file</span>):</span><br><span class="line">    im = Image.<span class="built_in">open</span>(file).convert(<span class="string">&#x27;L&#x27;</span>)                        <span class="comment">#将RGB转化为灰度图像，L代表灰度图像，像素值在0~255之间</span></span><br><span class="line">    im = im.resize((<span class="number">28</span>, <span class="number">28</span>), Image.ANTIALIAS)                 <span class="comment">#resize image with high-quality 图像大小为28*28</span></span><br><span class="line">    im = np.array(im).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).astype(np.float32)<span class="comment">#返回新形状的数组,把它变成一个 numpy 数组以匹配数据馈送格式。</span></span><br><span class="line">    <span class="comment"># print(im)</span></span><br><span class="line">    im = im / <span class="number">255.0</span> * <span class="number">2.0</span> - <span class="number">1.0</span>                               <span class="comment">#归一化到【-1~1】之间</span></span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure>
<h3 id="使用-Matplotlib-工具显示这张图像并预测">使用 Matplotlib 工具显示这张图像并预测</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">infer_path=<span class="string">&#x27;/home/aistudio/data/data2394/infer_3.png&#x27;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(infer_path)</span><br><span class="line">plt.imshow(img)   <span class="comment">#根据数组绘制图像</span></span><br><span class="line">plt.show()        <span class="comment">#显示图像</span></span><br><span class="line">label_list = [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>, <span class="string">&quot;4&quot;</span>, <span class="string">&quot;5&quot;</span>, <span class="string">&quot;6&quot;</span>, <span class="string">&quot;7&quot;</span>, <span class="string">&quot;8&quot;</span>, <span class="string">&quot;9&quot;</span>, <span class="string">&quot;10&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">模型预测</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">para_state_dict = paddle.load(<span class="string">&quot;work/fashion_mnist_epoch5.pdparams&quot;</span>)</span><br><span class="line">model = multilayer_perceptron()</span><br><span class="line">model.set_state_dict(para_state_dict) <span class="comment">#加载模型参数</span></span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment">#训练模式</span></span><br><span class="line">infer_img = load_image(infer_path)</span><br><span class="line">infer_img = np.array(infer_img).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">infer_img = infer_img[np.newaxis,:, : ,:]</span><br><span class="line">infer_img = paddle.fluid.dygraph.to_variable(infer_img)</span><br><span class="line">result = model(infer_img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">infer_img = np.array(infer_img).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">infer_img = infer_img[np.newaxis,:, : ,:]</span><br><span class="line">infer_img = paddle.fluid.dygraph.to_variable(infer_img)</span><br><span class="line">result = model(infer_img)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;infer results: %s&quot;</span> % label_list[np.argmax(result.numpy())])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210118120538808.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">infer results: 3</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>TCP/IP简介</title>
    <url>/2022/05/31/297d89bb91884aed8f829de39ec9c84b/</url>
    <content><![CDATA[<p>[toc]</p>
<p>提到网络协议栈结构，最著名的当属 <strong>OSI</strong> 七层模型，但是 <strong>TCP/IP</strong> 协议族的结构则稍有不同，它们之间的层次结构有如图对应关系：</p>
<p><img src="/resource/6bcfd56189304b9c9509fdad7d35e760.png" alt="2022-05-16-22-24-50.png"></p>
<p>可见 <strong>TCP/IP</strong> 被分为 4 层，每层承担的任务不一样，各层的协议的工作方式也不一样，每层封装上层数据的方式也不一样：</p>
<ul>
<li>应用层：应用程序通过这一层访问网络，常见 FTP、HTTP、DNS 和 TELNET 协议；</li>
<li>传输层：TCP 协议和 UDP 协议；</li>
<li>网络层：IP 协议，ARP、RARP 协议，ICMP 协议等；</li>
<li>网络接口层：是 TCP/IP 协议的基层，负责数据帧的发送和接收。</li>
</ul>
<p>本笔记自底向上分层次对 <strong>TCP/IP</strong> 的各协议做介绍。</p>
<p><strong>主要知识点：</strong></p>
<ul>
<li>IP 地址</li>
<li>域名</li>
<li>MAC 地址</li>
<li>端口号</li>
<li>封装和分用</li>
</ul>
<h2 id="起源">起源</h2>
<p>上世纪 70 年代，随着计算机技术的发展，计算机使用者意识到：要想发挥计算机更大的作用，就要将世界各地的计算机连接起来。但是简单的连接是远远不够的，因为计算机之间无法沟通。因此设计一种通用的“语言”来交流是必不可少的，这时 <strong>TCP/IP</strong> 协议就应运而生了。</p>
<p><strong>TCP/IP</strong>(Transmission Control Protocol/Internet Protocol)是传输控制协议和网络协议的简称，它定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。</p>
<p><strong>TCP/IP</strong> 不是一个协议，而是<strong>一个协议族的统称</strong>，里面包括了 IP 协议、ICMP 协议、TCP 协议、以及 http、ftp、pop3 协议等。网络中的计算机都采用这套协议族进行互联。</p>
<h2 id="IP-地址">IP 地址</h2>
<p>网络上每一个节点都必须有一个独立的 IP 地址，通常使用的 IP 地址是一个 32bit 的数字，被 <code>.</code> 分成 4 组，例如，<code>255.255.255.255</code> 就是一个 IP 地址。有了 IP 地址，用户的计算机就可以发现并连接互联网中的另外一台计算机。</p>
<p>在 终端输入 <code>ifconfig -a</code> 命令查看自己的 IP 地址 (inet 地址)：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shiyanlou:project/ $ ifconfig -a</span><br><span class="line">eth0      Link encap:以太网  硬件地址 02:42:c0:a8:2a:04</span><br><span class="line">          inet 地址:192.168.42.4  广播:192.168.42.255  掩码:255.255.255.0</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  跃点数:1</span><br><span class="line">          接收数据包:568 错误:0 丢弃:0 过载:0 帧数:0</span><br><span class="line">          发送数据包:736 错误:0 丢弃:0 过载:0 载波:0</span><br><span class="line">          碰撞:0 发送队列长度:0</span><br><span class="line">          接收字节:64277 (64.2 KB)  发送字节:4434558 (4.4 MB)</span><br><span class="line"></span><br><span class="line">lo        Link encap:本地环回</span><br><span class="line">          inet 地址:127.0.0.1  掩码:255.0.0.0</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  跃点数:1</span><br><span class="line">          接收数据包:0 错误:0 丢弃:0 过载:0 帧数:0</span><br><span class="line">          发送数据包:0 错误:0 丢弃:0 过载:0 载波:0</span><br><span class="line">          碰撞:0 发送队列长度:1000</span><br></pre></td></tr></table></figure>
<h2 id="域名">域名</h2>
<p>用 12 位数字组成的 IP 地址很难记忆，在实际应用时，用户一般不需要记住 IP 地址，互联网给每个 IP 地址起了一个别名，习惯上称作域名。</p>
<p>域名与计算机的 IP 地址相对应，并把这种对应关系存储在域名服务系统 <strong>DNS(Domain Name System)</strong> 中，这样用户只需记住域名就可以与指定的计算机进行通信了。</p>
<p>常见的域名包括 <strong>com</strong>、<strong>net</strong> 和 <strong>org</strong> 三种<strong>顶级域名后缀</strong>，除此之外每个国家还有自己国家专属的域名后缀（比如我国的域名后缀为 <strong>cn</strong>）。目前经常使用的域名诸如百度（<a href="http://www.baidu.xn--comLinux-hm3gy983p">www.baidu.com）、Linux</a> 组织（<a href="http://www.lwn.net">www.lwn.net</a>）等等。</p>
<p>我们可以使用命令 <code>nslookup</code> 或者 <code>ping</code> 来查看与域名相对应的 IP 地址，由于实验楼网络限制，我们可以使用 <code>ping github.com</code>（如果 github 也 ping 不通，那么可以使用 <code>ping labfile.oss.aliyuncs.com</code>。</p>
<h2 id="MAC-地址">MAC 地址</h2>
<p><strong>MAC（Media Access Control）<strong>地址，或称为</strong>物理地址</strong>、<strong>硬件地址</strong>，用来定义互联网中设备的位置。</p>
<p>在 <strong>TCP/IP</strong> 层次模型中，<strong>网络层</strong>管理 IP 地址，<strong>链路层</strong>则负责 <strong>MAC</strong> 地址。因此每个<strong>网络位置</strong>会有一个专属于它的 <strong>IP</strong> 地址，而每个<strong>主机</strong>会有一个专属于它 <strong>MAC</strong> 地址。</p>
<h2 id="端口号">端口号</h2>
<p>IP 地址是用来发现和查找网络中的地址，但是不同程序如何互相通信呢？这就需要<strong>端口号</strong>来识别了。如果把 IP 地址比作一间房子，端口就是出入这间房子的门。真正的房子只有几个门，但是<strong>端口采用 16 比特</strong>的端口号标识，<strong>一个 IP 地址的端口可以有 65536</strong>（即：$2^{16}$）个之多！</p>
<p><strong>服务器的默认程序</strong>一般都是通过人们所熟知的端口号来识别的。例如，对于每个 <strong>TCP/IP</strong> 实现来说，<strong>SMTP</strong>（简单邮件传输协议）服务器的 <strong>TCP</strong> 端口号都是 <code>25</code>，<strong>FTP</strong>（文件传输协议）服务器的 <strong>TCP</strong> 端口号都是 <code>21</code>，<strong>TFTP</strong>（简单文件传输协议）服务器的 <strong>UDP</strong> 端口号都是 <code>69</code>。任何 <strong>TCP/IP</strong> 实现所提供的服务都用众所周知的 <code>1－1023</code> 之间的端口号。这些人们所熟知的端口号由 Internet 端口号分配机构（Internet Assigned Numbers Authority，IANA）来管理。</p>
<table>
<thead>
<tr>
<th>常用协议</th>
<th>端口号</th>
</tr>
</thead>
<tbody>
<tr>
<td>SSH</td>
<td>22</td>
</tr>
<tr>
<td>FTP</td>
<td>20 &amp;&amp; 21</td>
</tr>
<tr>
<td>Telnet</td>
<td>23</td>
</tr>
<tr>
<td>SMTP</td>
<td>25</td>
</tr>
<tr>
<td>TFTP</td>
<td>69</td>
</tr>
<tr>
<td>HTTP</td>
<td>80</td>
</tr>
<tr>
<td>SNMP</td>
<td>161</td>
</tr>
<tr>
<td>Ping</td>
<td>使用 ICMP，无具体端口号</td>
</tr>
</tbody>
</table>
<h2 id="封装和分用">封装和分用</h2>
<p><strong>封装</strong>：当应用程序发送数据的时候，数据在协议层次当中自顶向下通过每一层，每一层都会对数据增加一些首部或尾部信息，这样的信息称之为<strong>协议数据单元（Protocol Data Unit，缩写为 PDU）</strong>，在<strong>分层协议系统</strong>里，在指定的协议层上传送的数据单元，包含了该层的协议控制信息和用户信息。如下图所示：</p>
<ul>
<li>物理层（一层）PDU 指数据位（Bit）</li>
<li>数据链路层（二层）PDU 指数据帧（Frame）</li>
<li>网络层（三层）PDU 指数据包（Packet）</li>
<li>传输层（四层）PDU 指数据段（Segment）</li>
<li>第五层以上为数据（data）</li>
</ul>
<p><img src="/resource/6d0665ef36ce4f339c1ffa8b45e501f0.png" alt="2022-05-16-22-41-20.png"></p>
<p><strong>分用</strong>：当主机收到一个数据帧时，数据就从协议层底向上升，通过每一层时，检查并去掉对应层次的报文首部或尾部，与封装过程正好相反。</p>
<h2 id="RFC">RFC</h2>
<p><strong>RFC（Request for Comment）</strong> 文档是所有<strong>以太网协议的正式标准</strong>，并在其官网上面公布，由 IETF 标准协会制定。大量的 RFC 并不是正式的标准，出版的目的只是为了提供信息。RFC 的篇幅不一，从几页到几百页不等。每一种协议都用一个数字来标识，如 RFC 3720 是 iSCSI 协议的标准，数字越大意味着 RFC 的内容越新或者是对应的协议（标准）出现的比较晚。</p>
<p>所有的 RFC 文档都可以从网络上找到，其官网为 IETF。在网站上面可以通过分类以及搜索快速找到目标协议的 RFC 文档。目前在 IETF 网站上面的 RFC 文档有数千个，但是我们不需要全部掌握，在工作或学习中如果遇到可以找到对应的解释，理论与实际结合会有更好地效果，单纯阅读 RFC 的效果一般。</p>
]]></content>
  </entry>
  <entry>
    <title>paddle2.0高层API快速实现LeNet(MNIST手写数字识别)</title>
    <url>/2022/06/01/4b627c671c26401e94c82e1860ce030d/</url>
    <content><![CDATA[<p>[toc]</p>
<p>『深度学习 7 日打卡营·快速入门特辑』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
</ul>
<h2 id="DL-万能公式">DL 万能公式</h2>
<p><img src="https://img-blog.csdnimg.cn/img_convert/56bdbd3236130b53f7479709dc59ea81.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">paddle.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;2.0.0&#x27;</span><br></pre></td></tr></table></figure>
<h2 id="数据加载和预处理">数据加载和预处理</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载和预处理</span></span><br><span class="line"><span class="comment"># [0-255] -&gt; [0-1]</span></span><br><span class="line">transform = T.Normalize(mean=[<span class="number">127.5</span>], std=[<span class="number">127.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">train_dataset = paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估数据集</span></span><br><span class="line">eval_dataset = paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;训练集样本量:<span class="subst">&#123;<span class="built_in">len</span>(train_dataset)&#125;</span>，验证集样本量:<span class="subst">&#123;<span class="built_in">len</span>(eval_dataset)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练集样本量:60000，验证集样本量:10000</span><br></pre></td></tr></table></figure>
<h2 id="查看数据">查看数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(train_dataset[<span class="number">0</span>][<span class="number">0</span>].reshape([<span class="number">28</span>, <span class="number">28</span>]), cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;label:&quot;</span>, train_dataset[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;data shape:&quot;</span>, train_dataset[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2021020422541041.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">label: [5]</span><br><span class="line">data shape: (1, 28, 28)</span><br></pre></td></tr></table></figure>
<h2 id="搭建-LeNet-5-卷积神经网络">搭建 LeNet-5 卷积神经网络</h2>
<p>选用 LeNet-5 网络结构。</p>
<p>LeNet-5 模型源于论文“LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.”，</p>
<p>论文地址：<a href="https://ieeexplore.ieee.org/document/726791">https://ieeexplore.ieee.org/document/726791</a></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4a5b3f033d32b75503c754a43e08c39c.png" alt=""></p>
<p><strong>每个阶段用到的 Layer</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ec3b4651aceba70886267131d27db267.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 网络搭建</span></span><br><span class="line">net = paddle.nn.Sequential(</span><br><span class="line">                            (<span class="string">&#x27;C1&#x27;</span>, paddle.nn.Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)),</span><br><span class="line">                            <span class="comment"># 6x28x28</span></span><br><span class="line">                            (<span class="string">&#x27;ReLU1&#x27;</span>, paddle.nn.ReLU()),</span><br><span class="line">                            (<span class="string">&#x27;S2&#x27;</span>, paddle.nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)),</span><br><span class="line">                            <span class="comment"># 6x14x14</span></span><br><span class="line">                            (<span class="string">&#x27;C3&#x27;</span>, paddle.nn.Conv2D(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)),</span><br><span class="line">                            <span class="comment"># 16x10x10</span></span><br><span class="line">                            (<span class="string">&#x27;ReLU2&#x27;</span>, paddle.nn.ReLU()),</span><br><span class="line">                            (<span class="string">&#x27;S4&#x27;</span>, paddle.nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)),</span><br><span class="line">                            <span class="comment"># 16x5x5</span></span><br><span class="line">                            (<span class="string">&#x27;C5&#x27;</span>, paddle.nn.Conv2D(<span class="number">16</span>, <span class="number">120</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)),</span><br><span class="line">                            <span class="comment"># 120x1x1</span></span><br><span class="line">                            (<span class="string">&#x27;ReLU3&#x27;</span>, paddle.nn.ReLU()),</span><br><span class="line">                            (<span class="string">&#x27;ReLU4&#x27;</span>, paddle.nn.Flatten()),</span><br><span class="line">                            <span class="comment"># 120</span></span><br><span class="line">                            (<span class="string">&#x27;F6&#x27;</span>, paddle.nn.Linear(<span class="number">120</span>, <span class="number">84</span>)),</span><br><span class="line">                            <span class="comment"># 84</span></span><br><span class="line">                            (<span class="string">&#x27;ReLU5&#x27;</span>, paddle.nn.ReLU()),</span><br><span class="line">                            (<span class="string">&#x27;OUTPUT&#x27;</span>, paddle.nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line">                          )</span><br></pre></td></tr></table></figure>
<h2 id="网络模型可视化">网络模型可视化</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型封装</span></span><br><span class="line">model = paddle.Model(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型可视化</span></span><br><span class="line">model.summary((<span class="number">8</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))  <span class="comment"># n c h w</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line"> Layer (type)       Input Shape          Output Shape         Param #</span><br><span class="line">===========================================================================</span><br><span class="line">   Conv2D-1       [[8, 1, 28, 28]]      [8, 6, 28, 28]          60</span><br><span class="line">    ReLU-1        [[8, 6, 28, 28]]      [8, 6, 28, 28]           0</span><br><span class="line">  MaxPool2D-1     [[8, 6, 28, 28]]      [8, 6, 14, 14]           0</span><br><span class="line">   Conv2D-2       [[8, 6, 14, 14]]     [8, 16, 10, 10]         2,416</span><br><span class="line">    ReLU-2       [[8, 16, 10, 10]]     [8, 16, 10, 10]           0</span><br><span class="line">  MaxPool2D-2    [[8, 16, 10, 10]]      [8, 16, 5, 5]            0</span><br><span class="line">   Conv2D-3       [[8, 16, 5, 5]]       [8, 120, 1, 1]        48,120</span><br><span class="line">    ReLU-3        [[8, 120, 1, 1]]      [8, 120, 1, 1]           0</span><br><span class="line">   Flatten-1      [[8, 120, 1, 1]]         [8, 120]              0</span><br><span class="line">   Linear-1          [[8, 120]]            [8, 84]            10,164</span><br><span class="line">    ReLU-4           [[8, 84]]             [8, 84]               0</span><br><span class="line">   Linear-2          [[8, 84]]             [8, 10]              850</span><br><span class="line">===========================================================================</span><br><span class="line">Total params: 61,610</span><br><span class="line">Trainable params: 61,610</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.02</span><br><span class="line">Forward/backward pass size (MB): 0.90</span><br><span class="line">Params size (MB): 0.24</span><br><span class="line">Estimated Total Size (MB): 1.16</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&#x27;total_params&#x27;: 61610, &#x27;trainable_params&#x27;: 61610&#125;</span><br></pre></td></tr></table></figure>
<h2 id="模型配置">模型配置</h2>
<ul>
<li>优化器：SGD</li>
<li>损失函数：交叉熵（cross entropy）</li>
<li>评估指标：Accuracy</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置优化器，损失函数，评估指标</span></span><br><span class="line">model.prepare(optimizer=paddle.optimizer.Adam(learning_rate=<span class="number">0.001</span>, parameters=net.parameters()),</span><br><span class="line">              loss=paddle.nn.CrossEntropyLoss(),</span><br><span class="line">              metrics=paddle.metric.Accuracy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动模型全流程训练</span></span><br><span class="line">model.fit(train_data=train_dataset,</span><br><span class="line">          eval_data=eval_dataset,</span><br><span class="line">          batch_size=<span class="number">64</span>,</span><br><span class="line">          epochs=<span class="number">5</span>,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">D:\Anaconda3\envs\paddle2\lib\site-packages\paddle\fluid\layers\utils.py:77: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated since Python 3.3,and in 3.9 it will stop working</span><br><span class="line">  return (isinstance(seq, collections.Sequence) and</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">step 938/938 [==============================] - loss: 0.0460 - acc: 0.9391 - 14ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 157/157 [==============================] - loss: 0.0032 - acc: 0.9759 - 11ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 2/5</span><br><span class="line">step 938/938 [==============================] - loss: 0.0375 - acc: 0.9801 - 14ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 157/157 [==============================] - loss: 0.0014 - acc: 0.9863 - 8ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 3/5</span><br><span class="line">step 938/938 [==============================] - loss: 0.0199 - acc: 0.9850 - 13ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 157/157 [==============================] - loss: 0.0128 - acc: 0.9847 - 15ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 4/5</span><br><span class="line">step 938/938 [==============================] - loss: 0.0043 - acc: 0.9884 - 21ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 157/157 [==============================] - loss: 0.0019 - acc: 0.9836 - 8ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 5/5</span><br><span class="line">step 938/938 [==============================] - loss: 0.0069 - acc: 0.9914 - 14ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 157/157 [==============================] - loss: 2.2102e-04 - acc: 0.9884 - 14ms/step</span><br><span class="line">Eval samples: 10000</span><br></pre></td></tr></table></figure>
<h2 id="模型评估">模型评估</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = model.evaluate(eval_dataset, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10000/10000 [==============================] - loss: 2.0623e-05 - acc: 0.9884 - 4ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">&#123;&#x27;loss&#x27;: [2.0622994e-05], &#x27;acc&#x27;: 0.9884&#125;</span><br></pre></td></tr></table></figure>
<h2 id="模型预测-2">模型预测</h2>
<h3 id="批量预测">批量预测</h3>
<p>使用<code>model.predit</code>接口完成对大量数据集的批量预测</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = model.predict(eval_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义画图方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_img</span>(<span class="params">img, predict</span>):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&#x27;predict:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(predict))</span><br><span class="line">    plt.imshow(img.reshape([<span class="number">28</span>, <span class="number">28</span>]), cmap=plt.cm.binary)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽样展示</span></span><br><span class="line">indexs = [<span class="number">2</span>, <span class="number">15</span>, <span class="number">38</span>, <span class="number">211</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> indexs:</span><br><span class="line">    show_img(eval_dataset[idx][<span class="number">0</span>], np.argmax(result[<span class="number">0</span>][idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 10000/10000 [==============================] - 4ms/step</span><br><span class="line">Predict samples: 10000</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210204225453567.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210204225457385.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210204225501944.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/2021020422550743.png" alt="在这里插入图片描述"></p>
<h2 id="单张图片预测">单张图片预测</h2>
<p>采用<code>model.predict_batch</code>来进行单张或者少量多张图片的预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取单张图片</span></span><br><span class="line">img = eval_dataset[<span class="number">233</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">result = model.predict_batch([img[np.newaxis, ...]])  <span class="comment"># 需要多添加一个batch轴，不然报错</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">show_img(img, np.argmax(result))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[array([[-3.4706905, -6.674865 , -1.9018929,  3.8094432, -5.66697  ,</span><br><span class="line">         1.5752668, -6.6928353, -2.2028043,  9.449063 ,  3.296681 ]],</span><br><span class="line">      dtype=float32)]</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210204225512156.png" alt="在这里插入图片描述"></p>
<h2 id="部署上线">部署上线</h2>
<h3 id="保存模型">保存模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;finetuning/mnist&#x27;</span>, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="继续调优训练">继续调优训练</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.static <span class="keyword">import</span> InputSpec</span><br><span class="line"></span><br><span class="line">model_2 = paddle.Model(net, inputs=[InputSpec(shape=[-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>], dtype=<span class="string">&#x27;float32&#x27;</span>, name=<span class="string">&#x27;image&#x27;</span>)])</span><br><span class="line"></span><br><span class="line">model_2.load(<span class="string">&#x27;./finetuning/mnist&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置优化器，损失函数，评估指标</span></span><br><span class="line">model_2.prepare(optimizer=paddle.optimizer.Adam(learning_rate=<span class="number">0.0001</span>, parameters=net.parameters()),</span><br><span class="line">              loss=paddle.nn.CrossEntropyLoss(),</span><br><span class="line">              metrics=paddle.metric.Accuracy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动模型全流程训练</span></span><br><span class="line">model_2.fit(train_data=train_dataset,</span><br><span class="line">            eval_data=eval_dataset,</span><br><span class="line">            batch_size=<span class="number">64</span>,</span><br><span class="line">            epochs=<span class="number">1</span>,</span><br><span class="line">            verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/1</span><br><span class="line">step 938/938 [==============================] - loss: 3.6613e-04 - acc: 0.9966 - 13ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 157/157 [==============================] - loss: 7.5415e-05 - acc: 0.9908 - 9ms/step</span><br><span class="line">Eval samples: 10000</span><br></pre></td></tr></table></figure>
<h2 id="保存预测模型">保存预测模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_2.save(<span class="string">&#x27;./infer/mnist&#x27;</span>, training=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>MobileNet v1/v2</title>
    <url>/2022/06/01/5c4327cec0e84a29a399ea713e35e1e2/</url>
    <content><![CDATA[<p>[toc]</p>
<p>卷积神经网络（CNN）已经普遍应用在计算机视觉领域，并且已经取得了不错的效果。图 1 为近几年来 CNN 在 ImageNet 竞赛的表现，可以看到为了追求分类准确度，模型深度越来越深，模型复杂度也越来越高，如深度残差网络（ResNet）其层数已经多达 152 层。</p>
<p>然而，在某些真实的应用场景如移动或者嵌入式设备，如此大而复杂的模型是难以被应用的。首先是模型过于庞大，面临着内存不足的问题，其次这些场景要求低延迟，或者说响应速度要快，想象一下自动驾驶汽车的行人检测系统如果速度很慢会发生什么可怕的事情。所以，研究小而高效的 CNN 模型在这些场景至关重要，至少目前是这样，尽管未来硬件也会越来越快。目前的研究总结来看分为两个方向：一是对训练好的复杂模型进行压缩得到小模型；二是直接设计小模型并进行训练。不管如何，其目标在保持模型性能（accuracy）的前提下降低模型大小（parameters size），同时提升模型速度（speed, low latency）。本文的主角<code>MobileNet</code>属于后者，其是 Google 最近提出的一种小巧而高效的 CNN 模型，其在 accuracy 和 latency 之间做了折中。</p>
<h2 id="DW-PW-卷积">DW &amp; PW 卷积</h2>
<p>MobileNet_V1 引入<code>DW</code>和<code>PW</code>卷积，减少了计算量和参数个数。</p>
<p><img src="https://img-blog.csdnimg.cn/20210204142714967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="深度可分离卷积">深度可分离卷积</h3>
<p>MobileNet 的基本单元是深度级可分离卷积（depthwise separable convolution）</p>
<p>深度级可分离卷积其实是一种可分解卷积操作（factorized convolutions），其可以分解为两个更小的操作：<code>depthwise convolution</code>和<code>pointwise convolution</code>，如<code>Figure 2</code>所示。</p>
<p><code>Depthwise convolution</code>和标准卷积不同，对于标准卷积其卷积核是用在所有的输入通道上（input channels），而 depthwise convolution 针对每个输入通道采用不同的卷积核，就是说一个卷积核对应一个输入通道，所以说 depthwise convolution 是 depth 级别的操作。而<code>pointwise convolution</code>其实就是普通的卷积，只不过其采用$1x1$的卷积核。</p>
<p><img src="https://img-blog.csdnimg.cn/20210204142742135.png" alt="在这里插入图片描述"></p>
<p>上图中更清晰地展示了两种操作，左边为<code>DW</code>卷积，右边为<code>PW</code>卷积。</p>
<p>对于<code>depthwise separable convolution</code>，其首先是采用<code>depthwise convolution</code>对不同输入通道分别进行卷积，然后采用<code>pointwise convolution</code>将上面的输出再进行结合，这样其实整体效果和一个标准卷积是差不多的，但是会大大减少计算量和模型参数量。</p>
<p>假设卷积核大小为$D_k\times D_k$，输入通道数为$M$，输出通道数为$N$，特征图的大小为$D_F \times D_F$。</p>
<blockquote>
<p>计算量压缩：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\frac&#123;D_k\cdot D_k\cdot M\cdot D_F\cdot D_F + M\cdot N\cdot D_F\cdot D_F&#125;&#123;D_k\cdot D_k\cdot M\cdot N\cdot D_F\cdot D_F&#125; = \frac&#123;1&#125;&#123;N&#125;+\frac&#123;1&#125;&#123;D_K^2&#125;</span><br><span class="line">$$</span><br><span class="line">可以得出采用$3\times 3$卷积一般可以减少8~9倍的计算量</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参数压缩：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\frac&#123;D_K^2\times M + 1 \times M\times N&#125;&#123;D_K^2\times M\times N&#125; = \frac&#123;1&#125;&#123;N&#125;+\frac&#123;1&#125;&#123;D_k^2&#125;</span><br><span class="line">$$</span><br><span class="line">若 $D_k$ = 3，参数量大约会减少到原来的 1/8 ~ 1/9</span><br></pre></td></tr></table></figure>
<h2 id="Conv-卷积结构">Conv 卷积结构</h2>
<p>论文中全部采用 Conv 实现，没有采用池化层，减少了一定的计算量<br>
下面是 MobileNet 的卷积结构：</p>
<p><img src="https://img-blog.csdnimg.cn/20210204142755708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>用<code>Conv/s2</code>，即步长为 2 的卷积代替<code>Maxpooling + Conv</code>，使得参数数量不变，计算量变为原来的 1/4 左右，且省去了 MaxPool 的计算量</p>
<h3 id="MobileNet-模型精简">MobileNet 模型精简</h3>
<p>前面说的<code>MobileNet</code>的基准模型，但是有时候你需要更小的模型，那么就要对<code>MobileNet</code>瘦身了。<br>
这里引入了两个超参数：<code>width multiplier</code>和<code>resolution multiplier</code>。</p>
<ul>
<li>
<p>Width Multiplier($\alpha$): Thinner Models</p>
<p>第一个参数<code>width multiplier</code>主要是按比例减少通道数，该参数记为 $\alpha$ ，其取值范围为$(0,1]$，那么输入与输出通道数将变成 $\alpha M$ 和 $\alpha N$ ，对于<code>depthwise separable convolution</code>，其计算量变为：</p>
<p>$$<br>
D_{K}\times D_{K}\times \alpha M\times D_{F}\times D_{F}+ \alpha M\times \alpha N\times D_{F}\times D_{F}<br>
$$</p>
<ul>
<li>所有层的 通道数（channel）乘以$\alpha$(四舍五入)，模型大小近似下降到原来的$\alpha^2$倍，计算量下降到原来的$\alpha^2$倍</li>
<li>$\alpha \in (0,1]$，典型值为 1, 0.75, 0.5, 0.25，降低模型的宽度</li>
</ul>
</li>
<li>
<p>Resolution Multiplier($\rho$): Reduced Representation</p>
<p>因为主要计算量在后一项，所以<code>width multiplier</code>可以按照比例降低计算量，其是参数量也会下降。第二个参数<code>resolution multiplier</code>主要是按比例降低特征图的大小，记为 $\rho$ ，比如原来输入特征图是$224\times 224$，可以减少为$192\times 192$，加上<code>resolution multiplier</code>，<code>depthwise separable convolution</code>的计算量为：</p>
<p>$$<br>
D_{K}\times D_{K}\times \alpha M\times \rho D_{F}\times \rho D_{F}+ \alpha M\times \alpha N\times \rho D_{F}\times \rho D_{F}<br>
$$</p>
<ul>
<li>输入层的分辨率乘以$\rho$参数（四舍五入），等价于所有层的分辨率乘以$\rho$，模型大小不变，计算朗下降到原来的$\rho^2$倍</li>
</ul>
<p>-$\rho \in (0,1]$，降低输入图像的分辨率</p>
</li>
</ul>
<p>要说明的是，<code>resolution multiplier</code>仅仅影响计算量，但是不改变参数量。引入两个参数会给肯定会降低<code>MobileNet</code>的性能，具体实验分析可以见 paper，总结来看是在<code>accuracy</code>和<code>computation</code>，以及<code>accuracy</code>和<code>model size</code>之间做折中。</p>
<h2 id="MobileNet-V1-网络结构">MobileNet_V1 网络结构</h2>
<p><img src="https://img-blog.csdnimg.cn/20210204142819385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks">MobileNetV2:Inverted Residuals and Linear Bottlenecks</h2>
<h3 id="主要改进点">主要改进点</h3>
<ul>
<li>引入倒残差结构，先升维再降维，增强梯度的传播，显著减少推理期间所需的内存占用（Inverted Residuals）</li>
<li>去掉 <code>Narrow layer(low dimension or depth)</code> 后的 <code>ReLU</code>，保留特征多样性，增强网络的表达能力（Linear Bottlenecks）</li>
<li>网络为全卷积，使得模型可以适应不同尺寸的图像；使用 <code>RELU6</code>（最高输出为 6）激活函数，使得模型在低精度计算下具有更强的鲁棒性</li>
<li>MobileNetV2 Inverted residual block 如下所示，若需要下采样，可在 <code>DW</code> 时采用步长为 2 的卷积</li>
<li>小网络使用小的扩张系数（expansion factor），大网络使用大一点的扩张系数（expansion factor），推荐是 5~10，论文中 t = 6 t = 6t=6</li>
</ul>
<h2 id="倒残差结构（Inverted-residual-block）">倒残差结构（Inverted residual block）</h2>
<p><code>ResNet</code>的 Bottleneck 结构是降维-&gt;卷积-&gt;升维，是两边细中间粗</p>
<p>而<code>MobileNetV2</code>是先升维（6 倍）-&gt; 卷积 -&gt; 降维，是沙漏形。</p>
<p><img src="https://img-blog.csdnimg.cn/20210204142830500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>区别于 MobileNetV1, MobileNetV2 的卷积结构如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20210204142838923.png" alt="在这里插入图片描述"></p>
<p>因为 DW 卷积不改变通道数，所以如果上一层的通道数很低时，DW 只能在低维空间提取特征，效果不好。所以 V2 版本在 DW 前面加了一层 PW 用来升维。</p>
<p>同时 V2 去除了第二个 PW 的激活函数改用线性激活，因为激活函数在高维空间能够有效地增加非线性，但在低维空间时会破坏特征。由于第二个 PW 主要的功能是降维，所以不宜再加 ReLU6。<br>
<img src="https://img-blog.csdnimg.cn/20210204142843812.png" alt="在这里插入图片描述"></p>
<p>当<code>strides=1</code>且输入特征矩阵与输出特征矩阵<code>shape</code>相同时才有 shortcut 连接</p>
<p><img src="https://img-blog.csdnimg.cn/20210204142853463.png" alt="在这里插入图片描述"></p>
<h1>MobileNet_V2 realized by tensorflow2</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential, Model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNReLU</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBNReLU, self).__init__(**kwargs)</span><br><span class="line">        self.conv = layers.Conv2D(filters=out_channel,</span><br><span class="line">                                  kernel_size=kernel_size,</span><br><span class="line">                                  strides=strides,</span><br><span class="line">                                  padding=<span class="string">&#x27;SAME&#x27;</span>,</span><br><span class="line">                                  use_bias=<span class="literal">False</span>,</span><br><span class="line">                                  name=<span class="string">&#x27;Conv2d&#x27;</span>)</span><br><span class="line">        self.bn = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;BatchNorm&#x27;</span>)</span><br><span class="line">        self.activation = layers.ReLU(max_value=<span class="number">6.0</span>)   <span class="comment"># ReLU6</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span>, **kargs</span>):</span><br><span class="line">        x = self.conv(inputs)</span><br><span class="line">        x = self.bn(x, training=training)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidualBlock</span>(layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, strides, expand_ratio, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidualBlock, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_channel = in_channel * expand_ratio</span><br><span class="line">        self.use_shortcut = (strides == <span class="number">1</span>) <span class="keyword">and</span> (in_channel == out_channel)</span><br><span class="line"></span><br><span class="line">        layer_list = []</span><br><span class="line">        <span class="comment"># first bottleneck does not need 1*1 conv</span></span><br><span class="line">        <span class="keyword">if</span> expand_ratio != <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 1x1 pointwise conv</span></span><br><span class="line">            layer_list.append(ConvBNReLU(out_channel=self.hidden_channel, kernel_size=<span class="number">1</span>, name=<span class="string">&#x27;expand&#x27;</span>))</span><br><span class="line">        layer_list.extend([</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 3x3 depthwise conv</span></span><br><span class="line">            layers.DepthwiseConv2D(kernel_size=<span class="number">3</span>, padding=<span class="string">&#x27;SAME&#x27;</span>, strides=strides, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;depthwise&#x27;</span>),</span><br><span class="line">            layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;depthwise/BatchNorm&#x27;</span>),</span><br><span class="line">            layers.ReLU(max_value=<span class="number">6.0</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment">#1x1 pointwise conv(linear)</span></span><br><span class="line">            <span class="comment"># linear activation y = x -&gt; no activation function</span></span><br><span class="line">            layers.Conv2D(filters=out_channel, kernel_size=<span class="number">1</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;project&#x27;</span>),</span><br><span class="line">            layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;project/BatchNorm&#x27;</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.main_branch = Sequential(layer_list, name=<span class="string">&#x27;expanded_conv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, **kargs</span>):</span><br><span class="line">        <span class="keyword">if</span> self.use_shortcut:</span><br><span class="line">            <span class="keyword">return</span> inputs + self.main_branch(inputs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.main_branch(inputs)</span><br></pre></td></tr></table></figure>
<h2 id="MobileNet-V2-网络结构">MobileNet_V2 网络结构</h2>
<p><img src="https://img-blog.csdnimg.cn/20210204143010527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_make_divisible</span>(<span class="params">ch, divisor=<span class="number">8</span>, min_ch=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function is taken from the original tf repo.</span></span><br><span class="line"><span class="string">    It ensures that all layers have a channel number that is divisible by 8</span></span><br><span class="line"><span class="string">    It can be seen here:</span></span><br><span class="line"><span class="string">    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> min_ch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        min_ch = divisor</span><br><span class="line">    new_ch = <span class="built_in">max</span>(min_ch, <span class="built_in">int</span>(ch + divisor / <span class="number">2</span>) // divisor * divisor)</span><br><span class="line">    <span class="comment"># Make sure that round down does not go down by more than 10%.</span></span><br><span class="line">    <span class="keyword">if</span> new_ch &lt; <span class="number">0.9</span> * ch:</span><br><span class="line">        new_ch += divisor</span><br><span class="line">    <span class="keyword">return</span> new_ch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">MobileNet_V2</span>(<span class="params">im_height=<span class="number">224</span>,</span></span><br><span class="line"><span class="params">                 im_width=<span class="number">224</span>,</span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 alpha=<span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">                 round_nearest=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">                 include_top=<span class="literal">True</span></span>):</span><br><span class="line"></span><br><span class="line">    block = InvertedResidualBlock</span><br><span class="line">    input_channel = _make_divisible(<span class="number">32</span>*alpha, round_nearest)</span><br><span class="line">    last_channel = _make_divisible(<span class="number">1280</span>*alpha, round_nearest)</span><br><span class="line">    <span class="comment"># t, c, n, s</span></span><br><span class="line">    inverted_residual_setting = [</span><br><span class="line">        [<span class="number">1</span>, <span class="number">16</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">24</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">96</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">160</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">320</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    input_image = layers.Input(shape=(im_height, im_width, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">    <span class="comment"># conv1</span></span><br><span class="line">    x = ConvBNReLU(input_channel, strides=<span class="number">2</span>, name=<span class="string">&#x27;Conv&#x27;</span>)(input_image)</span><br><span class="line">    <span class="comment"># building inverted residual blocks</span></span><br><span class="line">    <span class="keyword">for</span> idx, (t, c, n, s) <span class="keyword">in</span> <span class="built_in">enumerate</span>(inverted_residual_setting):</span><br><span class="line">        output_channel = _make_divisible(c*alpha, round_nearest)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            strides = s <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">            x = block(x.shape[-<span class="number">1</span>], <span class="comment"># n h w c</span></span><br><span class="line">                      output_channel,</span><br><span class="line">                      strides,</span><br><span class="line">                      expand_ratio=t)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># building last several layers</span></span><br><span class="line">    x = ConvBNReLU(last_channel, kernel_size=<span class="number">1</span>, name=<span class="string">&#x27;Conv_1&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> include_top <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># building classifier</span></span><br><span class="line">        x = layers.GlobalAveragePooling2D()(x)   <span class="comment"># pool + flatten</span></span><br><span class="line">        x = layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">        output = layers.Dense(num_classes, name=<span class="string">&#x27;Logits&#x27;</span>)(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = x</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=input_image, outputs=output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    gpus = tf.config.experimental.list_physical_devices(<span class="string">&quot;GPU&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> gpus:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">for</span> gpu <span class="keyword">in</span> gpus:</span><br><span class="line">                tf.config.experimental.set_memory_growth(gpu, <span class="literal">True</span>)</span><br><span class="line">                <span class="built_in">print</span>(gpu)</span><br><span class="line">        <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            exit(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    model = MobileNet_V2(im_height=<span class="number">224</span>,</span><br><span class="line">                 im_width=<span class="number">224</span>,</span><br><span class="line">                 num_classes=<span class="number">1000</span>,</span><br><span class="line">                 alpha=<span class="number">0.75</span>,</span><br><span class="line">                 round_nearest=<span class="number">8</span>,</span><br><span class="line">                 include_top=<span class="literal">True</span>)</span><br><span class="line">    model.build((<span class="literal">None</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    data = tf.random.normal([<span class="number">4</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>])</span><br><span class="line">    pred = model.predict(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input shape:\n&quot;</span>, data.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output shape:\n&quot;</span>, pred.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PhysicalDevice(name=&#x27;/physical_device:GPU:0&#x27;, device_type=&#x27;GPU&#x27;)</span><br><span class="line">Model: &quot;model_2&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #</span><br><span class="line">=================================================================</span><br><span class="line">input_5 (InputLayer)         [(None, 224, 224, 3)]     0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Conv (ConvBNReLU)            (None, 112, 112, 24)      744</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_52 ( (None, 112, 112, 16)      760</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_53 ( (None, 56, 56, 24)        5568</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_54 ( (None, 56, 56, 24)        9456</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_55 ( (None, 28, 28, 24)        9456</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_56 ( (None, 28, 28, 24)        9456</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_57 ( (None, 28, 28, 24)        9456</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_58 ( (None, 14, 14, 48)        13008</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_59 ( (None, 14, 14, 48)        32736</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_60 ( (None, 14, 14, 48)        32736</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_61 ( (None, 14, 14, 48)        32736</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_62 ( (None, 14, 14, 72)        39744</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_63 ( (None, 14, 14, 72)        69840</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_64 ( (None, 14, 14, 72)        69840</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_65 ( (None, 7, 7, 120)         90768</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_66 ( (None, 7, 7, 120)         185520</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_67 ( (None, 7, 7, 120)         185520</span><br><span class="line">_________________________________________________________________</span><br><span class="line">inverted_residual_block_68 ( (None, 7, 7, 240)         272400</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Conv_1 (ConvBNReLU)          (None, 7, 7, 960)         234240</span><br><span class="line">_________________________________________________________________</span><br><span class="line">global_average_pooling2d_3 ( (None, 960)               0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_3 (Dropout)          (None, 960)               0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Logits (Dense)               (None, 1000)              961000</span><br><span class="line">=================================================================</span><br><span class="line">Total params: 2,264,984</span><br><span class="line">Trainable params: 2,238,984</span><br><span class="line">Non-trainable params: 26,000</span><br><span class="line">_________________________________________________________________</span><br><span class="line">input shape:</span><br><span class="line"> (4, 224, 224, 3)</span><br><span class="line">output shape:</span><br><span class="line"> (4, 1000)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>paddle2.0高层API实现自定义数据集文本分类中的情感分析任务</title>
    <url>/2022/06/01/234b53c9dbdf4d4595e8b9f095173105/</url>
    <content><![CDATA[<p>[toc]</p>
<blockquote>
<p>本文包含了：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- 自定义文本分类数据集继承</span><br><span class="line">- 文本分类数据处理</span><br><span class="line">- 循环神经网络RNN, LSTM</span><br><span class="line">- ·seq2vec·</span><br><span class="line">- pretrained预训练模型</span><br></pre></td></tr></table></figure>
<p>『深度学习 7 日打卡营·day4』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
<li></li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>情感分析是自然语言处理领域一个老生常谈的任务。句子情感分析目的是为了判别说者的情感倾向，比如在某些话题上给出的的态度明确的观点，或者反映的情绪状态等。情感分析有着广泛应用，比如电商评论分析、舆情分析等。</p>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/febb8a1478e34258953e56611ddc76cd20b412fec89845b0a4a2e6b9f8aae774" hspace='10'/> <br />
</p>
<h2 id="环境介绍">环境介绍</h2>
<ul>
<li>
<p>PaddlePaddle 框架，AI Studio 平台已经默认安装最新版 2.0。</p>
</li>
<li>
<p>PaddleNLP，深度兼容框架 2.0，是飞桨框架 2.0 在 NLP 领域的最佳实践。</p>
</li>
</ul>
<p>这里使用的是 beta 版本，马上也会发布 rc 版哦。AI Studio 平台后续会默认安装 PaddleNLP，在此之前可使用如下命令安装。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载paddlenlp</span></span><br><span class="line">!pip install --upgrade paddlenlp==<span class="number">2.0</span><span class="number">.0</span>b4 -i https://pypi.org/simple</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Requirement already up-to-date: paddlenlp==2.0.0b4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.0b4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (4.1.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.4.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.42.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.9.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (1.2.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: numpy&gt;=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py-&gt;paddlenlp==2.0.0b4) (1.16.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py-&gt;paddlenlp==2.0.0b4) (1.15.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: protobuf&gt;=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (3.14.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Flask-Babel&gt;=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (1.0.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Pillow&gt;=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (7.1.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: flake8&gt;=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (3.8.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (1.21.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (0.7.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: flask&gt;=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (1.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (2.22.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl-&gt;paddlenlp==2.0.0b4) (0.8.53)</span><br><span class="line">Requirement already satisfied, skipping upgrade: scikit-learn&gt;=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval-&gt;paddlenlp==2.0.0b4) (0.22.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2019.3)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Babel&gt;=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.8.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Jinja2&gt;=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.10.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: importlib-metadata; python_version &lt; &quot;3.8&quot; in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.23)</span><br><span class="line">Requirement already satisfied, skipping upgrade: mccabe&lt;0.7.0,&gt;=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.6.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pycodestyle&lt;2.7.0,&gt;=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.6.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pyflakes&lt;2.3.0,&gt;=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.2.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.10.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (5.1.2)</span><br><span class="line">Requirement already satisfied, skipping upgrade: nodeenv&gt;=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.3.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: virtualenv&gt;=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (16.7.9)</span><br><span class="line">Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.3.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: identify&gt;=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.4.10)</span><br><span class="line">Requirement already satisfied, skipping upgrade: cfgv&gt;=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.0.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: Werkzeug&gt;=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask&gt;=1.1.1-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.16.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: itsdangerous&gt;=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask&gt;=1.1.1-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.1.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: click&gt;=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask&gt;=1.1.1-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (7.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2.8)</span><br><span class="line">Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.25.6)</span><br><span class="line">Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (2019.9.11)</span><br><span class="line">Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (3.0.4)</span><br><span class="line">Requirement already satisfied, skipping upgrade: future&gt;=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.18.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: pycryptodome&gt;=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (3.9.9)</span><br><span class="line">Requirement already satisfied, skipping upgrade: joblib&gt;=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;paddlenlp==2.0.0b4) (0.14.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: scipy&gt;=0.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;paddlenlp==2.0.0b4) (1.3.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2&gt;=2.5-&gt;Flask-Babel&gt;=1.0.0-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (1.1.1)</span><br><span class="line">Requirement already satisfied, skipping upgrade: zipp&gt;=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version &lt; &quot;3.8&quot;-&gt;flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (0.6.0)</span><br><span class="line">Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp&gt;=0.5-&gt;importlib-metadata; python_version &lt; &quot;3.8&quot;-&gt;flake8&gt;=3.7.9-&gt;visualdl-&gt;paddlenlp==2.0.0b4) (7.2.0)</span><br></pre></td></tr></table></figure>
<p>查看安装的版本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddlenlp</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(paddle.__version__, paddlenlp.__version__)</span><br><span class="line"></span><br><span class="line">paddle.set_device(<span class="string">&#x27;gpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2.0.0 2.0.0b4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CUDAPlace(0)</span><br></pre></td></tr></table></figure>
<h2 id="PaddleNLP-和-Paddle-框架是什么关系？">PaddleNLP 和 Paddle 框架是什么关系？</h2>
<p><img src="" alt=""></p>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/165924e86d9f4b5fa5d6fdee9e8496bf01be524e61f341b3879aceba48ae80fb" width = "300" height = "250"  hspace='10'/> <br />
</p><br></br>
<ul>
<li>Paddle 框架是基础底座，提供深度学习任务全流程 API。PaddleNLP 基于 Paddle 框架开发，适用于 NLP 任务。</li>
</ul>
<p>PaddleNLP 中数据处理、数据集、组网单元等 API 未来会沉淀到框架<code>paddle.text</code>中。</p>
<ul>
<li>代码中继承</li>
</ul>
<p><code>class TSVDataset(paddle.io.Dataset)</code></p>
<h2 id="使用飞桨完成深度学习任务的通用流程">使用飞桨完成深度学习任务的通用流程</h2>
<ul>
<li>
<p>数据集和数据处理<br>
<code>paddle.io.Dataset </code><br>
<code>paddle.io.DataLoader</code><br>
<code>paddlenlp.data </code></p>
</li>
<li>
<p>组网和网络配置</p>
<p><code>paddle.nn.Embedding</code><br>
<code>paddlenlp.seq2vec</code><br>
<code>paddle.nn.Linear</code><br>
<code>paddle.tanh</code></p>
<p><code>paddle.nn.CrossEntropyLoss</code><br>
<code>paddle.metric.Accuracy</code><br>
<code>paddle.optimizer</code></p>
<p><code>model.prepare</code></p>
</li>
<li>
<p>网络训练和评估</p>
<p><code>model.fit</code><br>
<code>model.evaluate</code></p>
</li>
<li>
<p>预测</p>
<p><code>model.predict</code></p>
</li>
</ul>
<p>注意：建议在 GPU 下运行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> paddlenlp <span class="keyword">as</span> ppnlp</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Pad, Stack, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">from</span> paddlenlp.datasets <span class="keyword">import</span> MapDatasetWrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_vocab, convert_example</span><br></pre></td></tr></table></figure>
<h1>数据集和数据处理</h1>
<h2 id="自定义数据集">自定义数据集</h2>
<p>映射式(map-style)数据集需要继承<code>paddle.io.Dataset</code></p>
<ul>
<li>
<p><code>__getitem__</code>: 根据给定索引获取数据集中指定样本，在 paddle.io.DataLoader 中需要使用此函数通过下标获取样本。</p>
</li>
<li>
<p><code>__len__</code>: 返回数据集样本个数， paddle.io.BatchSampler 中需要样本个数生成下标序列。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SelfDefinedDataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="built_in">super</span>(SelfDefinedDataset, self).__init__()</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_labels</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">txt_to_list</span>(<span class="params">file_name</span>):</span><br><span class="line">    res_list = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(file_name):</span><br><span class="line">        res_list.append(line.strip().split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> res_list</span><br><span class="line"></span><br><span class="line">trainlst = txt_to_list(<span class="string">&#x27;train.txt&#x27;</span>)</span><br><span class="line">devlst = txt_to_list(<span class="string">&#x27;dev.txt&#x27;</span>)</span><br><span class="line">testlst = txt_to_list(<span class="string">&#x27;test.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过get_datasets()函数，将list数据转换为dataset。</span></span><br><span class="line"><span class="comment"># get_datasets()可接收[list]参数，或[str]参数，根据自定义数据集的写法自由选择。</span></span><br><span class="line"><span class="comment"># train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets([&#x27;train&#x27;, &#x27;dev&#x27;, &#x27;test&#x27;])</span></span><br><span class="line">train_ds, dev_ds, test_ds = SelfDefinedDataset.get_datasets([trainlst, devlst, testlst])</span><br></pre></td></tr></table></figure>
<h2 id="训练数据查看">训练数据查看</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label_list = train_ds.get_labels()</span><br><span class="line"><span class="built_in">print</span>(label_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(train_ds[i])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;0&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;赢在心理，输在出品！杨枝太酸，三文鱼熟了，酥皮焗杏汁杂果可以换个名（九唔搭八）&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;服务一般，客人多，服务员少，但食品很不错&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;東坡肉竟然有好多毛，問佢地點解，佢地仲話係咁架\ue107\ue107\ue107\ue107\ue107\ue107\ue107冇天理，第一次食東坡肉有毛，波羅包就幾好食&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;父亲节去的，人很多，口味还可以上菜快！但是结账的时候，算错了没有打折，我也忘记拿清单了。说好打8折的，收银员没有打，人太多一时自己也没有想起。不知道收银员忘记，还是故意那钱露入自己钱包。。&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;吃野味，吃个新鲜，你当然一定要来广州吃鹿肉啦*价格便宜，量好足，&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;味道几好服务都五错推荐鹅肝乳鸽飞鱼&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;作为老字号，水准保持算是不错，龟岗分店可能是位置问题，人不算多，基本不用等位，自从抢了券，去过好几次了，每次都可以打85以上的评分，算是可以了～粉丝煲每次必点，哈哈，鱼也不错，还会来帮衬的，楼下还可以免费停车！&#x27;, &#x27;1&#x27;]</span><br><span class="line">[&#x27;边到正宗啊？味味都咸死人啦，粤菜讲求鲜甜，五知点解感多人话好吃。&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;环境卫生差，出品垃圾，冇下次，不知所为&#x27;, &#x27;0&#x27;]</span><br><span class="line">[&#x27;和苑真是精致粤菜第一家，服务菜品都一流&#x27;, &#x27;1&#x27;]</span><br></pre></td></tr></table></figure>
<h2 id="数据处理">数据处理</h2>
<p>为了将原始数据处理成模型可以读入的格式，本项目将对数据作以下处理：</p>
<ul>
<li>首先使用<code>jieba</code>切词，之后将<code>jieba</code>切完后的单词映射词表中单词 id。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/fb492aa32947698a198429ec2cb8e907.png" alt=""></p>
<ul>
<li>使用<code>paddle.io.DataLoader</code>接口多线程异步加载数据。</li>
</ul>
<p>其中用到了 PaddleNLP 中关于数据处理的 API。PaddleNLP 提供了许多关于 NLP 任务中构建有效的数据 pipeline 的常用 API</p>
<table>
<thead>
<tr>
<th>API</th>
<th style="text-align:left">简介</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>paddlenlp.data.Stack</code></td>
<td style="text-align:left">堆叠 N 个具有相同 shape 的输入数据来构建一个 batch，它的输入必须具有相同的 shape，输出便是这些输入的堆叠组成的 batch 数据。</td>
</tr>
<tr>
<td><code>paddlenlp.data.Pad</code></td>
<td style="text-align:left">堆叠 N 个输入数据来构建一个 batch，每个输入数据将会被 padding 到 N 个输入数据中最大的长度</td>
</tr>
<tr>
<td><code>paddlenlp.data.Tuple</code></td>
<td style="text-align:left">将多个组 batch 的函数包装在一起</td>
</tr>
</tbody>
</table>
<p>更多数据处理操作详见： <a href="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md">https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载词汇表文件word_dict.txt，用于构造词-id映射关系。</span></span><br><span class="line"><span class="comment"># !wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载词表</span></span><br><span class="line">vocab = load_vocab(<span class="string">&#x27;./senta_word_dict.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印填补单词及对应向量</span></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    <span class="built_in">print</span>(k, v)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[PAD] 0</span><br></pre></td></tr></table></figure>
<h2 id="构造-dataloder">构造 dataloder</h2>
<p>下面的<code>create_data_loader</code>函数用于创建运行和预测时所需要的<code>DataLoader</code>对象。</p>
<ul>
<li>
<p><code>paddle.io.DataLoader</code>返回一个迭代器，该迭代器根据<code>batch_sampler</code>指定的顺序迭代返回 dataset 数据。异步加载数据。</p>
</li>
<li>
<p><code>batch_sampler</code>：DataLoader 通过 batch_sampler 产生的 mini-batch 索引列表来 dataset 中索引样本并组成 mini-batch</p>
</li>
<li>
<p><code>collate_fn</code>：指定如何将样本列表组合为 mini-batch 数据。传给它参数需要是一个 callable 对象，需要实现对组建的 batch 的处理逻辑，并返回每个 batch 的数据。在这里传入的是<code>prepare_input</code>函数，对产生的数据进行 pad 操作，并返回实际长度等。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Reads data and generates mini-batches.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">dataset,</span></span><br><span class="line"><span class="params">                      trans_function=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                      mode=<span class="string">&#x27;train&#x27;</span>,</span></span><br><span class="line"><span class="params">                      batch_size=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                      pad_token_id=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                      batchify_fn=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> trans_function:</span><br><span class="line">        dataset = dataset.apply(trans_function, lazy=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return_list 数据是否以list形式返回</span></span><br><span class="line">    <span class="comment"># collate_fn  指定如何将样本列表组合为mini-batch数据。传给它参数需要是一个callable对象，需要实现对组建的batch的处理逻辑，并返回每个batch的数据。在这里传入的是`prepare_input`函数，对产生的数据进行pad操作，并返回实际长度等。</span></span><br><span class="line">    dataloader = paddle.io.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        return_list=<span class="literal">True</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        collate_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="comment"># python中的偏函数partial，把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。</span></span><br><span class="line">trans_function = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    vocab=vocab,</span><br><span class="line">    unk_token_id=vocab.get(<span class="string">&#x27;[UNK]&#x27;</span>, <span class="number">1</span>),</span><br><span class="line">    is_test=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的数据batch化处理，便于模型batch化运算。</span></span><br><span class="line"><span class="comment"># batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。</span></span><br><span class="line"><span class="comment"># 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.</span></span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=vocab[<span class="string">&#x27;[PAD]&#x27;</span>]),  <span class="comment"># input_ids</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>),  <span class="comment"># seq len</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loader = create_dataloader(</span><br><span class="line">    train_ds,</span><br><span class="line">    trans_function=trans_function,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">dev_loader = create_dataloader(</span><br><span class="line">    dev_ds,</span><br><span class="line">    trans_function=trans_function,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">test_loader = create_dataloader(</span><br><span class="line">    test_ds,</span><br><span class="line">    trans_function=trans_function,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br></pre></td></tr></table></figure>
<h1>模型搭建</h1>
<p>使用<code>LSTMencoder</code>搭建一个 BiLSTM 模型用于进行句子建模，得到句子的向量表示。</p>
<p>然后接一个线性变换层，完成二分类任务。</p>
<ul>
<li><code>paddle.nn.Embedding</code>组建 word-embedding 层</li>
<li><code>ppnlp.seq2vec.LSTMEncoder</code>组建句子建模层</li>
<li><code>paddle.nn.Linear</code>构造二分类器</li>
</ul>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/ecf309c20e5347399c55f1e067821daa088842fa46ad49be90de4933753cd3cf" width = "800" height = "450"  hspace='10'/> <br />
</p><br><center>图1：seq2vec示意图</center></br>
<ul>
<li>除 LSTM 外，<code>seq2vec</code>还提供了许多语义表征方法，详细可参考：<a href="https://aistudio.baidu.com/aistudio/projectdetail/1283423">seq2vec 介绍</a></li>
</ul>
<!---->
<ul>
<li><code>LSTMEncoder</code>参数：</li>
</ul>
<!---->
<ul>
<li><code>input_size</code>: int，必选。输入特征 Tensor 的最后一维维度。</li>
<li><code>hidden_size</code>: int，必选。lstm 运算的 hidden size。</li>
<li><code>num_layers</code>:int，可选，lstm 层数，默认为 1。</li>
<li><code>direction</code>: str，可选，lstm 运算方向，可选 forward， bidirectional。默认 forward。</li>
<li><code>dropout</code>: float，可选，dropout 概率值。如果设置非 0，则将对每一层 lstm 输出做 dropout 操作。默认为 0.0。</li>
<li><code>pooling_type</code>: str， 可选，默认为 None。可选 sum，max，mean。如<code>pooling_type=None</code>， 则将最后一层 lstm 的最后一个 step hidden 输出作为文本语义表征; 如<code>pooling_type!=None</code>， 则将最后一层 lstm 的所有 step 的 hidden 输出做指定 pooling 操作，其结果作为文本语义表征。</li>
</ul>
<p>更多<code>seq2vec</code>信息参考：<a href="https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/paddlenlp/seq2vec/encoder.py">https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/paddlenlp/seq2vec/encoder.py</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 num_classes,</span></span><br><span class="line"><span class="params">                 emb_dim=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 padding_idx=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 lstm_hidden_size=<span class="number">198</span>,</span></span><br><span class="line"><span class="params">                 direction=<span class="string">&#x27;forward&#x27;</span>,</span></span><br><span class="line"><span class="params">                 lstm_layers=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 pooling_type=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 fc_hidden_size=<span class="number">96</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 首先将输入word id 查表后映射成 word embedding</span></span><br><span class="line">        self.embedder = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size,</span><br><span class="line">            embedding_dim=emb_dim,</span><br><span class="line">            padding_idx=padding_idx)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将word embedding经过LSTMEncoder变换到文本语义表征空间中</span></span><br><span class="line">        self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder(</span><br><span class="line">            emb_dim,</span><br><span class="line">            lstm_hidden_size,</span><br><span class="line">            num_layers=lstm_layers,</span><br><span class="line">            direction=direction,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            pooling_type=pooling_type)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size</span></span><br><span class="line">        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后的分类器</span></span><br><span class="line">        self.output_layer = nn.Linear(fc_hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text, seq_len</span>):</span><br><span class="line">        <span class="comment"># text shape: (batch_size, num_tokens)</span></span><br><span class="line">        <span class="comment"># print(&#x27;input :&#x27;, text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># print(&#x27;after word-embeding:&#x27;, embedded_text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)</span></span><br><span class="line">        <span class="comment"># num_directions = 2 if direction is &#x27;bidirectional&#x27; else 1</span></span><br><span class="line">        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)</span><br><span class="line">        <span class="comment"># print(&#x27;after lstm:&#x27;, text_repr.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, fc_hidden_size)</span></span><br><span class="line">        fc_out = paddle.tanh(self.fc(text_repr))</span><br><span class="line">        <span class="comment"># print(&#x27;after Linear classifier:&#x27;, fc_out.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_classes)</span></span><br><span class="line">        logits = self.output_layer(fc_out)</span><br><span class="line">        <span class="comment"># print(&#x27;output:&#x27;, logits.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># probs 分类概率值</span></span><br><span class="line">        probs = F.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;output probability:&#x27;, probs.shape)</span></span><br><span class="line">        <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line">model= LSTMModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab),</span><br><span class="line">        <span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br></pre></td></tr></table></figure>
<h1>模型配置和训练</h1>
<h2 id="模型配置-2">模型配置</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">        parameters=model.parameters(), learning_rate=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line">loss = paddle.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, loss, metric)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置visualdl路径</span></span><br><span class="line">log_dir = <span class="string">&#x27;./visualdl&#x27;</span></span><br><span class="line">callbacks = paddle.callbacks.VisualDL(log_dir=log_dir)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练">模型训练</h2>
<p>训练过程中会输出 loss、acc 等信息。</p>
<p>这里一共训练了 10 个 epoch，在训练集上准确率约 97%。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Building prefix dict from the default dictionary ...</span><br><span class="line">Dumping model to file cache /tmp/jieba.cache</span><br><span class="line">Loading model cost 0.867 seconds.</span><br><span class="line">Prefix dict has been built successfully.</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  return (isinstance(seq, collections.Sequence) and</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">step  10/125 - loss: 0.6945 - acc: 0.4734 - 234ms/step</span><br><span class="line">step  20/125 - loss: 0.6929 - acc: 0.4914 - 163ms/step</span><br><span class="line">step  30/125 - loss: 0.6919 - acc: 0.5068 - 138ms/step</span><br><span class="line">step  40/125 - loss: 0.6904 - acc: 0.5109 - 125ms/step</span><br><span class="line">step  50/125 - loss: 0.6878 - acc: 0.5145 - 119ms/step</span><br><span class="line">step  60/125 - loss: 0.6949 - acc: 0.5137 - 115ms/step</span><br><span class="line">step  70/125 - loss: 0.6923 - acc: 0.5143 - 113ms/step</span><br><span class="line">step  80/125 - loss: 0.6877 - acc: 0.5125 - 111ms/step</span><br><span class="line">step  90/125 - loss: 0.6898 - acc: 0.5122 - 109ms/step</span><br><span class="line">step 100/125 - loss: 0.6846 - acc: 0.5141 - 107ms/step</span><br><span class="line">step 110/125 - loss: 0.6800 - acc: 0.5156 - 105ms/step</span><br><span class="line">step 120/125 - loss: 0.6790 - acc: 0.5281 - 104ms/step</span><br><span class="line">step 125/125 - loss: 0.6796 - acc: 0.5379 - 102ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6775 - acc: 0.7937 - 96ms/step</span><br><span class="line">step 20/84 - loss: 0.6774 - acc: 0.7941 - 84ms/step</span><br><span class="line">step 30/84 - loss: 0.6776 - acc: 0.7964 - 78ms/step</span><br><span class="line">step 40/84 - loss: 0.6765 - acc: 0.7986 - 74ms/step</span><br><span class="line">step 50/84 - loss: 0.6798 - acc: 0.7972 - 71ms/step</span><br><span class="line">step 60/84 - loss: 0.6748 - acc: 0.7991 - 69ms/step</span><br><span class="line">step 70/84 - loss: 0.6782 - acc: 0.8012 - 68ms/step</span><br><span class="line">step 80/84 - loss: 0.6776 - acc: 0.8011 - 66ms/step</span><br><span class="line">step 84/84 - loss: 0.6750 - acc: 0.8011 - 63ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 2/10</span><br><span class="line">step  10/125 - loss: 0.6812 - acc: 0.7531 - 125ms/step</span><br><span class="line">step  20/125 - loss: 0.6665 - acc: 0.7902 - 110ms/step</span><br><span class="line">step  30/125 - loss: 0.6578 - acc: 0.7987 - 108ms/step</span><br><span class="line">step  40/125 - loss: 0.6452 - acc: 0.7977 - 104ms/step</span><br><span class="line">step  50/125 - loss: 0.6238 - acc: 0.8003 - 103ms/step</span><br><span class="line">step  60/125 - loss: 0.5803 - acc: 0.8124 - 102ms/step</span><br><span class="line">step  70/125 - loss: 0.4889 - acc: 0.8177 - 101ms/step</span><br><span class="line">step  80/125 - loss: 0.4504 - acc: 0.8218 - 100ms/step</span><br><span class="line">step  90/125 - loss: 0.4354 - acc: 0.8266 - 99ms/step</span><br><span class="line">step 100/125 - loss: 0.3977 - acc: 0.8316 - 98ms/step</span><br><span class="line">step 110/125 - loss: 0.4341 - acc: 0.8364 - 97ms/step</span><br><span class="line">step 120/125 - loss: 0.4397 - acc: 0.8417 - 97ms/step</span><br><span class="line">step 125/125 - loss: 0.4236 - acc: 0.8430 - 95ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.4360 - acc: 0.8906 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.4167 - acc: 0.8898 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.4203 - acc: 0.8971 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.3835 - acc: 0.8986 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.3996 - acc: 0.8978 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.4477 - acc: 0.8962 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.4174 - acc: 0.8952 - 63ms/step</span><br><span class="line">step 80/84 - loss: 0.4231 - acc: 0.8960 - 65ms/step</span><br><span class="line">step 84/84 - loss: 0.4522 - acc: 0.8966 - 62ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 3/10</span><br><span class="line">step  10/125 - loss: 0.4684 - acc: 0.8922 - 107ms/step</span><br><span class="line">step  20/125 - loss: 0.4446 - acc: 0.8938 - 96ms/step</span><br><span class="line">step  30/125 - loss: 0.4317 - acc: 0.9008 - 100ms/step</span><br><span class="line">step  40/125 - loss: 0.4128 - acc: 0.9084 - 101ms/step</span><br><span class="line">step  50/125 - loss: 0.4111 - acc: 0.9125 - 98ms/step</span><br><span class="line">step  60/125 - loss: 0.3678 - acc: 0.9182 - 96ms/step</span><br><span class="line">step  70/125 - loss: 0.3552 - acc: 0.9212 - 95ms/step</span><br><span class="line">step  80/125 - loss: 0.3769 - acc: 0.9218 - 94ms/step</span><br><span class="line">step  90/125 - loss: 0.3651 - acc: 0.9234 - 94ms/step</span><br><span class="line">step 100/125 - loss: 0.3755 - acc: 0.9231 - 93ms/step</span><br><span class="line">step 110/125 - loss: 0.3678 - acc: 0.9234 - 93ms/step</span><br><span class="line">step 120/125 - loss: 0.3909 - acc: 0.9249 - 92ms/step</span><br><span class="line">step 125/125 - loss: 0.3978 - acc: 0.9244 - 90ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3998 - acc: 0.9320 - 64ms/step</span><br><span class="line">step 20/84 - loss: 0.3875 - acc: 0.9309 - 65ms/step</span><br><span class="line">step 30/84 - loss: 0.3662 - acc: 0.9305 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.3615 - acc: 0.9322 - 67ms/step</span><br><span class="line">step 50/84 - loss: 0.3896 - acc: 0.9309 - 66ms/step</span><br><span class="line">step 60/84 - loss: 0.3854 - acc: 0.9326 - 65ms/step</span><br><span class="line">step 70/84 - loss: 0.3862 - acc: 0.9317 - 64ms/step</span><br><span class="line">step 80/84 - loss: 0.3754 - acc: 0.9324 - 62ms/step</span><br><span class="line">step 84/84 - loss: 0.4394 - acc: 0.9332 - 60ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 4/10</span><br><span class="line">step  10/125 - loss: 0.4256 - acc: 0.9219 - 129ms/step</span><br><span class="line">step  20/125 - loss: 0.4016 - acc: 0.9305 - 108ms/step</span><br><span class="line">step  30/125 - loss: 0.3773 - acc: 0.9315 - 101ms/step</span><br><span class="line">step  40/125 - loss: 0.3954 - acc: 0.9346 - 96ms/step</span><br><span class="line">step  50/125 - loss: 0.3782 - acc: 0.9353 - 95ms/step</span><br><span class="line">step  60/125 - loss: 0.3464 - acc: 0.9398 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.3456 - acc: 0.9427 - 93ms/step</span><br><span class="line">step  80/125 - loss: 0.3636 - acc: 0.9429 - 93ms/step</span><br><span class="line">step  90/125 - loss: 0.3477 - acc: 0.9435 - 93ms/step</span><br><span class="line">step 100/125 - loss: 0.3602 - acc: 0.9432 - 92ms/step</span><br><span class="line">step 110/125 - loss: 0.3622 - acc: 0.9431 - 92ms/step</span><br><span class="line">step 120/125 - loss: 0.3756 - acc: 0.9439 - 92ms/step</span><br><span class="line">step 125/125 - loss: 0.3703 - acc: 0.9433 - 90ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3898 - acc: 0.9391 - 84ms/step</span><br><span class="line">step 20/84 - loss: 0.3813 - acc: 0.9410 - 73ms/step</span><br><span class="line">step 30/84 - loss: 0.3603 - acc: 0.9414 - 73ms/step</span><br><span class="line">step 40/84 - loss: 0.3644 - acc: 0.9422 - 71ms/step</span><br><span class="line">step 50/84 - loss: 0.3744 - acc: 0.9417 - 70ms/step</span><br><span class="line">step 60/84 - loss: 0.3567 - acc: 0.9437 - 70ms/step</span><br><span class="line">step 70/84 - loss: 0.3745 - acc: 0.9420 - 70ms/step</span><br><span class="line">step 80/84 - loss: 0.3677 - acc: 0.9426 - 68ms/step</span><br><span class="line">step 84/84 - loss: 0.4366 - acc: 0.9432 - 66ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 5/10</span><br><span class="line">step  10/125 - loss: 0.3941 - acc: 0.9328 - 114ms/step</span><br><span class="line">step  20/125 - loss: 0.3838 - acc: 0.9387 - 107ms/step</span><br><span class="line">step  30/125 - loss: 0.3766 - acc: 0.9414 - 102ms/step</span><br><span class="line">step  40/125 - loss: 0.3818 - acc: 0.9439 - 98ms/step</span><br><span class="line">step  50/125 - loss: 0.3641 - acc: 0.9450 - 97ms/step</span><br><span class="line">step  60/125 - loss: 0.3353 - acc: 0.9488 - 96ms/step</span><br><span class="line">step  70/125 - loss: 0.3363 - acc: 0.9510 - 95ms/step</span><br><span class="line">step  80/125 - loss: 0.3508 - acc: 0.9511 - 95ms/step</span><br><span class="line">step  90/125 - loss: 0.3450 - acc: 0.9513 - 95ms/step</span><br><span class="line">step 100/125 - loss: 0.3450 - acc: 0.9514 - 95ms/step</span><br><span class="line">step 110/125 - loss: 0.3547 - acc: 0.9513 - 94ms/step</span><br><span class="line">step 120/125 - loss: 0.3697 - acc: 0.9520 - 93ms/step</span><br><span class="line">step 125/125 - loss: 0.3807 - acc: 0.9512 - 92ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3845 - acc: 0.9414 - 86ms/step</span><br><span class="line">step 20/84 - loss: 0.3750 - acc: 0.9465 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.3583 - acc: 0.9458 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.3670 - acc: 0.9463 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.3688 - acc: 0.9453 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.3614 - acc: 0.9467 - 63ms/step</span><br><span class="line">step 70/84 - loss: 0.3717 - acc: 0.9452 - 64ms/step</span><br><span class="line">step 80/84 - loss: 0.3554 - acc: 0.9458 - 65ms/step</span><br><span class="line">step 84/84 - loss: 0.4361 - acc: 0.9465 - 64ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 6/10</span><br><span class="line">step  10/125 - loss: 0.3749 - acc: 0.9477 - 75ms/step</span><br><span class="line">step  20/125 - loss: 0.3694 - acc: 0.9504 - 75ms/step</span><br><span class="line">step  30/125 - loss: 0.3521 - acc: 0.9539 - 73ms/step</span><br><span class="line">step  40/125 - loss: 0.3791 - acc: 0.9541 - 78ms/step</span><br><span class="line">step  50/125 - loss: 0.3515 - acc: 0.9544 - 81ms/step</span><br><span class="line">step  60/125 - loss: 0.3352 - acc: 0.9574 - 81ms/step</span><br><span class="line">step  70/125 - loss: 0.3314 - acc: 0.9590 - 81ms/step</span><br><span class="line">step  80/125 - loss: 0.3496 - acc: 0.9584 - 82ms/step</span><br><span class="line">step  90/125 - loss: 0.3433 - acc: 0.9582 - 82ms/step</span><br><span class="line">step 100/125 - loss: 0.3400 - acc: 0.9580 - 83ms/step</span><br><span class="line">step 110/125 - loss: 0.3451 - acc: 0.9580 - 83ms/step</span><br><span class="line">step 120/125 - loss: 0.3599 - acc: 0.9589 - 83ms/step</span><br><span class="line">step 125/125 - loss: 0.3598 - acc: 0.9585 - 81ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3754 - acc: 0.9484 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.3714 - acc: 0.9535 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.3558 - acc: 0.9523 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.3603 - acc: 0.9533 - 62ms/step</span><br><span class="line">step 50/84 - loss: 0.3719 - acc: 0.9514 - 61ms/step</span><br><span class="line">step 60/84 - loss: 0.3442 - acc: 0.9525 - 60ms/step</span><br><span class="line">step 70/84 - loss: 0.3654 - acc: 0.9513 - 59ms/step</span><br><span class="line">step 80/84 - loss: 0.3602 - acc: 0.9514 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.4414 - acc: 0.9520 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 7/10</span><br><span class="line">step  10/125 - loss: 0.3673 - acc: 0.9523 - 104ms/step</span><br><span class="line">step  20/125 - loss: 0.3638 - acc: 0.9566 - 93ms/step</span><br><span class="line">step  30/125 - loss: 0.3488 - acc: 0.9599 - 90ms/step</span><br><span class="line">step  40/125 - loss: 0.3790 - acc: 0.9600 - 88ms/step</span><br><span class="line">step  50/125 - loss: 0.3557 - acc: 0.9583 - 88ms/step</span><br><span class="line">step  60/125 - loss: 0.3309 - acc: 0.9604 - 88ms/step</span><br><span class="line">step  70/125 - loss: 0.3366 - acc: 0.9621 - 88ms/step</span><br><span class="line">step  80/125 - loss: 0.3372 - acc: 0.9618 - 87ms/step</span><br><span class="line">step  90/125 - loss: 0.3326 - acc: 0.9615 - 87ms/step</span><br><span class="line">step 100/125 - loss: 0.3365 - acc: 0.9612 - 88ms/step</span><br><span class="line">step 110/125 - loss: 0.3404 - acc: 0.9615 - 88ms/step</span><br><span class="line">step 120/125 - loss: 0.3582 - acc: 0.9626 - 87ms/step</span><br><span class="line">step 125/125 - loss: 0.3549 - acc: 0.9621 - 86ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3800 - acc: 0.9531 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.3618 - acc: 0.9559 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.3520 - acc: 0.9555 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.3568 - acc: 0.9566 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.3752 - acc: 0.9552 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.3430 - acc: 0.9559 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.3786 - acc: 0.9550 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.3554 - acc: 0.9557 - 59ms/step</span><br><span class="line">step 84/84 - loss: 0.3533 - acc: 0.9563 - 57ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 8/10</span><br><span class="line">step  10/125 - loss: 0.3558 - acc: 0.9617 - 109ms/step</span><br><span class="line">step  20/125 - loss: 0.3595 - acc: 0.9641 - 97ms/step</span><br><span class="line">step  30/125 - loss: 0.3484 - acc: 0.9654 - 93ms/step</span><br><span class="line">step  40/125 - loss: 0.3728 - acc: 0.9639 - 90ms/step</span><br><span class="line">step  50/125 - loss: 0.3405 - acc: 0.9639 - 89ms/step</span><br><span class="line">step  60/125 - loss: 0.3275 - acc: 0.9660 - 88ms/step</span><br><span class="line">step  70/125 - loss: 0.3262 - acc: 0.9673 - 87ms/step</span><br><span class="line">step  80/125 - loss: 0.3359 - acc: 0.9668 - 87ms/step</span><br><span class="line">step  90/125 - loss: 0.3285 - acc: 0.9667 - 87ms/step</span><br><span class="line">step 100/125 - loss: 0.3344 - acc: 0.9663 - 87ms/step</span><br><span class="line">step 110/125 - loss: 0.3351 - acc: 0.9666 - 87ms/step</span><br><span class="line">step 120/125 - loss: 0.3564 - acc: 0.9676 - 87ms/step</span><br><span class="line">step 125/125 - loss: 0.3524 - acc: 0.9672 - 86ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3692 - acc: 0.9602 - 73ms/step</span><br><span class="line">step 20/84 - loss: 0.3669 - acc: 0.9582 - 72ms/step</span><br><span class="line">step 30/84 - loss: 0.3471 - acc: 0.9586 - 70ms/step</span><br><span class="line">step 40/84 - loss: 0.3467 - acc: 0.9586 - 69ms/step</span><br><span class="line">step 50/84 - loss: 0.3713 - acc: 0.9573 - 69ms/step</span><br><span class="line">step 60/84 - loss: 0.3442 - acc: 0.9578 - 69ms/step</span><br><span class="line">step 70/84 - loss: 0.3561 - acc: 0.9576 - 69ms/step</span><br><span class="line">step 80/84 - loss: 0.3410 - acc: 0.9579 - 69ms/step</span><br><span class="line">step 84/84 - loss: 0.4010 - acc: 0.9585 - 66ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 9/10</span><br><span class="line">step  10/125 - loss: 0.3517 - acc: 0.9602 - 106ms/step</span><br><span class="line">step  20/125 - loss: 0.3577 - acc: 0.9648 - 95ms/step</span><br><span class="line">step  30/125 - loss: 0.3434 - acc: 0.9669 - 92ms/step</span><br><span class="line">step  40/125 - loss: 0.3667 - acc: 0.9660 - 89ms/step</span><br><span class="line">step  50/125 - loss: 0.3391 - acc: 0.9661 - 89ms/step</span><br><span class="line">step  60/125 - loss: 0.3251 - acc: 0.9680 - 88ms/step</span><br><span class="line">step  70/125 - loss: 0.3235 - acc: 0.9695 - 87ms/step</span><br><span class="line">step  80/125 - loss: 0.3325 - acc: 0.9692 - 87ms/step</span><br><span class="line">step  90/125 - loss: 0.3263 - acc: 0.9694 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.3323 - acc: 0.9692 - 92ms/step</span><br><span class="line">step 110/125 - loss: 0.3316 - acc: 0.9694 - 93ms/step</span><br><span class="line">step 120/125 - loss: 0.3547 - acc: 0.9702 - 93ms/step</span><br><span class="line">step 125/125 - loss: 0.3506 - acc: 0.9699 - 91ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3649 - acc: 0.9609 - 84ms/step</span><br><span class="line">step 20/84 - loss: 0.3670 - acc: 0.9586 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.3463 - acc: 0.9586 - 64ms/step</span><br><span class="line">step 40/84 - loss: 0.3450 - acc: 0.9594 - 62ms/step</span><br><span class="line">step 50/84 - loss: 0.3687 - acc: 0.9583 - 61ms/step</span><br><span class="line">step 60/84 - loss: 0.3484 - acc: 0.9587 - 60ms/step</span><br><span class="line">step 70/84 - loss: 0.3511 - acc: 0.9587 - 59ms/step</span><br><span class="line">step 80/84 - loss: 0.3392 - acc: 0.9592 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.4006 - acc: 0.9597 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 10/10</span><br><span class="line">step  10/125 - loss: 0.3449 - acc: 0.9625 - 89ms/step</span><br><span class="line">step  20/125 - loss: 0.3561 - acc: 0.9676 - 105ms/step</span><br><span class="line">step  30/125 - loss: 0.3506 - acc: 0.9693 - 108ms/step</span><br><span class="line">step  40/125 - loss: 0.3665 - acc: 0.9676 - 109ms/step</span><br><span class="line">step  50/125 - loss: 0.3380 - acc: 0.9675 - 107ms/step</span><br><span class="line">step  60/125 - loss: 0.3235 - acc: 0.9693 - 106ms/step</span><br><span class="line">step  70/125 - loss: 0.3228 - acc: 0.9708 - 105ms/step</span><br><span class="line">step  80/125 - loss: 0.3274 - acc: 0.9706 - 106ms/step</span><br><span class="line">step  90/125 - loss: 0.3256 - acc: 0.9705 - 105ms/step</span><br><span class="line">step 100/125 - loss: 0.3307 - acc: 0.9702 - 103ms/step</span><br><span class="line">step 110/125 - loss: 0.3350 - acc: 0.9700 - 102ms/step</span><br><span class="line">step 120/125 - loss: 0.3551 - acc: 0.9709 - 100ms/step</span><br><span class="line">step 125/125 - loss: 0.3524 - acc: 0.9706 - 98ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3675 - acc: 0.9602 - 89ms/step</span><br><span class="line">step 20/84 - loss: 0.3638 - acc: 0.9602 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.3522 - acc: 0.9599 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.3541 - acc: 0.9600 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.3643 - acc: 0.9587 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.3276 - acc: 0.9600 - 60ms/step</span><br><span class="line">step 70/84 - loss: 0.3688 - acc: 0.9596 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.3467 - acc: 0.9597 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.3462 - acc: 0.9603 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/final</span><br></pre></td></tr></table></figure>
<h2 id="启动-VisualDL-查看训练过程可视化结果">启动 VisualDL 查看训练过程可视化结果</h2>
<p>启动步骤：</p>
<ul>
<li>1、切换到本界面左侧「可视化」</li>
<li>2、日志文件路径选择 ‘visualdl’</li>
<li>3、点击「启动 VisualDL」后点击「打开 VisualDL」，即可查看可视化结果：</li>
</ul>
<p>Accuracy 和 Loss 的实时变化趋势如下：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/03b59d954509455a44c4d6b4b6214b8d.png" alt="train_loss"><br>
<img src="https://img-blog.csdnimg.cn/img_convert/1bb4b6f02f714cfda3bd3b9e6827263c.png" alt="train_acc"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2f99481a79e83e90df9a91d3b8417046.png" alt="eval_loss"><br>
<img src="https://img-blog.csdnimg.cn/img_convert/f9347fec87e7dfb520e3e9c7c7153fd9.png" alt="eval_loss"></p>
<h2 id="评估">评估</h2>
<p>最终得到的评估准确率为 96%</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = model.evaluate(dev_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finally test acc: %.5f&quot;</span> % results[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.3675 - acc: 0.9602 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.3638 - acc: 0.9602 - 70ms/step</span><br><span class="line">step 30/84 - loss: 0.3522 - acc: 0.9599 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.3541 - acc: 0.9600 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.3643 - acc: 0.9587 - 63ms/step</span><br><span class="line">step 60/84 - loss: 0.3276 - acc: 0.9600 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.3688 - acc: 0.9596 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.3467 - acc: 0.9597 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.3462 - acc: 0.9603 - 59ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Finally test acc: 0.96027</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">model.save(<span class="string">&#x27;./finetuning/lstm/model&#x27;</span>, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1>预测</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model= LSTMModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab),</span><br><span class="line">        <span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br><span class="line"></span><br><span class="line">model.load(<span class="string">&#x27;./finetuning/model&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.prepare()</span><br><span class="line"></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="string">&#x27;negative&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;positive&#x27;</span>&#125;</span><br><span class="line">results = model.predict(test_loader, batch_size=<span class="number">128</span>)[<span class="number">0</span>]</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_probs <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># 映射分类label</span></span><br><span class="line">    idx = np.argmax(batch_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    idx = idx.tolist()</span><br><span class="line">    labels = [label_map[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    predictions.extend(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看预测数据前5个样例分类结果</span></span><br><span class="line"><span class="keyword">for</span> idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_ds.data[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Data: &#123;&#125; \t Label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data[<span class="number">0</span>], predictions[idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 42/42 [==============================] - ETA: 3s - 97ms/ste - ETA: 3s - 104ms/st - ETA: 3s - 108ms/st - ETA: 3s - 97ms/step - ETA: 2s - 89ms/ste - ETA: 2s - 85ms/ste - ETA: 2s - 81ms/ste - ETA: 2s - 78ms/ste - ETA: 1s - 76ms/ste - ETA: 1s - 74ms/ste - ETA: 1s - 73ms/ste - ETA: 1s - 72ms/ste - ETA: 1s - 71ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 69ms/ste - ETA: 0s - 69ms/ste - ETA: 0s - 68ms/ste - ETA: 0s - 67ms/ste - ETA: 0s - 64ms/ste - 62ms/step</span><br><span class="line">Predict samples: 5353</span><br><span class="line">Data: 楼面经理服务态度极差，等位和埋单都差，楼面小妹还挺好 	 Label: negative</span><br><span class="line">Data: 欺负北方人没吃过鲍鱼是怎么着？简直敷衍到可笑的程度，团购连青菜都是两人份？！难吃到死，菜色还特别可笑，什么时候粤菜的小菜改成拍黄瓜了？！把团购客人当傻子，可这满大厅的傻子谁还会再来？！ 	 Label: negative</span><br><span class="line">Data: 如果大家有时间而且不怕麻烦的话可以去这里试试，点一个饭等左2个钟，没错！是两个钟！期间催了n遍都说马上到，结果？呵呵。乳鸽的味道，太咸，可能不新鲜吧……要用重口味盖住异味。上菜超级慢！中途还搞什么表演，麻烦有人手的话就上菜啊，表什么演？！？！要大家饿着看表演吗？最后结账还算错单，我真心服了……有一种店叫不会有下次，大概就是指它吧 	 Label: negative</span><br><span class="line">Data: 偌大的一个大厅就一个人点菜，点菜速度超级慢，菜牌上多个菜停售，连续点了两个没标停售的菜也告知没有，粥上来是凉的，榴莲酥火大了，格格肉超级油腻而且咸?????? 	 Label: negative</span><br><span class="line">Data: 泥撕雞超級好吃！！！吃了一個再叫一個還想打包的節奏！ 	 Label: positive</span><br><span class="line">Data: 作为地道的广州人，从小就跟着家人在西关品尝各式美食，今日带着家中长辈来这个老字号泮溪酒家真实失望透顶，出品差、服务差、洗手间邋遢弥漫着浓郁尿骚味、丢广州人的脸、丢广州老字号的脸。 	 Label: negative</span><br><span class="line">Data: 辣味道很赞哦！猪肚鸡一直是我们的最爱，每次来都必点，服务很给力，环境很好，值得分享哦！西洋菜 	 Label: positive</span><br><span class="line">Data: 第一次吃到這麼脏的火鍋：吃着吃著吃出一條尾指粗的黑毛毛蟲——惡心！脏！！！第一次吃到這麼無誠信的火鍋服務：我們呼喚人員時，某女部長立即使服務員迅速取走蟲所在的碗，任我們多次叫「放下」論理，她們也置若罔聞轉身將蟲毁屍滅跡，還嘻皮笑臉辯稱只是把碗換走,態度行為惡劣——奸詐！毫無誠信！！爛！！！當然還有剛坐下時的情形：第一次吃到這樣的火鍋：所有肉食熟食都上桌了，鍋底遲遲沒上，足足等了半小時才姍姍來遲；---差！！第一次吃到這樣的火鍋：1元雞鍋、1碟6塊小牛肉、1碟小腐皮、1碟5塊裝的普通肥牛、1碟數片的細碎牛肚結帳便2百多元；---不值！！以下省略千字差評......白云路的稻香是最差、最失禮的稻香，天河城、華廈的都比它好上過萬倍！！白云路的稻香是史上最差的餐廳！！！ 	 Label: negative</span><br><span class="line">Data: 文昌鸡份量很少且很咸，其他菜味道很一般！服务态度差差差！还要10%的服务费、 	 Label: negative</span><br><span class="line">Data: 这个网站的评价真是越来越不可信了，搞不懂为什么这么多好评。真的是很一般，不要迷信什么哪里回来的大厨吧。环境和出品若是当作普通茶餐厅来看待就还说得过去，但是价格又不是茶餐厅的价格，这就很尴尬了。。服务也是有待提高。 	 Label: negative</span><br></pre></td></tr></table></figure>
<h2 id="修改seq2vec模型">修改<code>seq2vec</code>模型</h2>
<p><strong><code>seq2vec</code>模块</strong></p>
<ul>
<li>
<p>输入：文本序列的 Embedding Tensor，shape：(batch_size, num_token, emb_dim)</p>
</li>
<li>
<p>输出：文本语义表征 Enocded Texts Tensor，shape：(batch_sie,encoding_size)</p>
</li>
<li>
<p>提供了<code>BoWEncoder</code>，<code>CNNEncoder</code>，<code>GRUEncoder</code>，<code>LSTMEncoder</code>，<code>RNNEncoder</code>等模型</p>
<ul>
<li>
<p><code>BoWEncoder</code> 是将输入序列 Embedding Tensor 在 num_token 维度上叠加，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>CNNEncoder</code> 是将输入序列 Embedding Tensor 进行卷积操作，在对卷积结果进行 max_pooling，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>GRUEncoder</code> 是对输入序列 Embedding Tensor 进行 GRU 运算，在运算结果上进行 pooling 或者取最后一个 step 的隐表示，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>LSTMEncoder</code> 是对输入序列 Embedding Tensor 进行 LSTM 运算，在运算结果上进行 pooling 或者取最后一个 step 的隐表示，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
<li>
<p><code>RNNEncoder</code> 是对输入序列 Embedding Tensor 进行 RNN 运算，在运算结果上进行 pooling 或者取最后一个 step 的隐表示，得到文本语义表征 Enocded Texts Tensor。</p>
</li>
</ul>
</li>
<li>
<p><code>seq2vec</code>提供了许多语义表征方法，那么这些方法有什么特点呢？</p>
<ol>
<li><code>BoWEncoder</code>采用 Bag of Word Embedding 方法，其特点是简单。但其缺点是没有考虑文本的语境，所以对文本语义的表征不足以表意。</li>
<li><code>CNNEncoder</code>采用卷积操作，提取局部特征，其特点是可以共享权重。但其缺点同样只考虑了局部语义，上下文信息没有充分利用。</li>
</ol>
<center>
  <img src="https://ai-studio-static-online.cdn.bcebos.com/2b2498edd83e49d3b017c4a14e1be68506349249b8a24cdaa214755fb51eadcd" width="400" height="150" >
</center>
<center>
  图2：卷积示意图
</center>
</br>
<ol start="3">
<li><code>RNNEnocder</code>采用 RNN 方法，在计算下一个 token 语义信息时，利用上一个 token 语义信息作为其输入。但其缺点容易产生梯度消失和梯度爆炸。</li>
</ol>
  <p align="center">
  <img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fxilinx.eetrend.com%2Ffiles-eetrend-xilinx%2Farticle%2F201706%2F11511-30367-tu4rnn.jpg&refer=http%3A%2F%2Fxilinx.eetrend.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1615266778&t=23c2518ff8126315ddaa780303e79737" width = "40%" height = "20%"  hspace='10'/> 
  </p>
  <center>
    图3：RNN示意图
  </center>
  </br>
<ol start="4">
<li><code>LSTMEnocder</code>采用 LSTM 方法，LSTM 是 RNN 的一种变种。为了学到长期依赖关系，LSTM 中引入了门控机制来控制信息的累计速度，包括有选择地加入新的信息，并有选择地遗忘之前累计的信息。</li>
</ol>
<p align="center">
  <img src="https://ai-studio-static-online.cdn.bcebos.com/a5af1d93c69f422d963e094397a2f6ce978c30a26ab6480ab70d688dd1929de0" width = "50%" height = "30%"  hspace='10'/> 
</center>
<center>
  图4：LSTM示意图
</center>
</br>
<ol start="5">
<li><code>GRUEncoder</code>采用 GRU 方法，GRU 也是 RNN 的一种变种。一个 LSTM 单元有四个输入 ，因而参数是 RNN 的四倍，带来的结果是训练速度慢。GRU 对 LSTM 进行了简化，在不影响效果的前提下加快了训练速度。</li>
</ol>
</li>
</ul>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/fc848bc2cb494b40ae42af892b756f5888770320a1fa42348cec10d3df64ee2f" width = "40%" height = "25%"  hspace='10'/> 
  <br />
</p><br><center>图5：GRU示意图</center></br>
<p>关于 CNN、LSTM、GRU、RNN 等更多信息参考：</p>
<ul>
<li>Understanding LSTM Networks: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
<li>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling:<a href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</a></li>
<li>A Critical Review of Recurrent Neural Networks</li>
</ul>
<p>for Sequence Learning: <a href="https://arxiv.org/pdf/1506.00019">https://arxiv.org/pdf/1506.00019</a></p>
<ul>
<li>A Convolutional Neural Network for Modelling Sentences: <a href="https://arxiv.org/abs/1404.2188">https://arxiv.org/abs/1404.2188</a></li>
</ul>
<h3 id="数据准备-2">数据准备</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddlenlp <span class="keyword">as</span> ppnlp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用paddlenlp内置数据集</span></span><br><span class="line"><span class="comment"># train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets([&#x27;train&#x27;, &#x27;dev&#x27;, &#x27;test&#x27;])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># label_list = train_ds.get_labels()</span></span><br><span class="line"><span class="comment"># print(label_list)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for sent, label in train_ds[:5]:</span></span><br><span class="line"><span class="comment">#     print (sent, label)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tmp = ppnlp.datasets.ChnSentiCorp(&#x27;train&#x27;)</span></span><br><span class="line"><span class="comment"># tmp1 = ppnlp.datasets.MapDatasetWrapper(tmp)</span></span><br><span class="line"><span class="comment"># print(type(tmp), type(tmp1))</span></span><br><span class="line"><span class="comment"># for sent, label in tmp1[:5]:</span></span><br><span class="line"><span class="comment">#     print(sent, label)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Pad, Stack, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> create_dataloader,convert_example</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reads data and generates mini-batches.</span></span><br><span class="line">trans_fn = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    vocab=vocab,</span><br><span class="line">    unk_token_id=vocab.get(<span class="string">&#x27;[UNK]&#x27;</span>, <span class="number">1</span>),</span><br><span class="line">    is_test=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的数据batch化处理，便于模型batch化运算。</span></span><br><span class="line"><span class="comment"># batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。</span></span><br><span class="line"><span class="comment"># 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.</span></span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=vocab[<span class="string">&#x27;[PAD]&#x27;</span>]),  <span class="comment"># input_ids</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>),  <span class="comment"># seq len</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br><span class="line">train_loader = create_dataloader(</span><br><span class="line">    train_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">dev_loader = create_dataloader(</span><br><span class="line">    dev_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">test_loader = create_dataloader(</span><br><span class="line">    test_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    mode=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br></pre></td></tr></table></figure>
<h3 id="模型建立">模型建立</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GRUModel</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 num_classes,</span></span><br><span class="line"><span class="params">                 emb_dim=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 padding_idx=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 hidden_size=<span class="number">198</span>,</span></span><br><span class="line"><span class="params">                 direction=<span class="string">&#x27;forward&#x27;</span>,</span></span><br><span class="line"><span class="params">                 num_layers=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 pooling_type=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 fc_hidden_size=<span class="number">96</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 首先将输入word id 查表后映射成 word embedding</span></span><br><span class="line">        self.embedder = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size,</span><br><span class="line">            embedding_dim=emb_dim,</span><br><span class="line">            padding_idx=padding_idx)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将word embedding经过LSTMEncoder变换到文本语义表征空间中</span></span><br><span class="line">        self.gru_encoder = ppnlp.seq2vec.GRUEncoder(</span><br><span class="line">            emb_dim,</span><br><span class="line">            hidden_size,</span><br><span class="line">            num_layers=num_layers,</span><br><span class="line">            direction=direction,</span><br><span class="line">            dropout=dropout_rate,</span><br><span class="line">            pooling_type=pooling_type)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size</span></span><br><span class="line">        self.fc = nn.Linear(self.gru_encoder.get_output_dim(), fc_hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后的分类器</span></span><br><span class="line">        self.output_layer = nn.Linear(fc_hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text, seq_len</span>):</span><br><span class="line">        <span class="comment"># text shape: (batch_size, num_tokens)</span></span><br><span class="line">        <span class="comment"># print(&#x27;input :&#x27;, text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># print(&#x27;after word-embeding:&#x27;, embedded_text.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)</span></span><br><span class="line">        <span class="comment"># num_directions = 2 if direction is &#x27;bidirectional&#x27; else 1</span></span><br><span class="line">        text_repr = self.gru_encoder(embedded_text, sequence_length=seq_len)</span><br><span class="line">        <span class="comment"># print(&#x27;after lstm:&#x27;, text_repr.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, fc_hidden_size)</span></span><br><span class="line">        fc_out = paddle.tanh(self.fc(text_repr))</span><br><span class="line">        <span class="comment"># print(&#x27;after Linear classifier:&#x27;, fc_out.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_classes)</span></span><br><span class="line">        logits = self.output_layer(fc_out)</span><br><span class="line">        <span class="comment"># print(&#x27;output:&#x27;, logits.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># probs 分类概率值</span></span><br><span class="line">        probs = F.softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;output probability:&#x27;, probs.shape)</span></span><br><span class="line">        <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure>
<h3 id="模型配置-3">模型配置</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model= GRUModel(</span><br><span class="line">        <span class="built_in">len</span>(vocab),</span><br><span class="line">        <span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br><span class="line"></span><br><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">        parameters=model.parameters(), learning_rate=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line">loss = paddle.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, loss, metric)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置visualdl路径</span></span><br><span class="line">log_dir = <span class="string">&#x27;./visualdl/gru_v1&#x27;</span></span><br><span class="line">callbacks = paddle.callbacks.VisualDL(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints/gru_v1&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line">step  10/125 - loss: 1.1046 - acc: 0.0000e+00 - 111ms/step</span><br><span class="line">step  20/125 - loss: 1.0817 - acc: 0.2332 - 99ms/step</span><br><span class="line">step  30/125 - loss: 1.0604 - acc: 0.3221 - 95ms/step</span><br><span class="line">step  40/125 - loss: 1.0446 - acc: 0.3602 - 94ms/step</span><br><span class="line">step  50/125 - loss: 1.0306 - acc: 0.3811 - 92ms/step</span><br><span class="line">step  60/125 - loss: 1.0166 - acc: 0.4025 - 91ms/step</span><br><span class="line">step  70/125 - loss: 1.0039 - acc: 0.4552 - 91ms/step</span><br><span class="line">step  80/125 - loss: 0.9935 - acc: 0.4743 - 91ms/step</span><br><span class="line">step  90/125 - loss: 0.9841 - acc: 0.5078 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.9752 - acc: 0.5260 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.9695 - acc: 0.5223 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.9620 - acc: 0.5324 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.9583 - acc: 0.5418 - 88ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_v1/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.9590 - acc: 0.7398 - 95ms/step</span><br><span class="line">step 20/84 - loss: 0.9589 - acc: 0.7324 - 75ms/step</span><br><span class="line">step 30/84 - loss: 0.9585 - acc: 0.7328 - 69ms/step</span><br><span class="line">step 40/84 - loss: 0.9579 - acc: 0.7295 - 67ms/step</span><br><span class="line">step 50/84 - loss: 0.9593 - acc: 0.7316 - 67ms/step</span><br><span class="line">step 60/84 - loss: 0.9556 - acc: 0.7346 - 67ms/step</span><br><span class="line">step 70/84 - loss: 0.9594 - acc: 0.7323 - 67ms/step</span><br><span class="line">step 80/84 - loss: 0.9589 - acc: 0.7323 - 66ms/step</span><br><span class="line">step 84/84 - loss: 0.9616 - acc: 0.7330 - 63ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 2/10</span><br><span class="line">step  10/125 - loss: 0.9529 - acc: 0.7086 - 114ms/step</span><br><span class="line">step  20/125 - loss: 0.9500 - acc: 0.7051 - 104ms/step</span><br><span class="line">step  30/125 - loss: 0.9377 - acc: 0.7341 - 98ms/step</span><br><span class="line">step  40/125 - loss: 0.9367 - acc: 0.7318 - 96ms/step</span><br><span class="line">step  50/125 - loss: 0.9213 - acc: 0.7439 - 94ms/step</span><br><span class="line">step  60/125 - loss: 0.8881 - acc: 0.7379 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.8537 - acc: 0.7412 - 92ms/step</span><br><span class="line">step  80/125 - loss: 0.8674 - acc: 0.7401 - 93ms/step</span><br><span class="line">step  90/125 - loss: 0.8386 - acc: 0.7376 - 92ms/step</span><br><span class="line">step 100/125 - loss: 0.8068 - acc: 0.7366 - 92ms/step</span><br><span class="line">step 110/125 - loss: 0.8452 - acc: 0.7364 - 92ms/step</span><br><span class="line">step 120/125 - loss: 0.7921 - acc: 0.7374 - 91ms/step</span><br><span class="line">step 125/125 - loss: 0.8001 - acc: 0.7378 - 89ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.7727 - acc: 0.7500 - 88ms/step</span><br><span class="line">step 20/84 - loss: 0.7982 - acc: 0.7555 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.7934 - acc: 0.7549 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.7760 - acc: 0.7557 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.7713 - acc: 0.7572 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.7942 - acc: 0.7568 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.8002 - acc: 0.7545 - 62ms/step</span><br><span class="line">step 80/84 - loss: 0.7784 - acc: 0.7554 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.8570 - acc: 0.7559 - 59ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 3/10</span><br><span class="line">step  10/125 - loss: 0.7971 - acc: 0.7773 - 109ms/step</span><br><span class="line">step  20/125 - loss: 0.7422 - acc: 0.7855 - 100ms/step</span><br><span class="line">step  30/125 - loss: 0.7125 - acc: 0.7951 - 97ms/step</span><br><span class="line">step  40/125 - loss: 0.7575 - acc: 0.8016 - 94ms/step</span><br><span class="line">step  50/125 - loss: 0.6859 - acc: 0.8089 - 93ms/step</span><br><span class="line">step  60/125 - loss: 0.6763 - acc: 0.8214 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.6569 - acc: 0.8319 - 94ms/step</span><br><span class="line">step  80/125 - loss: 0.6440 - acc: 0.8431 - 93ms/step</span><br><span class="line">step  90/125 - loss: 0.6573 - acc: 0.8521 - 95ms/step</span><br><span class="line">step 100/125 - loss: 0.6323 - acc: 0.8586 - 94ms/step</span><br><span class="line">step 110/125 - loss: 0.6311 - acc: 0.8650 - 94ms/step</span><br><span class="line">step 120/125 - loss: 0.6425 - acc: 0.8699 - 94ms/step</span><br><span class="line">step 125/125 - loss: 0.6360 - acc: 0.8723 - 92ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6429 - acc: 0.9258 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.6527 - acc: 0.9227 - 70ms/step</span><br><span class="line">step 30/84 - loss: 0.6373 - acc: 0.9237 - 66ms/step</span><br><span class="line">step 40/84 - loss: 0.6258 - acc: 0.9271 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6468 - acc: 0.9269 - 63ms/step</span><br><span class="line">step 60/84 - loss: 0.6249 - acc: 0.9279 - 63ms/step</span><br><span class="line">step 70/84 - loss: 0.6554 - acc: 0.9261 - 62ms/step</span><br><span class="line">step 80/84 - loss: 0.6408 - acc: 0.9262 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.6208 - acc: 0.9280 - 58ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 4/10</span><br><span class="line">step  10/125 - loss: 0.6157 - acc: 0.9445 - 107ms/step</span><br><span class="line">step  20/125 - loss: 0.6325 - acc: 0.9371 - 97ms/step</span><br><span class="line">step  30/125 - loss: 0.6596 - acc: 0.9365 - 95ms/step</span><br><span class="line">step  40/125 - loss: 0.6046 - acc: 0.9361 - 93ms/step</span><br><span class="line">step  50/125 - loss: 0.6530 - acc: 0.9320 - 92ms/step</span><br><span class="line">step  60/125 - loss: 0.6364 - acc: 0.9341 - 91ms/step</span><br><span class="line">step  70/125 - loss: 0.6242 - acc: 0.9348 - 90ms/step</span><br><span class="line">step  80/125 - loss: 0.6116 - acc: 0.9347 - 90ms/step</span><br><span class="line">step  90/125 - loss: 0.6371 - acc: 0.9350 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.6125 - acc: 0.9355 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5946 - acc: 0.9368 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.6274 - acc: 0.9363 - 89ms/step</span><br><span class="line">step 125/125 - loss: 0.5729 - acc: 0.9368 - 87ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6248 - acc: 0.9398 - 84ms/step</span><br><span class="line">step 20/84 - loss: 0.6306 - acc: 0.9383 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.6160 - acc: 0.9401 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.6045 - acc: 0.9424 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6228 - acc: 0.9414 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5969 - acc: 0.9419 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.6313 - acc: 0.9408 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.6134 - acc: 0.9413 - 59ms/step</span><br><span class="line">step 84/84 - loss: 0.6030 - acc: 0.9427 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 5/10</span><br><span class="line">step  10/125 - loss: 0.5897 - acc: 0.9445 - 107ms/step</span><br><span class="line">step  20/125 - loss: 0.6003 - acc: 0.9520 - 98ms/step</span><br><span class="line">step  30/125 - loss: 0.6177 - acc: 0.9516 - 96ms/step</span><br><span class="line">step  40/125 - loss: 0.6054 - acc: 0.9525 - 94ms/step</span><br><span class="line">step  50/125 - loss: 0.6303 - acc: 0.9537 - 93ms/step</span><br><span class="line">step  60/125 - loss: 0.6067 - acc: 0.9530 - 94ms/step</span><br><span class="line">step  70/125 - loss: 0.5856 - acc: 0.9540 - 93ms/step</span><br><span class="line">step  80/125 - loss: 0.5976 - acc: 0.9545 - 92ms/step</span><br><span class="line">step  90/125 - loss: 0.6220 - acc: 0.9537 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.6000 - acc: 0.9540 - 91ms/step</span><br><span class="line">step 110/125 - loss: 0.6048 - acc: 0.9522 - 91ms/step</span><br><span class="line">step 120/125 - loss: 0.6144 - acc: 0.9520 - 91ms/step</span><br><span class="line">step 125/125 - loss: 0.6159 - acc: 0.9520 - 89ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6338 - acc: 0.9406 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.6171 - acc: 0.9441 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.6064 - acc: 0.9461 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5934 - acc: 0.9490 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.6212 - acc: 0.9481 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.5881 - acc: 0.9484 - 64ms/step</span><br><span class="line">step 70/84 - loss: 0.6167 - acc: 0.9477 - 63ms/step</span><br><span class="line">step 80/84 - loss: 0.5918 - acc: 0.9479 - 61ms/step</span><br><span class="line">step 84/84 - loss: 0.6166 - acc: 0.9488 - 59ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 6/10</span><br><span class="line">step  10/125 - loss: 0.5790 - acc: 0.9539 - 111ms/step</span><br><span class="line">step  20/125 - loss: 0.5977 - acc: 0.9559 - 99ms/step</span><br><span class="line">step  30/125 - loss: 0.5986 - acc: 0.9557 - 96ms/step</span><br><span class="line">step  40/125 - loss: 0.5777 - acc: 0.9551 - 95ms/step</span><br><span class="line">step  50/125 - loss: 0.5906 - acc: 0.9563 - 93ms/step</span><br><span class="line">step  60/125 - loss: 0.5921 - acc: 0.9577 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.5816 - acc: 0.9589 - 92ms/step</span><br><span class="line">step  80/125 - loss: 0.6051 - acc: 0.9584 - 92ms/step</span><br><span class="line">step  90/125 - loss: 0.5874 - acc: 0.9579 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.5957 - acc: 0.9590 - 91ms/step</span><br><span class="line">step 110/125 - loss: 0.6152 - acc: 0.9592 - 91ms/step</span><br><span class="line">step 120/125 - loss: 0.5884 - acc: 0.9595 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.6076 - acc: 0.9595 - 88ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_v1/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6259 - acc: 0.9492 - 86ms/step</span><br><span class="line">step 20/84 - loss: 0.6103 - acc: 0.9500 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.5972 - acc: 0.9516 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.5912 - acc: 0.9543 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.6174 - acc: 0.9533 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5815 - acc: 0.9540 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.6090 - acc: 0.9535 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.5879 - acc: 0.9540 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.6208 - acc: 0.9546 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 7/10</span><br><span class="line">step  10/125 - loss: 0.5885 - acc: 0.9609 - 111ms/step</span><br><span class="line">step  20/125 - loss: 0.5848 - acc: 0.9637 - 98ms/step</span><br><span class="line">step  30/125 - loss: 0.5899 - acc: 0.9648 - 95ms/step</span><br><span class="line">step  40/125 - loss: 0.6268 - acc: 0.9602 - 93ms/step</span><br><span class="line">step  50/125 - loss: 0.5984 - acc: 0.9617 - 92ms/step</span><br><span class="line">step  60/125 - loss: 0.6060 - acc: 0.9624 - 91ms/step</span><br><span class="line">step  70/125 - loss: 0.6193 - acc: 0.9633 - 90ms/step</span><br><span class="line">step  80/125 - loss: 0.6107 - acc: 0.9638 - 90ms/step</span><br><span class="line">step  90/125 - loss: 0.6120 - acc: 0.9636 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.5850 - acc: 0.9638 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.6033 - acc: 0.9646 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.5901 - acc: 0.9643 - 89ms/step</span><br><span class="line">step 125/125 - loss: 0.5671 - acc: 0.9646 - 87ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6079 - acc: 0.9547 - 87ms/step</span><br><span class="line">step 20/84 - loss: 0.5994 - acc: 0.9566 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.5997 - acc: 0.9573 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5883 - acc: 0.9590 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.6115 - acc: 0.9583 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.5703 - acc: 0.9592 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.6082 - acc: 0.9585 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.5885 - acc: 0.9590 - 60ms/step</span><br><span class="line">step 84/84 - loss: 0.5901 - acc: 0.9596 - 58ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 8/10</span><br><span class="line">step  10/125 - loss: 0.6017 - acc: 0.9648 - 105ms/step</span><br><span class="line">step  20/125 - loss: 0.5801 - acc: 0.9695 - 97ms/step</span><br><span class="line">step  30/125 - loss: 0.5824 - acc: 0.9693 - 92ms/step</span><br><span class="line">step  40/125 - loss: 0.6109 - acc: 0.9689 - 91ms/step</span><br><span class="line">step  50/125 - loss: 0.5899 - acc: 0.9686 - 90ms/step</span><br><span class="line">step  60/125 - loss: 0.5632 - acc: 0.9691 - 89ms/step</span><br><span class="line">step  70/125 - loss: 0.6070 - acc: 0.9694 - 89ms/step</span><br><span class="line">step  80/125 - loss: 0.5853 - acc: 0.9693 - 89ms/step</span><br><span class="line">step  90/125 - loss: 0.6099 - acc: 0.9687 - 90ms/step</span><br><span class="line">step 100/125 - loss: 0.5829 - acc: 0.9694 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5884 - acc: 0.9694 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.6085 - acc: 0.9689 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.5566 - acc: 0.9688 - 88ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6086 - acc: 0.9563 - 89ms/step</span><br><span class="line">step 20/84 - loss: 0.5964 - acc: 0.9582 - 72ms/step</span><br><span class="line">step 30/84 - loss: 0.5983 - acc: 0.9591 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5864 - acc: 0.9605 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6144 - acc: 0.9600 - 63ms/step</span><br><span class="line">step 60/84 - loss: 0.5730 - acc: 0.9604 - 63ms/step</span><br><span class="line">step 70/84 - loss: 0.6029 - acc: 0.9597 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.5849 - acc: 0.9601 - 60ms/step</span><br><span class="line">step 84/84 - loss: 0.5951 - acc: 0.9606 - 58ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 9/10</span><br><span class="line">step  10/125 - loss: 0.5861 - acc: 0.9633 - 110ms/step</span><br><span class="line">step  20/125 - loss: 0.5853 - acc: 0.9652 - 100ms/step</span><br><span class="line">step  30/125 - loss: 0.5744 - acc: 0.9695 - 95ms/step</span><br><span class="line">step  40/125 - loss: 0.5724 - acc: 0.9689 - 93ms/step</span><br><span class="line">step  50/125 - loss: 0.5910 - acc: 0.9702 - 92ms/step</span><br><span class="line">step  60/125 - loss: 0.5868 - acc: 0.9701 - 91ms/step</span><br><span class="line">step  70/125 - loss: 0.5931 - acc: 0.9705 - 91ms/step</span><br><span class="line">step  80/125 - loss: 0.5752 - acc: 0.9715 - 91ms/step</span><br><span class="line">step  90/125 - loss: 0.5677 - acc: 0.9720 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.5638 - acc: 0.9712 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5650 - acc: 0.9715 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.5765 - acc: 0.9715 - 90ms/step</span><br><span class="line">step 125/125 - loss: 0.5677 - acc: 0.9713 - 88ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6051 - acc: 0.9578 - 89ms/step</span><br><span class="line">step 20/84 - loss: 0.5957 - acc: 0.9602 - 73ms/step</span><br><span class="line">step 30/84 - loss: 0.5952 - acc: 0.9607 - 68ms/step</span><br><span class="line">step 40/84 - loss: 0.5860 - acc: 0.9625 - 65ms/step</span><br><span class="line">step 50/84 - loss: 0.6130 - acc: 0.9616 - 64ms/step</span><br><span class="line">step 60/84 - loss: 0.5709 - acc: 0.9620 - 62ms/step</span><br><span class="line">step 70/84 - loss: 0.5985 - acc: 0.9615 - 61ms/step</span><br><span class="line">step 80/84 - loss: 0.5859 - acc: 0.9617 - 60ms/step</span><br><span class="line">step 84/84 - loss: 0.5923 - acc: 0.9622 - 57ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Epoch 10/10</span><br><span class="line">step  10/125 - loss: 0.5898 - acc: 0.9703 - 109ms/step</span><br><span class="line">step  20/125 - loss: 0.5808 - acc: 0.9699 - 96ms/step</span><br><span class="line">step  30/125 - loss: 0.5680 - acc: 0.9706 - 94ms/step</span><br><span class="line">step  40/125 - loss: 0.5849 - acc: 0.9723 - 94ms/step</span><br><span class="line">step  50/125 - loss: 0.5791 - acc: 0.9741 - 94ms/step</span><br><span class="line">step  60/125 - loss: 0.5722 - acc: 0.9742 - 93ms/step</span><br><span class="line">step  70/125 - loss: 0.5931 - acc: 0.9735 - 92ms/step</span><br><span class="line">step  80/125 - loss: 0.5773 - acc: 0.9725 - 91ms/step</span><br><span class="line">step  90/125 - loss: 0.5967 - acc: 0.9731 - 91ms/step</span><br><span class="line">step 100/125 - loss: 0.5766 - acc: 0.9735 - 90ms/step</span><br><span class="line">step 110/125 - loss: 0.5773 - acc: 0.9739 - 90ms/step</span><br><span class="line">step 120/125 - loss: 0.5805 - acc: 0.9740 - 89ms/step</span><br><span class="line">step 125/125 - loss: 0.5547 - acc: 0.9741 - 88ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6034 - acc: 0.9563 - 85ms/step</span><br><span class="line">step 20/84 - loss: 0.5972 - acc: 0.9598 - 69ms/step</span><br><span class="line">step 30/84 - loss: 0.5895 - acc: 0.9604 - 65ms/step</span><br><span class="line">step 40/84 - loss: 0.5856 - acc: 0.9623 - 63ms/step</span><br><span class="line">step 50/84 - loss: 0.6117 - acc: 0.9614 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5724 - acc: 0.9618 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.5927 - acc: 0.9614 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.5870 - acc: 0.9614 - 59ms/step</span><br><span class="line">step 84/84 - loss: 0.5899 - acc: 0.9620 - 57ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_v1/final</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;finetuning/gru/model&#x27;</span>, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="VisualDL-可视化">VisualDL 可视化</h3>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2b39e7bf1322f63a5f0f64c1b107d8ec.png" alt=""></p>
<h3 id="模型评估-2">模型评估</h3>
<ul>
<li>
<p>调用<code>model.evaluate</code>一键评估模型</p>
</li>
<li>
<p>参数：</p>
</li>
</ul>
<!---->
<ul>
<li><code>eval_data</code> (<code>Dataset</code>|<code>DataLoader</code>) - 一个可迭代的数据源，推荐给定一个 <code>paddle.io.Dataset</code> 或 <code>paddle.io.Dataloader</code> 的实例。默认值：None。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = model.evaluate(dev_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finally test acc: %.5f&quot;</span> % results[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/84 - loss: 0.6034 - acc: 0.9563 - 88ms/step</span><br><span class="line">step 20/84 - loss: 0.5972 - acc: 0.9598 - 71ms/step</span><br><span class="line">step 30/84 - loss: 0.5895 - acc: 0.9604 - 67ms/step</span><br><span class="line">step 40/84 - loss: 0.5856 - acc: 0.9623 - 64ms/step</span><br><span class="line">step 50/84 - loss: 0.6117 - acc: 0.9614 - 62ms/step</span><br><span class="line">step 60/84 - loss: 0.5724 - acc: 0.9618 - 61ms/step</span><br><span class="line">step 70/84 - loss: 0.5927 - acc: 0.9614 - 60ms/step</span><br><span class="line">step 80/84 - loss: 0.5870 - acc: 0.9614 - 58ms/step</span><br><span class="line">step 84/84 - loss: 0.5899 - acc: 0.9620 - 56ms/step</span><br><span class="line">Eval samples: 10646</span><br><span class="line">Finally test acc: 0.96196</span><br></pre></td></tr></table></figure>
<h3 id="模型">模型</h3>
<ul>
<li>
<p>调用<code>model.predict</code>进行预测。</p>
</li>
<li>
<p>参数</p>
</li>
</ul>
<!---->
<ul>
<li><code>test_data</code> (<code>Dataset</code>|<code>DataLoader</code>): 一个可迭代的数据源，推荐给定一个<code>paddle.io.Dataset</code> 或 <code>paddle.io.Dataloader</code> 的实例。默认值：None。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="string">&#x27;negative&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;positive&#x27;</span>&#125;</span><br><span class="line">results = model.predict(test_loader, batch_size=<span class="number">64</span>)[<span class="number">0</span>]</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_probs <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># 映射分类label</span></span><br><span class="line">    idx = np.argmax(batch_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    idx = idx.tolist()</span><br><span class="line">    labels = [label_map[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    predictions.extend(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看预测数据前5个样例分类结果</span></span><br><span class="line"><span class="keyword">for</span> idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_ds.data[:<span class="number">5</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Data: &#123;&#125; \t Label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data[<span class="number">0</span>], predictions[idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 42/42 [==============================] - ETA: 3s - 89ms/ste - ETA: 3s - 86ms/ste - ETA: 3s - 96ms/ste - ETA: 3s - 95ms/ste - ETA: 2s - 87ms/ste - ETA: 2s - 83ms/ste - ETA: 2s - 80ms/ste - ETA: 2s - 78ms/ste - ETA: 1s - 75ms/ste - ETA: 1s - 74ms/ste - ETA: 1s - 74ms/ste - ETA: 1s - 70ms/ste - ETA: 1s - 68ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 71ms/ste - ETA: 0s - 71ms/ste - ETA: 0s - 71ms/ste - ETA: 0s - 70ms/ste - ETA: 0s - 68ms/ste - ETA: 0s - 66ms/ste - 64ms/step</span><br><span class="line">Predict samples: 5353</span><br><span class="line">Data: 楼面经理服务态度极差，等位和埋单都差，楼面小妹还挺好 	 Label: negative</span><br><span class="line">Data: 欺负北方人没吃过鲍鱼是怎么着？简直敷衍到可笑的程度，团购连青菜都是两人份？！难吃到死，菜色还特别可笑，什么时候粤菜的小菜改成拍黄瓜了？！把团购客人当傻子，可这满大厅的傻子谁还会再来？！ 	 Label: negative</span><br><span class="line">Data: 如果大家有时间而且不怕麻烦的话可以去这里试试，点一个饭等左2个钟，没错！是两个钟！期间催了n遍都说马上到，结果？呵呵。乳鸽的味道，太咸，可能不新鲜吧……要用重口味盖住异味。上菜超级慢！中途还搞什么表演，麻烦有人手的话就上菜啊，表什么演？！？！要大家饿着看表演吗？最后结账还算错单，我真心服了……有一种店叫不会有下次，大概就是指它吧 	 Label: negative</span><br><span class="line">Data: 偌大的一个大厅就一个人点菜，点菜速度超级慢，菜牌上多个菜停售，连续点了两个没标停售的菜也告知没有，粥上来是凉的，榴莲酥火大了，格格肉超级油腻而且咸?????? 	 Label: negative</span><br><span class="line">Data: 泥撕雞超級好吃！！！吃了一個再叫一個還想打包的節奏！ 	 Label: positive</span><br></pre></td></tr></table></figure>
<h2 id="更换三分类数据集进行测试">更换三分类数据集进行测试</h2>
<p>三分类除了涉及到 positive 和 negative 两种情感外，还有一种 neural 情感，从原始数据集中可以提取到有语义转折的句子，“然而”，“但”都是关键词。从而可以得到 3 份不同语义的数据集。</p>
<h3 id="数据准备-3">数据准备</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Classifier3Dataset</span>(paddle.io.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="built_in">super</span>(Classifier3Dataset, self).__init__()</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_labels</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;2&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">txt_to_list</span>(<span class="params">file_name</span>):</span><br><span class="line">    res_list = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(file_name):</span><br><span class="line">        res_list.append(line.strip().split(<span class="string">&#x27;\t&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> res_list</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Pad, Stack, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> create_dataloader, convert_example</span><br><span class="line"></span><br><span class="line">trainlst = txt_to_list(<span class="string">&#x27;./my_data/train.txt&#x27;</span>)</span><br><span class="line">devlst = txt_to_list(<span class="string">&#x27;./my_data/dev.txt&#x27;</span>)</span><br><span class="line">testlst = txt_to_list(<span class="string">&#x27;./my_data/test.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过get_datasets()函数，将list数据转换为dataset。</span></span><br><span class="line"><span class="comment"># get_datasets()可接收[list]参数，或[str]参数，根据自定义数据集的写法自由选择。</span></span><br><span class="line"><span class="comment"># train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets([&#x27;train&#x27;, &#x27;dev&#x27;, &#x27;test&#x27;])</span></span><br><span class="line">train_ds, dev_ds, test_ds = Classifier3Dataset.get_datasets([trainlst, devlst, testlst])</span><br><span class="line"></span><br><span class="line">label_list = train_ds.get_labels()</span><br><span class="line"><span class="built_in">print</span>(label_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sent, label <span class="keyword">in</span> train_ds[:<span class="number">5</span>]:</span><br><span class="line">    <span class="built_in">print</span> (sent, label)</span><br><span class="line"></span><br><span class="line">tmp = ppnlp.datasets.ChnSentiCorp(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">tmp1 = ppnlp.datasets.MapDatasetWrapper(tmp)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tmp), <span class="built_in">type</span>(tmp1))</span><br><span class="line"><span class="keyword">for</span> sent, label <span class="keyword">in</span> tmp1[:<span class="number">5</span>]:</span><br><span class="line">    <span class="built_in">print</span>(sent, label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reads data and generates mini-batches.</span></span><br><span class="line">trans_fn = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    vocab=vocab,</span><br><span class="line">    unk_token_id=vocab.get(<span class="string">&#x27;[UNK]&#x27;</span>, <span class="number">1</span>),</span><br><span class="line">    is_test=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将读入的数据batch化处理，便于模型batch化运算。</span></span><br><span class="line"><span class="comment"># batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。</span></span><br><span class="line"><span class="comment"># 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.</span></span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=vocab[<span class="string">&#x27;[PAD]&#x27;</span>]),  <span class="comment"># input_ids</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>),  <span class="comment"># seq len</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size改为256</span></span><br><span class="line">train_loader = create_dataloader(</span><br><span class="line">    train_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">dev_loader = create_dataloader(</span><br><span class="line">    dev_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    mode=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br><span class="line"></span><br><span class="line">test_loader = create_dataloader(</span><br><span class="line">    test_ds,</span><br><span class="line">    trans_fn=trans_fn,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    mode=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    batchify_fn=batchify_fn)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;]</span><br><span class="line">环境不错，叉烧包小孩爱吃，三色煎糕很一般 2</span><br><span class="line">刚来的时候让我们等位置，我们就在门口等了十分钟左右，没有见到有人离开，然后有工作人员让我们上来二楼，上来后看到有十来桌是没有人的，既然有位置，为什么非得让我们在门口等位！！！本以为有座位后就可以马上吃饭了，让人内心崩溃的是点菜就等了十分钟才有人有空过来理我们。上菜更是郁闷，端来了一锅猪肚鸡，但是没人开火，要点调味料也没人管，服务质量太差，之前来过一次觉得还可以，这次让我再也不想来这家店了！真心失望 0</span><br><span class="line">口味还可以服务真的差到爆啊我来过45次真的次次都只给差评东西确实不错但你看看你们的服务还收服务费我的天干蒸什么的60块比太古汇翠园还贵主要是没人收台没人倒水谁还要自己倒我的天给你服务费还什么都自己干我接受不了钱花了服务没有实在不行 0</span><br><span class="line">出品不错老字号就是好有山有水有树有鱼赞 1</span><br><span class="line">在江南大道这间～服务态度好差，食物出品平凡，应该唔会再去了 1</span><br><span class="line">&lt;class &#x27;paddlenlp.datasets.chnsenticorp.ChnSentiCorp&#x27;&gt; &lt;class &#x27;paddlenlp.datasets.dataset.MapDatasetWrapper&#x27;&gt;</span><br><span class="line">选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般 1</span><br><span class="line">15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错 1</span><br><span class="line">房间太小。其他的都一般。。。。。。。。。 0</span><br><span class="line">1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸，一摸一个印. 4.硬盘分区不好办. 0</span><br><span class="line">今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量商量,单独出个第6卷,让我们的孩子不会有所遗憾。 1</span><br></pre></td></tr></table></figure>
<blockquote>
<p>训练感觉有点过拟合 加大了<code>batch_size 128-&gt;256</code>, <code>hidden_size 96-&gt;128</code> 同时引入了<code>dropout=0.2</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab = load_vocab(<span class="string">&#x27;./senta_word_dict.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model = GRUModel(</span><br><span class="line">        vocab_size=<span class="built_in">len</span>(vocab),</span><br><span class="line">        num_classes=<span class="built_in">len</span>(label_list),</span><br><span class="line">        direction=<span class="string">&#x27;bidirectional&#x27;</span>,</span><br><span class="line">        padding_idx=vocab[<span class="string">&#x27;[PAD]&#x27;</span>],</span><br><span class="line">        dropout_rate=<span class="number">0.2</span>,</span><br><span class="line">        fc_hidden_size=<span class="number">128</span>) <span class="comment"># out -&gt; 128 -&gt; 3</span></span><br><span class="line"></span><br><span class="line">model = paddle.Model(model)</span><br><span class="line"></span><br><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">        parameters=model.parameters(), learning_rate=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line">loss = paddle.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, loss, metric)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置visualdl路径</span></span><br><span class="line">log_dir = <span class="string">&#x27;./visualdl/gru_3&#x27;</span></span><br><span class="line">callbacks = paddle.callbacks.VisualDL(log_dir=log_dir)</span><br><span class="line"></span><br><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">10</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints/gru_3&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line">step  10/126 - loss: 1.0978 - acc: 0.3398 - 208ms/step</span><br><span class="line">step  20/126 - loss: 1.0996 - acc: 0.3350 - 181ms/step</span><br><span class="line">step  30/126 - loss: 1.0976 - acc: 0.3408 - 173ms/step</span><br><span class="line">step  40/126 - loss: 1.0953 - acc: 0.3434 - 169ms/step</span><br><span class="line">step  50/126 - loss: 1.0956 - acc: 0.3451 - 165ms/step</span><br><span class="line">step  60/126 - loss: 1.0965 - acc: 0.3449 - 162ms/step</span><br><span class="line">step  70/126 - loss: 1.0983 - acc: 0.3444 - 161ms/step</span><br><span class="line">step  80/126 - loss: 1.0960 - acc: 0.3454 - 160ms/step</span><br><span class="line">step  90/126 - loss: 1.0955 - acc: 0.3456 - 159ms/step</span><br><span class="line">step 100/126 - loss: 1.0925 - acc: 0.3445 - 158ms/step</span><br><span class="line">step 110/126 - loss: 1.0915 - acc: 0.3480 - 158ms/step</span><br><span class="line">step 120/126 - loss: 1.0898 - acc: 0.3595 - 158ms/step</span><br><span class="line">step 126/126 - loss: 1.0895 - acc: 0.3675 - 154ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 1.0886 - acc: 0.5535 - 129ms/step</span><br><span class="line">step 16/16 - loss: 1.0892 - acc: 0.5592 - 117ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 2/10</span><br><span class="line">step  10/126 - loss: 1.0875 - acc: 0.5953 - 200ms/step</span><br><span class="line">step  20/126 - loss: 1.0836 - acc: 0.5965 - 177ms/step</span><br><span class="line">step  30/126 - loss: 1.0774 - acc: 0.5948 - 171ms/step</span><br><span class="line">step  40/126 - loss: 1.0721 - acc: 0.5929 - 166ms/step</span><br><span class="line">step  50/126 - loss: 1.0589 - acc: 0.5895 - 164ms/step</span><br><span class="line">step  60/126 - loss: 1.0572 - acc: 0.5824 - 161ms/step</span><br><span class="line">step  70/126 - loss: 1.0510 - acc: 0.5797 - 161ms/step</span><br><span class="line">step  80/126 - loss: 1.0356 - acc: 0.5745 - 160ms/step</span><br><span class="line">step  90/126 - loss: 0.9759 - acc: 0.5755 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.9620 - acc: 0.5741 - 160ms/step</span><br><span class="line">step 110/126 - loss: 0.9256 - acc: 0.5756 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.9095 - acc: 0.5785 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.8743 - acc: 0.5817 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.8846 - acc: 0.6320 - 138ms/step</span><br><span class="line">step 16/16 - loss: 0.9308 - acc: 0.6308 - 123ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 3/10</span><br><span class="line">step  10/126 - loss: 0.9291 - acc: 0.6621 - 197ms/step</span><br><span class="line">step  20/126 - loss: 0.8573 - acc: 0.6590 - 176ms/step</span><br><span class="line">step  30/126 - loss: 0.9236 - acc: 0.6594 - 168ms/step</span><br><span class="line">step  40/126 - loss: 0.8650 - acc: 0.6684 - 164ms/step</span><br><span class="line">step  50/126 - loss: 0.8187 - acc: 0.6738 - 162ms/step</span><br><span class="line">step  60/126 - loss: 0.8715 - acc: 0.6797 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.8505 - acc: 0.6854 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.8257 - acc: 0.6878 - 159ms/step</span><br><span class="line">step  90/126 - loss: 0.8558 - acc: 0.6905 - 159ms/step</span><br><span class="line">step 100/126 - loss: 0.8403 - acc: 0.6926 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7945 - acc: 0.6958 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.8609 - acc: 0.6980 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.8882 - acc: 0.6992 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.8116 - acc: 0.7141 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8394 - acc: 0.7193 - 119ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 4/10</span><br><span class="line">step  10/126 - loss: 0.7935 - acc: 0.7508 - 231ms/step</span><br><span class="line">step  20/126 - loss: 0.7982 - acc: 0.7494 - 190ms/step</span><br><span class="line">step  30/126 - loss: 0.8068 - acc: 0.7480 - 177ms/step</span><br><span class="line">step  40/126 - loss: 0.7855 - acc: 0.7446 - 172ms/step</span><br><span class="line">step  50/126 - loss: 0.8038 - acc: 0.7469 - 168ms/step</span><br><span class="line">step  60/126 - loss: 0.8076 - acc: 0.7500 - 165ms/step</span><br><span class="line">step  70/126 - loss: 0.7809 - acc: 0.7518 - 165ms/step</span><br><span class="line">step  80/126 - loss: 0.7585 - acc: 0.7547 - 163ms/step</span><br><span class="line">step  90/126 - loss: 0.7997 - acc: 0.7564 - 162ms/step</span><br><span class="line">step 100/126 - loss: 0.8227 - acc: 0.7561 - 161ms/step</span><br><span class="line">step 110/126 - loss: 0.7757 - acc: 0.7562 - 160ms/step</span><br><span class="line">step 120/126 - loss: 0.7927 - acc: 0.7562 - 160ms/step</span><br><span class="line">step 126/126 - loss: 0.7383 - acc: 0.7566 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7797 - acc: 0.7344 - 131ms/step</span><br><span class="line">step 16/16 - loss: 0.8309 - acc: 0.7424 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 5/10</span><br><span class="line">step  10/126 - loss: 0.7490 - acc: 0.7945 - 204ms/step</span><br><span class="line">step  20/126 - loss: 0.7892 - acc: 0.7848 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.7733 - acc: 0.7818 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7219 - acc: 0.7829 - 165ms/step</span><br><span class="line">step  50/126 - loss: 0.7361 - acc: 0.7833 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.7994 - acc: 0.7804 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7618 - acc: 0.7810 - 161ms/step</span><br><span class="line">step  80/126 - loss: 0.7607 - acc: 0.7832 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.7378 - acc: 0.7851 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.7430 - acc: 0.7852 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7676 - acc: 0.7856 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.7475 - acc: 0.7865 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.7938 - acc: 0.7866 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7679 - acc: 0.7441 - 132ms/step</span><br><span class="line">step 16/16 - loss: 0.8193 - acc: 0.7505 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 6/10</span><br><span class="line">step  10/126 - loss: 0.7762 - acc: 0.8020 - 200ms/step</span><br><span class="line">step  20/126 - loss: 0.7412 - acc: 0.8004 - 177ms/step</span><br><span class="line">step  30/126 - loss: 0.7627 - acc: 0.8049 - 169ms/step</span><br><span class="line">step  40/126 - loss: 0.7367 - acc: 0.8074 - 165ms/step</span><br><span class="line">step  50/126 - loss: 0.7610 - acc: 0.8068 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.7663 - acc: 0.8056 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7318 - acc: 0.8050 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.7516 - acc: 0.8081 - 159ms/step</span><br><span class="line">step  90/126 - loss: 0.7567 - acc: 0.8073 - 158ms/step</span><br><span class="line">step 100/126 - loss: 0.7430 - acc: 0.8080 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7549 - acc: 0.8069 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.7199 - acc: 0.8068 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.7319 - acc: 0.8067 - 155ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/5</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7777 - acc: 0.7492 - 131ms/step</span><br><span class="line">step 16/16 - loss: 0.8116 - acc: 0.7538 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 7/10</span><br><span class="line">step  10/126 - loss: 0.7169 - acc: 0.8000 - 195ms/step</span><br><span class="line">step  20/126 - loss: 0.7229 - acc: 0.8127 - 176ms/step</span><br><span class="line">step  30/126 - loss: 0.7203 - acc: 0.8156 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7103 - acc: 0.8191 - 166ms/step</span><br><span class="line">step  50/126 - loss: 0.6762 - acc: 0.8223 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.7651 - acc: 0.8215 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7337 - acc: 0.8228 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.7348 - acc: 0.8228 - 160ms/step</span><br><span class="line">step  90/126 - loss: 0.7023 - acc: 0.8231 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.7326 - acc: 0.8229 - 159ms/step</span><br><span class="line">step 110/126 - loss: 0.7133 - acc: 0.8237 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.7220 - acc: 0.8237 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6864 - acc: 0.8238 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7588 - acc: 0.7543 - 133ms/step</span><br><span class="line">step 16/16 - loss: 0.8104 - acc: 0.7581 - 118ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 8/10</span><br><span class="line">step  10/126 - loss: 0.7200 - acc: 0.8258 - 196ms/step</span><br><span class="line">step  20/126 - loss: 0.7397 - acc: 0.8305 - 176ms/step</span><br><span class="line">step  30/126 - loss: 0.7124 - acc: 0.8366 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7328 - acc: 0.8372 - 167ms/step</span><br><span class="line">step  50/126 - loss: 0.7100 - acc: 0.8402 - 164ms/step</span><br><span class="line">step  60/126 - loss: 0.6964 - acc: 0.8393 - 162ms/step</span><br><span class="line">step  70/126 - loss: 0.7164 - acc: 0.8388 - 163ms/step</span><br><span class="line">step  80/126 - loss: 0.7102 - acc: 0.8390 - 165ms/step</span><br><span class="line">step  90/126 - loss: 0.7163 - acc: 0.8385 - 165ms/step</span><br><span class="line">step 100/126 - loss: 0.7125 - acc: 0.8386 - 165ms/step</span><br><span class="line">step 110/126 - loss: 0.7342 - acc: 0.8384 - 164ms/step</span><br><span class="line">step 120/126 - loss: 0.7394 - acc: 0.8379 - 163ms/step</span><br><span class="line">step 126/126 - loss: 0.6978 - acc: 0.8382 - 158ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7574 - acc: 0.7547 - 140ms/step</span><br><span class="line">step 16/16 - loss: 0.8107 - acc: 0.7558 - 127ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 9/10</span><br><span class="line">step  10/126 - loss: 0.6952 - acc: 0.8582 - 208ms/step</span><br><span class="line">step  20/126 - loss: 0.7105 - acc: 0.8520 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.7099 - acc: 0.8492 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7073 - acc: 0.8480 - 167ms/step</span><br><span class="line">step  50/126 - loss: 0.7057 - acc: 0.8480 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.6870 - acc: 0.8474 - 160ms/step</span><br><span class="line">step  70/126 - loss: 0.6980 - acc: 0.8481 - 159ms/step</span><br><span class="line">step  80/126 - loss: 0.6783 - acc: 0.8481 - 158ms/step</span><br><span class="line">step  90/126 - loss: 0.7179 - acc: 0.8467 - 158ms/step</span><br><span class="line">step 100/126 - loss: 0.6865 - acc: 0.8482 - 158ms/step</span><br><span class="line">step 110/126 - loss: 0.6836 - acc: 0.8488 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.7049 - acc: 0.8488 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.7239 - acc: 0.8485 - 154ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7657 - acc: 0.7578 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8159 - acc: 0.7608 - 121ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 10/10</span><br><span class="line">step  10/126 - loss: 0.6735 - acc: 0.8539 - 202ms/step</span><br><span class="line">step  20/126 - loss: 0.7280 - acc: 0.8547 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.7055 - acc: 0.8539 - 170ms/step</span><br><span class="line">step  40/126 - loss: 0.7137 - acc: 0.8554 - 165ms/step</span><br><span class="line">step  50/126 - loss: 0.6825 - acc: 0.8568 - 162ms/step</span><br><span class="line">step  60/126 - loss: 0.7012 - acc: 0.8569 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.7151 - acc: 0.8573 - 160ms/step</span><br><span class="line">step  80/126 - loss: 0.7342 - acc: 0.8571 - 158ms/step</span><br><span class="line">step  90/126 - loss: 0.6726 - acc: 0.8569 - 158ms/step</span><br><span class="line">step 100/126 - loss: 0.7126 - acc: 0.8577 - 157ms/step</span><br><span class="line">step 110/126 - loss: 0.6896 - acc: 0.8578 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.6932 - acc: 0.8581 - 158ms/step</span><br><span class="line">step 126/126 - loss: 0.7185 - acc: 0.8580 - 153ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7604 - acc: 0.7590 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8171 - acc: 0.7626 - 121ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/final</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由于acc还在上升 再训练5个epoch</span></span><br><span class="line"></span><br><span class="line">model.fit(train_loader,</span><br><span class="line">            dev_loader,</span><br><span class="line">            epochs=<span class="number">5</span>,</span><br><span class="line">            save_dir=<span class="string">&#x27;./checkpoints/gru_3&#x27;</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/5</span><br><span class="line">step  10/126 - loss: 0.6639 - acc: 0.8621 - 205ms/step</span><br><span class="line">step  20/126 - loss: 0.6666 - acc: 0.8641 - 184ms/step</span><br><span class="line">step  30/126 - loss: 0.6868 - acc: 0.8629 - 178ms/step</span><br><span class="line">step  40/126 - loss: 0.6952 - acc: 0.8665 - 174ms/step</span><br><span class="line">step  50/126 - loss: 0.6786 - acc: 0.8670 - 172ms/step</span><br><span class="line">step  60/126 - loss: 0.6718 - acc: 0.8674 - 172ms/step</span><br><span class="line">step  70/126 - loss: 0.7139 - acc: 0.8652 - 170ms/step</span><br><span class="line">step  80/126 - loss: 0.7146 - acc: 0.8647 - 168ms/step</span><br><span class="line">step  90/126 - loss: 0.6890 - acc: 0.8660 - 167ms/step</span><br><span class="line">step 100/126 - loss: 0.7247 - acc: 0.8655 - 165ms/step</span><br><span class="line">step 110/126 - loss: 0.6748 - acc: 0.8660 - 164ms/step</span><br><span class="line">step 120/126 - loss: 0.6856 - acc: 0.8658 - 163ms/step</span><br><span class="line">step 126/126 - loss: 0.7343 - acc: 0.8657 - 159ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7668 - acc: 0.7523 - 130ms/step</span><br><span class="line">step 16/16 - loss: 0.8106 - acc: 0.7581 - 116ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 2/5</span><br><span class="line">step  10/126 - loss: 0.6910 - acc: 0.8793 - 201ms/step</span><br><span class="line">step  20/126 - loss: 0.6806 - acc: 0.8732 - 178ms/step</span><br><span class="line">step  30/126 - loss: 0.6818 - acc: 0.8702 - 168ms/step</span><br><span class="line">step  40/126 - loss: 0.7119 - acc: 0.8700 - 164ms/step</span><br><span class="line">step  50/126 - loss: 0.6921 - acc: 0.8701 - 163ms/step</span><br><span class="line">step  60/126 - loss: 0.6909 - acc: 0.8715 - 162ms/step</span><br><span class="line">step  70/126 - loss: 0.7123 - acc: 0.8719 - 162ms/step</span><br><span class="line">step  80/126 - loss: 0.6695 - acc: 0.8720 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.6982 - acc: 0.8723 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.7042 - acc: 0.8723 - 160ms/step</span><br><span class="line">step 110/126 - loss: 0.6810 - acc: 0.8721 - 160ms/step</span><br><span class="line">step 120/126 - loss: 0.7072 - acc: 0.8723 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6921 - acc: 0.8724 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7636 - acc: 0.7559 - 144ms/step</span><br><span class="line">step 16/16 - loss: 0.8205 - acc: 0.7601 - 127ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 3/5</span><br><span class="line">step  10/126 - loss: 0.6746 - acc: 0.8824 - 208ms/step</span><br><span class="line">step  20/126 - loss: 0.6689 - acc: 0.8826 - 183ms/step</span><br><span class="line">step  30/126 - loss: 0.6737 - acc: 0.8811 - 174ms/step</span><br><span class="line">step  40/126 - loss: 0.6970 - acc: 0.8804 - 169ms/step</span><br><span class="line">step  50/126 - loss: 0.6662 - acc: 0.8800 - 165ms/step</span><br><span class="line">step  60/126 - loss: 0.6893 - acc: 0.8801 - 163ms/step</span><br><span class="line">step  70/126 - loss: 0.6819 - acc: 0.8804 - 163ms/step</span><br><span class="line">step  80/126 - loss: 0.6664 - acc: 0.8803 - 162ms/step</span><br><span class="line">step  90/126 - loss: 0.6590 - acc: 0.8793 - 161ms/step</span><br><span class="line">step 100/126 - loss: 0.6681 - acc: 0.8790 - 161ms/step</span><br><span class="line">step 110/126 - loss: 0.6805 - acc: 0.8778 - 160ms/step</span><br><span class="line">step 120/126 - loss: 0.7028 - acc: 0.8781 - 160ms/step</span><br><span class="line">step 126/126 - loss: 0.6371 - acc: 0.8775 - 156ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7640 - acc: 0.7527 - 134ms/step</span><br><span class="line">step 16/16 - loss: 0.8242 - acc: 0.7586 - 120ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 4/5</span><br><span class="line">step  10/126 - loss: 0.6708 - acc: 0.8879 - 200ms/step</span><br><span class="line">step  20/126 - loss: 0.6879 - acc: 0.8863 - 177ms/step</span><br><span class="line">step  30/126 - loss: 0.6686 - acc: 0.8828 - 168ms/step</span><br><span class="line">step  40/126 - loss: 0.6895 - acc: 0.8804 - 164ms/step</span><br><span class="line">step  50/126 - loss: 0.6907 - acc: 0.8794 - 162ms/step</span><br><span class="line">step  60/126 - loss: 0.6387 - acc: 0.8808 - 161ms/step</span><br><span class="line">step  70/126 - loss: 0.6723 - acc: 0.8815 - 162ms/step</span><br><span class="line">step  80/126 - loss: 0.6770 - acc: 0.8814 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.6581 - acc: 0.8829 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.6441 - acc: 0.8826 - 160ms/step</span><br><span class="line">step 110/126 - loss: 0.6712 - acc: 0.8829 - 159ms/step</span><br><span class="line">step 120/126 - loss: 0.6454 - acc: 0.8835 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6550 - acc: 0.8836 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7569 - acc: 0.7500 - 139ms/step</span><br><span class="line">step 16/16 - loss: 0.8168 - acc: 0.7573 - 123ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Epoch 5/5</span><br><span class="line">step  10/126 - loss: 0.6532 - acc: 0.8828 - 193ms/step</span><br><span class="line">step  20/126 - loss: 0.6534 - acc: 0.8863 - 175ms/step</span><br><span class="line">step  30/126 - loss: 0.6523 - acc: 0.8850 - 169ms/step</span><br><span class="line">step  40/126 - loss: 0.6625 - acc: 0.8868 - 166ms/step</span><br><span class="line">step  50/126 - loss: 0.6752 - acc: 0.8874 - 164ms/step</span><br><span class="line">step  60/126 - loss: 0.6603 - acc: 0.8874 - 163ms/step</span><br><span class="line">step  70/126 - loss: 0.6711 - acc: 0.8879 - 162ms/step</span><br><span class="line">step  80/126 - loss: 0.6867 - acc: 0.8875 - 161ms/step</span><br><span class="line">step  90/126 - loss: 0.6505 - acc: 0.8882 - 160ms/step</span><br><span class="line">step 100/126 - loss: 0.6640 - acc: 0.8889 - 158ms/step</span><br><span class="line">step 110/126 - loss: 0.6786 - acc: 0.8891 - 158ms/step</span><br><span class="line">step 120/126 - loss: 0.6796 - acc: 0.8883 - 159ms/step</span><br><span class="line">step 126/126 - loss: 0.6795 - acc: 0.8881 - 155ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7610 - acc: 0.7520 - 136ms/step</span><br><span class="line">step 16/16 - loss: 0.8282 - acc: 0.7571 - 121ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/gru_3/final</span><br></pre></td></tr></table></figure>
<h3 id="VisualDL-可视化-2">VisualDL 可视化</h3>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8e024860774ce180feb704bdfd839d0e.png" alt=""></p>
<h3 id="评估-2">评估</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = model.evaluate(dev_loader)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finally test acc: %.5f&quot;</span> % results[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 10/16 - loss: 0.7610 - acc: 0.7520 - 131ms/step</span><br><span class="line">step 16/16 - loss: 0.8282 - acc: 0.7571 - 117ms/step</span><br><span class="line">Eval samples: 3968</span><br><span class="line">Finally test acc: 0.75706</span><br></pre></td></tr></table></figure>
<h3 id="预测">预测</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">label_map = &#123;<span class="number">0</span>: <span class="string">&#x27;negative&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;positive&#x27;</span>, <span class="number">2</span>:<span class="string">&#x27;neural&#x27;</span>&#125;</span><br><span class="line">results = model.predict(test_loader, batch_size=<span class="number">64</span>)[<span class="number">0</span>]</span><br><span class="line">predictions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_probs <span class="keyword">in</span> results:</span><br><span class="line">    <span class="comment"># 映射分类label</span></span><br><span class="line">    idx = np.argmax(batch_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    idx = idx.tolist()</span><br><span class="line">    labels = [label_map[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]</span><br><span class="line">    predictions.extend(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看预测数据前5个样例分类结果</span></span><br><span class="line"><span class="keyword">for</span> idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_ds.data[:<span class="number">10</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Data: &#123;&#125; \t Label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data[<span class="number">0</span>], predictions[idx]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Predict begin...</span><br><span class="line">step 16/16 [==============================] - ETA: 2s - 191ms/st - ETA: 1s - 162ms/st - ETA: 1s - 155ms/st - ETA: 1s - 146ms/st - ETA: 0s - 144ms/st - ETA: 0s - 141ms/st - ETA: 0s - 138ms/st - 128ms/step</span><br><span class="line">Predict samples: 3986</span><br><span class="line">Data: 出品与环境都算可以吧，服务亦过得去。 	 Label: positive</span><br><span class="line">Data: 菜真是一般般滴，除了贵没啥优点… 	 Label: negative</span><br><span class="line">Data: 第一次是朋友约的喝茶，两层楼都是坐满满客人，我们一行四个人，东西味道很正宗的广式茶点，最爱红米肠，还有乳鸽...网上团购真心觉得划算，在惠福路靠近北京路步行街总觉得东西会很贵，买单时让我感觉很意外四个人才一百多两百，旁边的一些客人也在说好划算哦，因为味道正宗，服务好，性价比高，这就是它为什么现在很多人选择它的原因，朋友家人聚餐的又一个不错的选择... 	 Label: positive</span><br><span class="line">Data: 抱住期待来…有d失望咯…点左bb猪，乳鸽松，上汤豆苗，薄撑，燕窝鹧鸪粥bb猪一般，无好好吃乳鸽松其实不错，不过实在太咸啦！！！！而且好多味精！吃完点饮水都唔够！上汤豆苗d豆苗唔知系乜豆苗，d汤底几好，不过都系咸左小小薄撑边系得得地啊，系唔多得！皮不够烟韧，又唔够脆，特別系在上边的，馅糖太多，花生系咸的！！！！！燕窝粥我无食…环境ok，就系有点嘈。服务一般，不太值10%。总结一句无乜动力令我再帮衬，实在太咸太甜啦！！！！！我宜家仲想买支水一口气喝掉半瓶！！！！！ 	 Label: positive</span><br><span class="line">Data: 点了一个小时的单，还没有上，上了的东西还是生的，跟服务员理论还一脸臭相。差！ 	 Label: negative</span><br><span class="line">Data: 服务贴心。不过鲍鱼没有想象中的那么大，团购划算不少！每人一份感觉挺丰盛的。已经团了两次了，自己觉得好还带家人去尝试。 	 Label: neural</span><br><span class="line">Data: 环境可以就是吃饭时间人比较多，出品比较精致分量男生可能少点 	 Label: positive</span><br><span class="line">Data: 差评！非繁忙时段，两位不可以坐卡座，卡座还有一大片是空着的，二人桌只剩一张而且对着门口不想坐，看到别人两位可以坐卡座，就我们两位不可以坐！差评！果断走！不是只有你一间吃饭的！ 	 Label: negative</span><br><span class="line">Data: 好久都没去过广州塔和珠江边附近走走了，又刚好有个灯光节，在加上抽奖抽中了穿粤传奇的【4D魔幻灯光秀】，就必须要过去看看咯，感受一下灯光节的夜景??不知不觉看完4D魔幻灯光秀都快到九点了，就不如来个宵夜吧。走着走着看到了广州塔下面，有一件叫做赏点点心喝茶的好地方。哈哈…于是我们两个人决定去那吃宵夜咯，增肥的节奏哇！一进去很有广州喝茶的感觉，连装修风格也很像之前茶酒的感觉，好喜欢这种风格哦，不错不错。我们一坐下就看了菜单，看到价格都不是很贵，还是挺实惠的。早上是十一点前埋单，还有得打折呢。我们两个人一共点了五样食物。说真的，他家的出品还真的不错，和我经常去的“点都德”相比来说，他家的份量多点，味道也好点，有一些价格上还便宜先。不管白天还是晚上，喝完茶以后，走走珠江边，看看广州塔，还真是个不错的选择。 	 Label: neural</span><br><span class="line">Data: 晚市点了一条鱼，一份翡翠饺子，一个咖喱牛筋。饺子很一般很一般，鱼很难吃，牛筋少且难吃。两个人花了178没一道菜及格的也是醉了，因为太难吃了到最后菜都没吃完，不知道是不是我们太***丝了到这里都要点上百的菜。反正除了环境好，感觉就只剩招牌了。 	 Label: negative</span><br></pre></td></tr></table></figure>
<h2 id="预训练模型">预训练模型</h2>
<p>近年来随着深度学习的发展，模型参数数量飞速增长，为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分 NLP 任务来说，构建大规模的标注数据集成本过高，非常困难，特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 能够习得通用的语言表示，将预训练模型 Fine-tune 到下游任务，能够获得出色的表现。另外，预训练模型能够避免从零开始训练模型。</p>
<p>参考<a href="https://aistudio.baidu.com/aistudio/projectdetail/1294333">如何通过预训练模型 Fine-tune 下游任务</a></p>
<p align="center">
<img src="https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e" width="60%" height="50%"> <br />
</p>
<br><center>图2：预训练模型一览，图片来源：https://github.com/thunlp/PLMpapers</center></br>
<h3 id="PaddleNLP一键加载预训练模型"><code>PaddleNLP</code>一键加载预训练模型</h3>
<p>情感分析本质是一个文本分类任务，PaddleNLP 对于各种预训练模型已经内置了对于下游任务-文本分类的 Fine-tune 网络。以下教程 ERNIE 为例，介绍如何将预训练模型 Fine-tune 完成文本分类任务。</p>
<ul>
<li>
<p><code>paddlenlp.transformers.ErnieModel()</code></p>
<p>一行代码即可加载预训练模型 ERNIE。</p>
</li>
<li>
<p><code>paddlenlp.transformers.ErnieForSequenceClassification()</code></p>
<p>一行代码即可加载预训练模型 ERNIE 用于文本分类任务的 Fine-tune 网络。<br>
其在 ERNIE 模型后拼接上一个全连接网络（Full Connected）进行分类。</p>
</li>
<li>
<p><code>paddlenlp.transformers.ErnieForSequenceClassification.from_pretrained()</code></p>
<p>只需指定想要使用的模型名称和文本分类的类别数即可完成网络定义。</p>
<p>PaddleNLP 不仅支持 ERNIE 预训练模型，还支持 BERT、RoBERTa、Electra 等预训练模型。</p>
</li>
</ul>
<h3 id="调用ppnlp-transformers-ErnieTokenizer进行数据处理">调用<code>ppnlp.transformers.ErnieTokenizer</code>进行数据处理</h3>
<p>预训练模型 ERNIE 对中文数据的处理是以字为单位。PaddleNLP 对于各种预训练模型已经内置了相应的 tokenizer。指定想要使用的模型名字即可加载对应的 tokenizer。</p>
<p>tokenizer 作用为将原始输入文本转化成模型 model 可以接受的输入数据形式。</p>
<p align="center">
<img src="https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png" hspace='10'/> <br />
</p>
<p align="center">
<img src="https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png" hspace='10'/> <br />
</p>
<br><center>ERNIE模型框架示意图</center></br
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用ernie预训练模型</span></span><br><span class="line"><span class="comment"># ernie</span></span><br><span class="line">model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(<span class="string">&#x27;ernie-1.0&#x27;</span>, num_classes=<span class="number">2</span>)</span><br><span class="line">tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(<span class="string">&#x27;ernie-1.0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ernie-tiny</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.ErnieForSequenceClassification.rom_pretrained(&#x27;ernie-tiny&#x27;,num_classes=2))</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.ErnieTinyTokenizer.from_pretrained(&#x27;ernie-tiny&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用bert预训练模型</span></span><br><span class="line"><span class="comment"># bert-base-chinese</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(&#x27;bert-base-chinese&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(&#x27;bert-base-chinese&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bert-wwm-chinese</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(&#x27;bert-wwm-chinese&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(&#x27;bert-wwm-chinese&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bert-wwm-ext-chinese</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(&#x27;bert-wwm-ext-chinese&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(&#x27;bert-wwm-ext-chinese&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用roberta预训练模型</span></span><br><span class="line"><span class="comment"># roberta-wwm-ext</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(&#x27;roberta-wwm-ext&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># roberta-wwm-ext</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;)</span></span><br><span class="line"></span><br><span class="line">ForSequenceClassification.from_pretrained(<span class="string">&#x27;roberta-wwm-ext&#x27;</span>, num_class=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># roberta-wwm-ext</span></span><br><span class="line"><span class="comment"># model = ppnlp.transformers.RobertaForSequenceClassification.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;, num_class=2)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(&#x27;roberta-wwm-ext-large&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[2021-02-07 16:58:12,351] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams</span><br><span class="line">[2021-02-07 16:58:14,238] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt</span><br></pre></td></tr></table></figure>
<p>github 源码：<a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples/text_classification/pretrained_models">https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples/text_classification/pretrained_models</a></p>
<h1>PaddleNLP 更多项目</h1>
<ul>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1283423">瞧瞧怎么使用 PaddleNLP 内置数据集-基于 seq2vec 的情感分析</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1294333">如何通过预训练模型 Fine-tune 下游任务</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1317771">使用 BiGRU-CRF 模型完成快递单信息抽取</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1329361">使用预训练模型 ERNIE 优化快递单信息抽取</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1321118">使用 Seq2Seq 模型完成自动对联</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1339888">使用预训练模型 ERNIE-GEN 实现智能写诗</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1290873">使用 TCN 网络完成新冠疫情病例数预测</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1339612">使用预训练模型完成阅读理解</a></li>
</ul>
<h1>加入交流群，一起学习吧</h1>
<p>现在就加入 PaddleNLP 的 QQ 技术交流群，一起交流 NLP 技术吧！</p>
<img src="https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394" width="200" height="250" >
]]></content>
  </entry>
  <entry>
    <title>八大排序算法(Python实现)</title>
    <url>/2022/06/01/2fd9e712ce6b4a49a24775586e47e44b/</url>
    <content><![CDATA[<p>[toc]</p>
<p>排序算法可以分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。</p>
<p>常见的内部排序算法有：<code>插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序</code>等。用一张图概括：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExVUDFkNGtXcmd6UTM5WkdIdDZmbjU5OE91aWE3ck1wVzMyWHZCNlBJMUNrQUcwY3JVZ3ZwaElBLzY0MA?x-oss-process=image/format,png" alt="sort"></p>
<p>关于时间复杂度：</p>
<ol>
<li>
<p>平方阶 ($ O(n^2) $) 排序 各类简单排序：直接插入、直接选择和冒泡排序。</p>
</li>
<li>
<p>线性对数阶 ($O(nlog_2(n))$) 排序 快速排序、堆排序和归并排序。</p>
</li>
<li>
<p>($O(n+§)$) 排序，§ 是介于 0 和 1 之间的常数。 希尔排序。</p>
</li>
<li>
<p>线性阶 ($O(n)$) 排序 基数排序，此外还有桶、箱排序。</p>
</li>
</ol>
<p>关于稳定性：</p>
<ol>
<li>
<p>稳定的排序算法：<code>冒泡排序、插入排序、归并排序和基数排序</code>。</p>
</li>
<li>
<p>不是稳定的排序算法：<code>选择排序、快速排序、希尔排序、堆排序</code>。</p>
</li>
</ol>
<p>名词解释：</p>
<ul>
<li>
<p><code>n</code>：数据规模</p>
</li>
<li>
<p><code>k</code>：“桶”的个数</p>
</li>
<li>
<p><code>In-place</code>：占用常数内存，不占用额外内存</p>
</li>
<li>
<p><code>Out-place</code>：占用额外内存</p>
</li>
<li>
<p><code>稳定性</code>：排序后 2 个相等键值的顺序和排序之前它们的顺序相同</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">counter</span>(<span class="params">sort</span>):</span><br><span class="line">    nums = []</span><br><span class="line">    time_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        nums = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            nums.append(i)</span><br><span class="line">        time_start = time.time()</span><br><span class="line">        sort(nums)</span><br><span class="line">        time_end = time.time()</span><br><span class="line">        time_sum += (time_end - time_start)</span><br><span class="line">    <span class="keyword">return</span> time_sum</span><br></pre></td></tr></table></figure>
<h2 id="冒泡排序">冒泡排序</h2>
<p>冒泡排序算法的运作如下：</p>
<p>比较相邻的元素。如果第一个比第二个大，就交换他们两个。<br>
对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。<br>
针对所有的元素重复以上的步骤，除了最后一个。<br>
持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。<br>
以上节选自维基百科</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExPbmtacjZzVEN4YkZsdUpuTGljVjN1MjhRWkdlQnFOdkp3dlFNQ0NkRXNyUDJUQXlvUVVMSFpRLzY0MA?x-oss-process=image/format,png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bubble_sort</span>(<span class="params">nums</span>):</span><br><span class="line">    length = <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">        change_flag = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, length - i):</span><br><span class="line">            <span class="keyword">if</span> nums[j - <span class="number">1</span>] &gt; nums[j]:</span><br><span class="line">                change_flag = <span class="literal">True</span></span><br><span class="line">                nums[j], nums[j - <span class="number">1</span>] = nums[j - <span class="number">1</span>], nums[j]</span><br><span class="line">        <span class="keyword">if</span> change_flag == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">return</span> nums</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(bubble_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(bubble_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[11, 5, 8, 12, 9, 6, 1, 19, 17, 3, 18, 4, 13, 10, 14, 2, 7, 0, 15, 16]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.00996851921081543(s)</span><br></pre></td></tr></table></figure>
<h2 id="选择排序">选择排序</h2>
<p>选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>
<p>以上节选自维基百科</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExRZ3VKQVkwaWI2SFcyY1VaWU9FaWJZa2NhSlRpYXdlRVQ2a0dPZVpBYVVaZ1JiZHdtNlNNQkVoa3cvNjQw?x-oss-process=image/format,png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">selectionSort</span>(<span class="params">arr</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 记录最小数的索引</span></span><br><span class="line">        minIndex = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">            <span class="keyword">if</span> arr[j] &lt; arr[minIndex]:</span><br><span class="line">                minIndex = j</span><br><span class="line">        <span class="comment"># i 不是最小数时，将 i 和最小数进行交换</span></span><br><span class="line">        <span class="keyword">if</span> i != minIndex:</span><br><span class="line">            arr[i], arr[minIndex] = arr[minIndex], arr[i]</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(selectionSort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(selectionSort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[10, 13, 0, 18, 12, 2, 8, 17, 7, 16, 15, 11, 6, 3, 9, 5, 1, 19, 4, 14]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.4408740997314453(s)</span><br></pre></td></tr></table></figure>
<h2 id="插入排序">插入排序</h2>
<p>步骤如下</p>
<p>从第一个元素开始，该元素可以认为已经被排序<br>
取出下一个元素，在已经排序的元素序列中从后向前扫描<br>
如果该元素（已排序）大于新元素，将该元素移到下一位置<br>
重复步骤 3，直到找到已排序的元素小于或者等于新元素的位置<br>
将新元素插入到该位置后<br>
重复步骤 2~5<br>
以上节选自维基百科</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExQN1FSRk0yY2ZkSWFna3NBNE9ySWRPUWUyVFhSS0JxdjNLVmNraGliZWtpY2liZW1sa0ppY091SHlRLzY0MA?x-oss-process=image/format,png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">insertionSort</span>(<span class="params">arr</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        preIndex = i - <span class="number">1</span></span><br><span class="line">        current = arr[i]</span><br><span class="line">        <span class="keyword">while</span> preIndex &gt;= <span class="number">0</span> <span class="keyword">and</span> arr[preIndex] &gt; current:</span><br><span class="line">            arr[preIndex + <span class="number">1</span>] = arr[preIndex]</span><br><span class="line">            preIndex -= <span class="number">1</span></span><br><span class="line">        arr[preIndex + <span class="number">1</span>] = current</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(insertionSort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(insertionSort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[10, 15, 14, 7, 9, 8, 6, 3, 11, 16, 19, 18, 0, 4, 2, 17, 1, 12, 5, 13]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.024000167846679688(s)</span><br></pre></td></tr></table></figure>
<h2 id="希尔排序">希尔排序</h2>
<p>希尔排序通过将比较的全部元素分为几个区域来提升插入排序的性能。这样可以让一个元素可以一次性地朝最终位置前进一大步。然后算法再取越来越小的步长进行排序，算法的最后一步就是普通的插入排序，但是到了这步，需排序的数据几乎是已排好的了（此时插入排序较快）。</p>
<p>以上节选自维基百科</p>
<p><img src="https://img-blog.csdnimg.cn/20200703215518705.gif#pic_center" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">shell_sort</span>(<span class="params">numberlist</span>):</span><br><span class="line">    length = <span class="built_in">len</span>(numberlist)</span><br><span class="line">    gap = length // <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> gap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(gap, length):</span><br><span class="line">            temp = numberlist[i]</span><br><span class="line">            j = i</span><br><span class="line">            <span class="keyword">while</span> j &gt;= gap <span class="keyword">and</span> numberlist[j - gap] &gt; temp:</span><br><span class="line">                numberlist[j] = numberlist[j - gap]</span><br><span class="line">                j -= gap</span><br><span class="line">            numberlist[j] = temp</span><br><span class="line">        gap = gap // <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> numberlist</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(shell_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(shell_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[17, 9, 12, 18, 5, 14, 15, 2, 19, 13, 0, 16, 11, 6, 4, 7, 1, 8, 10, 3]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.11478352546691895(s)</span><br></pre></td></tr></table></figure>
<h2 id="归并排序">归并排序</h2>
<p>原理如下（假设序列共有{displaystyle n}个元素）：</p>
<p>将序列每相邻两个数字进行归并操作，形成{displaystyle ceil(n/2)}个序列，排序后每个序列包含两/一个元素<br>
若此时序列数不是 1 个则将上述序列再次归并，形成{displaystyle ceil(n/4)}个序列，每个序列包含四/三个元素<br>
重复步骤 2，直到所有元素排序完毕，即序列数为 1<br>
以上节选自维基百科</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExoUXZJRHFVUGFwR1Vzd09pYVM5bFVTTTZMa1RCTkF0N2J3QVpDdlhJUHpVd3VnMGJYMWtpYTZXZy82NDA?x-oss-process=image/format,png" alt=""></p>
<p><img src="https://assets.leetcode-cn.com/solution-static/912/912_fig4.gif" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_sort</span>(<span class="params">arr</span>):</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">len</span>(arr) &lt; <span class="number">2</span>):</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    middle = <span class="built_in">len</span>(arr) // <span class="number">2</span></span><br><span class="line">    left, right = arr[<span class="number">0</span>:middle], arr[middle:]</span><br><span class="line">    <span class="keyword">return</span> merge(mergeSort(left), mergeSort(right))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">left, right</span>):</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">while</span> left <span class="keyword">and</span> right:</span><br><span class="line">        <span class="keyword">if</span> left[<span class="number">0</span>] &lt;= right[<span class="number">0</span>]:</span><br><span class="line">            result.append(left.pop(<span class="number">0</span>));</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result.append(right.pop(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">while</span> left:</span><br><span class="line">        result.append(left.pop(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">while</span> right:</span><br><span class="line">        result.append(right.pop(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(merge_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(merge_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[17, 0, 15, 18, 11, 5, 2, 1, 10, 9, 7, 19, 12, 6, 4, 3, 14, 13, 16, 8]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.2873060703277588(s)</span><br></pre></td></tr></table></figure>
<h2 id="快速排序">快速排序</h2>
<p>从数列中挑出一个元素，称为“基准”（pivot），<br>
重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任何一边）。在这个分割结束之后，该基准就处于数列的中间位置。这个称为分割（partition）操作。<br>
递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。<br>
递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。</p>
<p>以上节选自维基百科</p>
<blockquote>
<p>快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。</p>
</blockquote>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cEw1NW9kRzV6c0dOb1FMWXJuMnRzc2Z3emR6OG1HcHJzVmwxaWJ1dHppYlVBeWljcVpaaEJhOWJpY1l3LzY0MA?x-oss-process=image/format,png" alt=""></p>
<p><img src="https://assets.leetcode-cn.com/solution-static/912/912_fig1.gif" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">quick_sort</span>(<span class="params">nums</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">randomized_partition</span>(<span class="params">nums, l, r</span>):</span><br><span class="line">        pivot = random.randint(l, r)</span><br><span class="line">        nums[pivot], nums[r] = nums[r], nums[pivot]</span><br><span class="line">        i = l - <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(l, r):</span><br><span class="line">            <span class="keyword">if</span> nums[j] &lt; nums[r]:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                nums[j], nums[i] = nums[i], nums[j]</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        nums[i], nums[r] = nums[r], nums[i]</span><br><span class="line">        <span class="keyword">return</span> i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">randomized_quicksort</span>(<span class="params">nums, l, r</span>):</span><br><span class="line">        <span class="keyword">if</span> r - l &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        mid = randomized_partition(nums, l, r)</span><br><span class="line">        randomized_quicksort(nums, l, mid - <span class="number">1</span>)</span><br><span class="line">        randomized_quicksort(nums, mid + <span class="number">1</span>, r)</span><br><span class="line"></span><br><span class="line">    randomized_quicksort(nums, <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(quick_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(quick_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[13, 5, 14, 2, 19, 7, 11, 4, 3, 0, 12, 16, 8, 15, 1, 6, 10, 18, 9, 17]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.31716418266296387(s)</span><br></pre></td></tr></table></figure>
<h2 id="堆排序">堆排序</h2>
<p>若以升序排序说明，把数组转换成最大堆积(Max-Heap Heap)，这是一种满足最大堆积性质(Max-Heap Property)的二叉树：对于除了根之外的每个节点 i, A[parent(i)] ≥ A[i]。</p>
<p>重复从最大堆积取出数值最大的结点(把根结点和最后一个结点交换，把交换后的最后一个结点移出堆)，并让残余的堆积维持最大堆积性质。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExXaFFDd3FrVlF0eHBDRXdpYTNLOHRFM2M3R3puU1V4SmhRMWY2OTNTejVCcE40TnQwbVF0RGd3LzY0MA?x-oss-process=image/format,png" alt=""></p>
<p><img src="https://assets.leetcode-cn.com/solution-static/912/912_fig2.gif" alt=""></p>
<p><img src="https://assets.leetcode-cn.com/solution-static/912/912_fig3.gif" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">heap_sort</span>(<span class="params">numberlist</span>):</span><br><span class="line">    <span class="comment"># 下沉</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sift_down</span>(<span class="params">start, end</span>):</span><br><span class="line">        root = start</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            child = <span class="number">2</span> * root + <span class="number">1</span>  <span class="comment"># left child</span></span><br><span class="line">            <span class="keyword">if</span> child &gt; end:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> child + <span class="number">1</span> &lt;= end <span class="keyword">and</span> numberlist[child] &lt; numberlist[child + <span class="number">1</span>]:  <span class="comment"># has right_child &amp;&amp; left_child.val &lt; right_child.val</span></span><br><span class="line">                child += <span class="number">1</span>  <span class="comment"># change to right_child</span></span><br><span class="line">            <span class="keyword">if</span> numberlist[root] &lt; numberlist[child]: <span class="comment"># root.val &lt; left child.val</span></span><br><span class="line">                numberlist[root], numberlist[child] = numberlist[child], numberlist[root]</span><br><span class="line">                root = child</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    length = <span class="built_in">len</span>(numberlist)</span><br><span class="line">    <span class="comment"># 创建最大堆</span></span><br><span class="line">    <span class="keyword">for</span> start <span class="keyword">in</span> <span class="built_in">range</span>((length - <span class="number">2</span>) // <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        sift_down(start, length - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 堆排序</span></span><br><span class="line">    <span class="keyword">for</span> end <span class="keyword">in</span> <span class="built_in">range</span>(length - <span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>):</span><br><span class="line">        numberlist[<span class="number">0</span>], numberlist[end] = numberlist[end], numberlist[<span class="number">0</span>]</span><br><span class="line">        sift_down(<span class="number">0</span>, end - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> numberlist</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(heap_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(heap_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[18, 2, 11, 1, 9, 10, 13, 8, 14, 7, 17, 6, 16, 4, 3, 5, 15, 12, 0, 19]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.4019196033477783(s)</span><br></pre></td></tr></table></figure>
<h2 id="python-heapq">python heapq</h2>
<p>堆的定义：</p>
<p>堆是一种特殊的数据结构，它的通常的表示是它的根结点的值最大或者是最小。</p>
<p>python 中<code>heapq</code>的使用</p>
<p>列出一些常见的用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">heap = [] <span class="comment"># 建立一个常见的堆</span></span><br><span class="line"></span><br><span class="line">heappush(heap,item) <span class="comment"># 往堆中插入一条新的值</span></span><br><span class="line"></span><br><span class="line">item = heappop(heap) <span class="comment"># 弹出最小的值</span></span><br><span class="line"></span><br><span class="line">item = heap[<span class="number">0</span>] <span class="comment"># 查看堆中最小的值，不弹出</span></span><br><span class="line"></span><br><span class="line">heapify(x) <span class="comment"># 以线性时间将一个列表转为堆</span></span><br><span class="line"></span><br><span class="line">item = heapreplace(heap,item) <span class="comment"># 弹出一个最小的值，然后将item插入到堆当中。堆的整体的结构不会发生改变。</span></span><br><span class="line">heappoppush() <span class="comment"># 弹出最小的值，并且将新的值插入其中</span></span><br><span class="line"></span><br><span class="line">merge() <span class="comment"># 将多个堆进行合并</span></span><br><span class="line"></span><br><span class="line">nlargest(n , iterbale, key=<span class="literal">None</span>) <span class="comment"># 从堆中找出做大的N个数，key的作用和sorted( )方法里面的key类似，用列表元素的某个属性和函数作为关键字</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用python内置库</span></span><br><span class="line"><span class="keyword">from</span> heapq <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">heapsort</span>(<span class="params">iterable</span>):</span><br><span class="line">    h = []</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> iterable:</span><br><span class="line">        heappush(h, value)</span><br><span class="line">    <span class="keyword">return</span> [heappop(h) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(h))]</span><br></pre></td></tr></table></figure>
<h2 id="计数排序">计数排序</h2>
<p>以上节选自维基百科<br>
计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExpYXlFNExkaWFVcW1tek9aakl5bzhHNTFVZzlrcEZISGljdDBTNHdhY2UxdTVzMk9LZU1HZVBoaWJRLzY0MA?x-oss-process=image/format,png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">counting_sort</span>(<span class="params">input_list</span>):</span><br><span class="line">    length = <span class="built_in">len</span>(input_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> length &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> input_list</span><br><span class="line"></span><br><span class="line">    max_num = <span class="built_in">max</span>(input_list)</span><br><span class="line">    count = [<span class="number">0</span>] * (max_num + <span class="number">1</span>)  <span class="comment"># 多加一个0</span></span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> input_list:</span><br><span class="line">        count[element] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    output_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_num + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(count[i]):  <span class="comment"># count[i]表示元素i出现的次数，如果有多次，通过循环重复追加</span></span><br><span class="line">            output_list.append(i)</span><br><span class="line">    <span class="keyword">return</span> output_list</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nums = [16, 16, 16, 16, 10, 4, 8, 2, 12, 14, 13, 17, 15, 7, 0, 3, 1, 5, 6, 9]</span></span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(counting_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(counting_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[6, 19, 5, 7, 17, 0, 12, 9, 3, 15, 18, 13, 14, 10, 11, 1, 2, 16, 8, 4]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.039923667907714844(s)</span><br></pre></td></tr></table></figure>
<h2 id="桶排序">桶排序</h2>
<p>桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：</p>
<p>在额外空间充足的情况下，尽量增大桶的数量</p>
<p>使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中</p>
<p>同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。</p>
<p>（1）什么时候最快</p>
<p>当输入的数据可以均匀的分配到每一个桶中。</p>
<p>（2）什么时候最慢</p>
<p>当输入的数据被分配到了同一个桶中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bucket_sort</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;桶排序&quot;&quot;&quot;</span></span><br><span class="line">    min_num = <span class="built_in">min</span>(s)</span><br><span class="line">    max_num = <span class="built_in">max</span>(s)</span><br><span class="line">    <span class="comment"># 桶的大小</span></span><br><span class="line">    bucket_range = (max_num - min_num) / <span class="built_in">len</span>(s)</span><br><span class="line">    <span class="comment"># 桶数组</span></span><br><span class="line">    count_list = [ [] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s) + <span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># 向桶数组填数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">        count_list[<span class="built_in">int</span>((i-min_num) // bucket_range)].append(i)</span><br><span class="line">    s.clear()</span><br><span class="line">    <span class="comment"># 回填，这里桶内部排序直接调用了sorted</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> count_list:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">sorted</span>(i):</span><br><span class="line">            s.append(j)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nums = [16, 16, 16, 16, 10, 4, 8, 2, 12, 14, 13, 17, 15, 7, 0, 3, 1, 5, 6, 9]</span></span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(bucket_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(bucket_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[17, 6, 3, 19, 11, 12, 15, 9, 4, 0, 16, 18, 14, 8, 13, 7, 1, 10, 5, 2]</span><br><span class="line">排序后数组：</span><br><span class="line">None</span><br><span class="line">运行时间测试: 0.05981326103210449(s)</span><br></pre></td></tr></table></figure>
<h2 id="基数排序">基数排序</h2>
<p>基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。</p>
<h3 id="基数排序-vs-计数排序-vs-桶排序">基数排序 vs 计数排序 vs 桶排序</h3>
<p>这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异：</p>
<ul>
<li>
<p>基数排序：根据键值的每位数字来分配桶；</p>
</li>
<li>
<p>计数排序：每个桶只存储单一键值；</p>
</li>
<li>
<p>桶排序：每个桶存储一定范围的数值。</p>
</li>
</ul>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9nUHRQU21ZRDM2aWNRVUFXZ2tVSTRyYzZzczFWYXV5cExKR0Zqb25oUHpvNzQwNzRJQmp4Q1BHSFdzdnFzeTVORmhpYmZQSGVFQ25Mc2JiMHBZSFM4NEJ3LzY0MA?x-oss-process=image/format,png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">radix_sort</span>(<span class="params"><span class="built_in">list</span></span>):</span><br><span class="line">    i = <span class="number">0</span>                                    <span class="comment">#初始为个位排序</span></span><br><span class="line">    n = <span class="number">1</span>                                     <span class="comment">#最小的位数置为1（包含0）</span></span><br><span class="line">    max_num = <span class="built_in">max</span>(<span class="built_in">list</span>) <span class="comment">#得到带排序数组中最大数</span></span><br><span class="line">    <span class="keyword">while</span> max_num &gt; <span class="number">10</span>**n: <span class="comment">#得到最大数是几位数</span></span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; n:</span><br><span class="line">        bucket = &#123;&#125; <span class="comment">#用字典构建桶</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            bucket.setdefault(x, []) <span class="comment">#将每个桶置空</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">list</span>: <span class="comment">#对每一位进行排序</span></span><br><span class="line">            radix =<span class="built_in">int</span>((x / (<span class="number">10</span>**i)) % <span class="number">10</span>) <span class="comment">#得到每位的基数</span></span><br><span class="line">            bucket[radix].append(x) <span class="comment">#将对应的数组元素加入到相 #应位基数的桶中</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(bucket[k]) != <span class="number">0</span>: <span class="comment">#若桶不为空</span></span><br><span class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> bucket[k]: <span class="comment">#将该桶中每个元素</span></span><br><span class="line">                    <span class="built_in">list</span>[j] = y <span class="comment">#放回到数组中</span></span><br><span class="line">                    j += <span class="number">1</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span>  <span class="built_in">list</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    nums.append(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nums = [16, 16, 16, 16, 10, 4, 8, 2, 12, 14, 13, 17, 15, 7, 0, 3, 1, 5, 6, 9]</span></span><br><span class="line"></span><br><span class="line">random.shuffle(nums)  <span class="comment"># 打乱数组nums的顺序</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;未排序数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;排序后数组：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(radix_sort(nums))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;运行时间测试: <span class="subst">&#123;counter(radix_sort)&#125;</span>(s)&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">未排序数组：</span><br><span class="line">[11, 4, 9, 7, 2, 13, 15, 5, 19, 10, 1, 18, 16, 17, 6, 3, 0, 8, 14, 12]</span><br><span class="line">排序后数组：</span><br><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</span><br><span class="line">运行时间测试: 0.09178805351257324(s)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Linux 正则表达式</title>
    <url>/2022/06/01/7610eab370f54ebe8bdc50a4dbc427b8/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="正则表达式">正则表达式</h2>
<p>什么是正则表达式呢？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">正则表达式，又称正规表示式、正规表示法、正规表达式、规则表达式、常规表示法（英语：Regular Expression，在代码中常简写为 regex、regexp 或 RE），计算机科学的一个概念。正则表达式使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。在很多文本编辑器里，正则表达式通常被用来检索、替换那些符合某个模式的文本。</span><br><span class="line"></span><br><span class="line">许多程序设计语言都支持利用正则表达式进行字符串操作。例如，在 Perl 中就内建了一个功能强大的正则表达式引擎。正则表达式这个概念最初是由 UNIX 中的工具软件（例如sed和grep）普及开的。正则表达式通常缩写成“regex”，单数有 regexp、regex，复数有 regexps、regexes、regexen。</span><br></pre></td></tr></table></figure>
<h3 id="基本语法">基本语法</h3>
<p>一个正则表达式通常被称为一个模式（pattern），为用来描述或者匹配一系列符合某个句法规则的字符串。<br>
<strong>选择</strong><br>
<code>|</code> 竖直分隔符表示选择，例如 <code>boy|girl</code> 可以匹配 <code>boy</code> 或者 <code>girl</code>。</p>
<p><strong>数量限定</strong><br>
数量限定除了我们举例用的 * 还有 + 加号 ? 问号，如果在一个模式中不加数量限定符则表示出现一次且仅出现一次：</p>
<ul>
<li><code>+</code> 表示前面的字符必须出现至少一次(1 次或多次)，例如 goo+gle 可以匹配 gooogle，goooogle 等；</li>
<li><code>?</code> 表示前面的字符最多出现一次（0 次或 1 次），例如，colou?r，可以匹配 color 或者 colour;</li>
<li><code>*</code> 星号代表前面的字符可以不出现，也可以出现一次或者多次（0 次、或 1 次、或多次），例如，0*42 可以匹配 42、042、0042、00042 等。</li>
</ul>
<p><strong>范围和优先级</strong><br>
<code>()</code> 圆括号可以用来定义模式字符串的范围和优先级，这可以简单的理解为是否将括号内的模式串作为一个整体。例如，<code>gr(a|e)y</code> 等价于 <code>gray|grey</code>，（这里体现了优先级，竖直分隔符用于选择 <code>a</code> 或者 <code>e</code> 而不是 <code>gra</code> 和 <code>ey）</code>，<code>(grand)?father</code> 匹配 <code>father</code> 和 <code>grandfather</code>（这里体现了范围，<code>?</code> 将圆括号内容作为一个整体匹配）。</p>
<p><strong>语法（部分）</strong><br>
正则表达式有多种不同的风格，下面列举一些常用的作为 PCRE 子集的适用于 <code>perl</code> 和 <code>python</code> 编程语言及 <code>grep</code> 或 <code>egrep</code> 的正则表达式匹配规则：</p>
<p>PCRE（Perl Compatible Regular Expressions 中文含义：perl 语言兼容正则表达式）是一个用 C 语言编写的正则表达式函数库，由菲利普.海泽(Philip Hazel)编写。PCRE 是一个轻量级的函数库，比 Boost 之类的正则表达式库小得多。PCRE 十分易用，同时功能也很强大，性能超过了 POSIX 正则表达式库和一些经典的正则表达式库。</p>
<table>
<thead>
<tr>
<th>字符</th>
<th>描述</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>\</code></td>
<td>将下一个字符标记为一个特殊字符、或一个原义字符。 例如 <code>n</code> 匹配字符 <code>n</code>。<code>\n</code> 匹配一个换行符。序列 <code>\\</code> 匹配 <code>\</code> 而 <code>\(</code> 则匹配 <code>(</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>^</code></td>
<td>匹配输入字符串的开始位置。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>$</code></td>
<td>匹配输入字符串的结束位置。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>&#123;n&#125;</code></td>
<td>n 是一个非负整数。匹配确定的 n 次。例如 <code>o&#123;2&#125;</code> 不能匹配 <code>Bob</code> 中的 <code>o</code>，但是能匹配 <code>food</code> 中的两个 <code>o</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>&#123;n,&#125;</code></td>
<td>n 是一个非负整数。至少匹配 n 次。例如 <code>o&#123;2,&#125;</code> 不能匹配 Bob 中的 o，但能匹配 <code>foooood</code> 中的所有 <code>o</code>。<code>o&#123;1,&#125;</code> 等价于 <code>o+</code>。<code>o&#123;0,&#125;</code> 则等价于 <code>o*</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>&#123;n,m&#125;</code></td>
<td>m 和 n 均为非负整数，其中 n&lt;=m。最少匹配 n 次且最多匹配 m 次。例如，<code>o&#123;1,3&#125;</code> 将匹配 <code>fooooood</code> 中的前三个 o。<code>o&#123;0,1&#125;</code> 等价于 <code>o?</code>。请注意在逗号和两个数之间不能有空格。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>*</code></td>
<td>匹配前面的子表达式零次或多次。例如，<code>zo*</code> 能匹配 <code>z、zo</code> 以及 <code>zoo</code>。<code>*</code> 等价于 <code>&#123;0,&#125;</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>+</code></td>
<td>匹配前面的子表达式一次或多次。例如，<code>zo+</code> 能匹配 <code>zo</code> 以及 <code>zoo</code>，但不能匹配 <code>z</code>。<code>+</code> 等价于 <code>&#123;1,&#125;</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>?</code></td>
<td>匹配前面的子表达式零次或一次。例如，<code>do(es)?</code> 可以匹配 <code>do</code> 或 <code>does</code> 中的 <code>do</code>。<code>?</code> 等价于 <code>&#123;0,1&#125;</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>?</code></td>
<td>当该字符紧跟在任何一个其他限制符<code>（*，+，?，&#123;n&#125;，&#123;n,&#125;，&#123;n,m&#125;）</code>后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 <code>oooo</code>，<code>o+?</code> 将匹配单个 <code>o</code>，而 <code>o+</code> 将匹配所有 <code>o</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>.</code></td>
<td>匹配除 <code>\n</code> 之外的任何单个字符。要匹配包括 <code>\n</code> 在内的任何字符，请使用类似 <code>(.｜\n)</code> 的模式。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>(pattern)</code></td>
<td>匹配 pattern 并获取这一匹配的子字符串。该子字符串用于向后引用。要匹配圆括号字符，请使用 <code>\(</code> 和 <code>\)</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>`x</td>
<td>y`</td>
<td>匹配 x 或 y。例如，`z</td>
<td>food<code>能匹配 </code>z<code>或</code>food<code>。</code>(z</td>
<td>f)ood<code>则匹配 </code>zood<code>或</code>food`。</td>
</tr>
<tr>
<td><code>[xyz]</code></td>
<td>字符集合（character class）。匹配所包含的任意一个字符。例如，<code>[abc]</code> 可以匹配 <code>plain</code> 中的 <code>a</code>。其中特殊字符仅有反斜线 <code>\</code> 保持特殊含义，用于转义字符。其它特殊字符如星号、加号、各种括号等均作为普通字符。脱字符<code>^</code>如果出现在首位则表示负值字符集合, 如果出现在字符串中间就仅作为普通字符；连字符 <code>-</code> 如果出现在字符串中间表示字符范围描述；如果出现在首位则仅作为普通字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>[^xyz]</code></td>
<td>排除型（negate）字符集合。匹配未列出的任意字符。例如，<code>[^abc]</code> 可以匹配 <code>plain</code> 中的 <code>plin</code>。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>[a-z]</code></td>
<td>字符范围。匹配指定范围内的任意字符。例如，<code>[a-z]</code> 可以匹配 <code>a</code> 到 <code>z</code>范围内的任意小写字母字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>[^a-z]</code></td>
<td>排除型的字符范围。匹配任何不在指定范围内的任意字符。例如，<code>[^a-z]</code> 可以匹配任何不在 <code>a</code> 到 <code>z</code> 范围内的任意字符。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>优先级</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>优先级为从上到下从左到右，依次降低：</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>运算符</th>
<th>说明</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>\</code></td>
<td>转义符</td>
<td></td>
</tr>
<tr>
<td><code>()·</code>，<code>(?:)</code>，<code>(?=)</code>，<code>[]</code></td>
<td>括号和中括号</td>
<td></td>
</tr>
<tr>
<td><code>*</code>，<code>+</code>，<code>?</code>，<code>&#123;n&#125;</code>，<code>&#123;n,&#125;</code>，<code>&#123;n,m&#125;</code></td>
<td>限定符</td>
<td></td>
</tr>
<tr>
<td><code>^</code>，<code>$</code>，<code>\</code></td>
<td>任何元字符 定位点和序列</td>
<td></td>
</tr>
<tr>
<td>`</td>
<td>`</td>
<td>选择</td>
</tr>
</tbody>
</table>
<h3 id="grep"><code>grep</code></h3>
<p><code>grep</code> 命令用于打印输出文本中匹配的模式串，它使用正则表达式作为模式匹配的条件。<code>grep</code> 支持三种正则表达式引擎，分别用三个参数指定：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-E</code></td>
<td>POSIX 扩展正则表达式，ERE</td>
</tr>
<tr>
<td><code>-G</code></td>
<td>POSIX 基本正则表达式，BRE</td>
</tr>
<tr>
<td><code>-P</code></td>
<td>Perl 正则表达式，PCRE</td>
</tr>
</tbody>
</table>
<p>常用参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-b</code></td>
<td>将二进制文件作为文本来进行匹配</td>
</tr>
<tr>
<td><code>-c</code></td>
<td>统计以模式匹配的数目</td>
</tr>
<tr>
<td><code>-i</code></td>
<td>忽略大小写</td>
</tr>
<tr>
<td><code>-n</code></td>
<td>显示匹配文本所在行的行号</td>
</tr>
<tr>
<td><code>-v</code></td>
<td>反选，输出不匹配行的内容</td>
</tr>
<tr>
<td><code>-r</code></td>
<td>递归匹配查找</td>
</tr>
<tr>
<td><code>-A n</code></td>
<td>n 为正整数，表示 after 的意思，除了列出匹配行之外，还列出后面的 n 行</td>
</tr>
<tr>
<td><code>-B n</code></td>
<td>n 为正整数，表示 before 的意思，除了列出匹配行之外，还列出前面的 n 行</td>
</tr>
<tr>
<td><code>--color=auto</code></td>
<td>将输出中的匹配项设置为自动颜色显示</td>
</tr>
</tbody>
</table>
<ul>
<li>位置<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grep <span class="string">&#x27;shiyanlou&#x27;</span> /etc/group</span><br><span class="line"><span class="comment"># 匹配开头</span></span><br><span class="line">grep <span class="string">&#x27;^shiyanlou&#x27;</span> /etc/group</span><br></pre></td></tr></table></figure>
</li>
<li>数量<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将匹配以&#x27;z&#x27;开头以&#x27;o&#x27;结尾的所有字符串</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;zero\nzo\nzoo&#x27;</span> | grep <span class="string">&#x27;z.*o&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配以&#x27;z&#x27;开头以&#x27;o&#x27;结尾，中间包含一个任意字符的字符串</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;zero\nzo\nzoo&#x27;</span> | grep <span class="string">&#x27;z.o&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配以&#x27;z&#x27;开头，以任意多个&#x27;o&#x27;结尾的字符串</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;zero\nzo\nzoo&#x27;</span> | grep <span class="string">&#x27;zo*&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>选择<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># grep默认是区分大小写的，这里将匹配所有的小写字母</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[a-z]&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配所有的数字</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[0-9]&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配所有的数字</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[[:digit:]]&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配所有的小写字母</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[[:lower:]]&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配所有的大写字母</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[[:upper:]]&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配所有的字母和数字，包括0-9，a-z，A-Z</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[[:alnum:]]&#x27;</span></span><br><span class="line"><span class="comment"># 将匹配所有的字母</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;1234\nabcd&#x27;</span> | grep <span class="string">&#x27;[[:alpha:]]&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="sed"><code>sed</code></h3>
<p><code>sed</code> 工具在 <code>man</code> 手册里面的全名为&quot;sed - stream editor for filtering and transforming text &quot;，意即，用于过滤和转换文本的流编辑器。</p>
<p>在 Linux/UNIX 的世界里敢称为编辑器的工具，大都非等闲之辈，比如前面的 <code>vi/vim</code>（编辑器之神），<code>emacs</code>（神的编辑器），<code>gedit</code> 这些编辑器。<code>sed</code> 与上述的最大不同之处在于它是一个非交互式的编辑器，下面我们就开始介绍 <code>sed</code> 这个编辑器。</p>
<p><code>sed</code>命令基本格式：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed [参数]... [执行命令] [输入文件]...</span><br><span class="line"><span class="comment"># 形如：</span></span><br><span class="line">$ sed -i <span class="string">&#x27;s/sad/happy/&#x27;</span> <span class="built_in">test</span> <span class="comment"># 表示将test文件中的&quot;sad&quot;替换为&quot;happy&quot;</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-n</code></td>
<td>安静模式，只打印受影响的行，默认打印输入数据的全部内容</td>
</tr>
<tr>
<td><code>-e</code></td>
<td>用于在脚本中添加多个执行命令一次执行，在命令行中执行多个命令通常不需要加该参数</td>
</tr>
<tr>
<td><code>-f filename</code></td>
<td>指定执行 filename 文件中的命令</td>
</tr>
<tr>
<td><code>-r</code></td>
<td>使用扩展正则表达式，默认为标准正则表达式</td>
</tr>
<tr>
<td><code>-i</code></td>
<td>将直接修改输入文件内容，而不是打印到标准输出设备</td>
</tr>
</tbody>
</table>
<p>sed 执行命令格式：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[n1][,n2]<span class="built_in">command</span></span><br><span class="line">[n1][~step]<span class="built_in">command</span></span><br></pre></td></tr></table></figure>
<p>其中一些命令可以在后面加上作用范围，形如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/sad/happy/g&#x27;</span> <span class="built_in">test</span> <span class="comment"># g 表示全局范围</span></span><br><span class="line">sed -i <span class="string">&#x27;s/sad/happy/4&#x27;</span> <span class="built_in">test</span> <span class="comment"># 4 表示指定行中的第四个匹配字符串</span></span><br></pre></td></tr></table></figure>
<p>其中 <code>n1</code>,<code>n2</code> 表示输入内容的行号，它们之间为 , 逗号则表示从 n1 到 n2 行，如果为 <code>~</code> 波浪号则表示从 n1 开始以 step 为步进的所有行；command 为执行动作，下面为一些常用动作指令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>s</code></td>
<td>行内替换</td>
</tr>
<tr>
<td><code>c</code></td>
<td>整行替换</td>
</tr>
<tr>
<td><code>a</code></td>
<td>插入到指定行的后面</td>
</tr>
<tr>
<td><code>i</code></td>
<td>插入到指定行的前面</td>
</tr>
<tr>
<td><code>p</code></td>
<td>打印指定行，通常与 <code>-n</code> 参数配合使用</td>
</tr>
<tr>
<td><code>d</code></td>
<td>删除指定行</td>
</tr>
</tbody>
</table>
<h3 id="awk"><code>awk</code></h3>
<p>AWK 是一种优良的文本处理工具，Linux 及 Unix 环境中现有的功能最强大的数据处理引擎之一。其名称得自于它的创始人 Alfred Aho（阿尔佛雷德·艾侯）、Peter Jay Weinberger（彼得·温伯格）和 Brian Wilson Kernighan（布莱恩·柯林汉)姓氏的首个字母 <code>AWK</code>，三位创建者已将它正式定义为“样式扫描和处理语言”。它允许你创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。最简单地说，AWK 是一种用于处理文本的编程语言工具。</p>
<p>在大多数 Linux 发行版上面，实际我们使用的是 gawk（GNU awk，awk 的 GNU 版本），在我们的环境中 ubuntu 上，默认提供的是 mawk，不过我们通常可以直接使用 awk 命令（awk 语言的解释器），因为系统已经为我们创建好了 awk 指向 mawk 的符号链接。</p>
<p>awk 所有的操作都是基于 pattern(模式)—action(动作)对来完成的，如下面的形式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pattern &#123;action&#125;</span><br></pre></td></tr></table></figure>
<p>awk 基本命令格式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">awk [-F fs] [-v var=value] [-f prog-file | &#x27;program text&#x27;] [file...]</span><br></pre></td></tr></table></figure>
<p>其中 <code>-F</code> 参数用于预先指定前面提到的字段分隔符（还有其他指定字段的方式），<code>-v</code> 用于预先为 <code>awk</code> 程序指定变量，<code>-f</code> 参数用于指定 <code>awk</code> 命令要执行的程序文件，或者在不加 <code>-f</code> 参数的情况下直接将程序语句放在这里，最后为 <code>awk</code> 需要处理的文本输入，且可以同时输入多个文本文件。现在我们还是直接来具体体验一下吧。</p>
<p><code>awk</code>常用内置变量</p>
<table>
<thead>
<tr>
<th>变量名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>FILENAME</code></td>
<td>当前输入文件名，若有多个文件，则只表示第一个。如果输入是来自标准输入，则为空字符串</td>
</tr>
<tr>
<td><code>$0</code></td>
<td>当前记录的内容</td>
</tr>
<tr>
<td><code>$N</code></td>
<td><code>N</code> 表示字段号，最大值为<code>NF</code>变量的值</td>
</tr>
<tr>
<td><code>FS</code></td>
<td>字段分隔符，由正则表达式表示，默认为空格</td>
</tr>
<tr>
<td><code>RS</code></td>
<td>输入记录分隔符，默认为 <code>\n</code>，即一行为一个记录</td>
</tr>
<tr>
<td><code>NF</code></td>
<td>当前记录字段数</td>
</tr>
<tr>
<td><code>NR</code></td>
<td>已经读入的记录数</td>
</tr>
<tr>
<td><code>FNR</code></td>
<td>当前输入文件的记录数，请注意它与 NR 的区别</td>
</tr>
<tr>
<td><code>OFS</code></td>
<td>输出字段分隔符，默认为空格</td>
</tr>
<tr>
<td><code>ORS</code></td>
<td>输出记录分隔符，默认为 <code>\n</code></td>
</tr>
</tbody>
</table>
]]></content>
  </entry>
  <entry>
    <title>传输层：UDP协议</title>
    <url>/2022/05/31/74b8d451045a4084a5d37fa7915ca96a/</url>
    <content><![CDATA[<p>[toc]</p>
<p>从之前介绍的<strong>网络层</strong>协议来看，通信的两端是两台主机，IP 数据报首部就标明了这两台主机的 IP 地址。但是从<strong>传输层</strong>来看，是发送方主机中的一个进程与接收方主机中的一个进程在交换数据，因此严格地讲，通信双方不是主机，而是<strong>主机中的进程</strong>。</p>
<p>传输层协议有 UDP 和 TCP，本节我们就先来介绍 UDP。</p>
<p>主机中常常有多个应用进程同时在与外部通信（比如你的浏览器和 QQ 在同时运行），下图中，A 主机的 AP1 进程在与 B 主机的 AP3 进程通信，同时主机 A 的 AP2 进程也在与 B 主机的 AP4 进程通信。</p>
<p>两个主机的传输层之间有一个灰色双向箭头，写着“传输层提供应用进程间的逻辑通信”。</p>
<p><strong>逻辑通信</strong>：看起来数据似乎是沿着双向箭头在传输层水平传输的，但实际上是沿图中的虚线经多个协议层次而传输。</p>
<p><img src="/resource/68f52920e8214708ab0c8e376bc6ea86.png" alt="23779e089ace8ea8ea8e0582e0ede203.png"></p>
<p>TCP/IP 协议栈传输层有两个重要协议——UDP 和 TCP，不同的应用进程在传输层使用 TCP 或 UDP 之一。</p>
<p>这一节先介绍比较简单的 UDP，比较复杂的 TCP 将在下一节讨论。</p>
<h2 id="端口">端口</h2>
<p><strong>端口</strong>的作用体现在<strong>传输层</strong>。</p>
<p>刚才的图中，AP1 与 AP3 的通信与 AP2 与 AP4 的通信可以使用同一个<strong>传输层协议</strong>来传输(TCP 或 UDP)，根据 IP 地址或 MAC 地址都只能把数据传到正确的主机，但具体需要传到哪一个进程，是通过端口来辨认的。</p>
<p>比如同时使用浏览器和 QQ，浏览器占用 80 端口，而 QQ 占用 4000 端口，那么发送过来的 QQ 消息便会通过 4000 端口显示在 QQ 客户端，而不会错误地显示在浏览器上。</p>
<p>端口号有 0 ～ 65535 的编号，其中：</p>
<ul>
<li>编号 0 ～ 1023 为 <strong>系统端口号</strong> ，这些端口号可以在<a href="www.iana.org">IANA</a> 查询到，它们被指派给了 TCP/IP 最重要的一些应用程序，以下是一些常见的系统端口号：</li>
</ul>
<table>
<thead>
<tr>
<th>应用层协议</th>
<th>FTP</th>
<th>TELNET</th>
<th>SMTP</th>
<th>DNS</th>
<th>TFTP</th>
<th>HTTP</th>
<th>SNMP</th>
</tr>
</thead>
<tbody>
<tr>
<td>系统端口号</td>
<td>21</td>
<td>23</td>
<td>25</td>
<td>53</td>
<td>69</td>
<td>80</td>
<td>161</td>
</tr>
</tbody>
</table>
<ul>
<li>编号 1024 ～ 49151 为<strong>登记端口号</strong>，为没有系统端口号的应用程序使用，使用这类端口号必须在 <strong>IANA</strong> 按规定手续登记，以防止重复。</li>
<li>编号 49152 ～ 65535 为<strong>短暂端口号</strong>，是留给客户进程选择暂时使用的，使用结束后，这类端口号会被放开以供其它程序使用。</li>
</ul>
<h2 id="UDP-简述">UDP 简述</h2>
<p>UDP(User Datagram Protocol)用户数据报协议，它只在 IP 数据报服务之上增加了很少一点功能，它的主要特点有：</p>
<ul>
<li>UDP 是无连接的，发送数据之前不需要建立连接(而 TCP 需要)，减少了开销和时延。</li>
<li>UDP 尽最大努力交付，不保证交付可靠性。</li>
<li>UDP 是面向报文的，对于从应用层交付下来的 IP 数据报，只做很简单的封装(8 字节 UDP 报头)，首部开销小。</li>
<li>UDP 没有拥塞控制，出现网络拥塞时发送方也不会降低发送速率。这种特性对某些实时应用是很重要的，比如 IP 电话，视频会议等，它们允许拥塞时丢失一些数据，因为如果不抛弃这些数据，极可能造成时延的累积。</li>
<li>UDP 支持一对一、一对多、多对一和多对多的交互通信。</li>
</ul>
<p>从应用层到传输层，再到网络层的各层次封装：<br>
<img src="/resource/3caf822848c74f6998bfaf54844441a8.png" alt="fc8f2aeaec997a83081d3ae83b0a0ce5.png"></p>
<h2 id="UDP-报文">UDP 报文</h2>
<p>UDP 数据报可分为两部分：<strong>UDP 报头</strong>和<strong>数据部分</strong>。其中数据部分是应用层交付下来的数据。UDP 报头总共 8 字节，而这 8 字节又分为 4 个字段：<br>
<img src="/resource/0845481cd58f49f6a07569bd0e53e4df.png" alt="4f52034695fd2302f086bbd520f65fee.png"></p>
<ul>
<li>源端口：2 字节，在对方需要回信时可用，不需要时可以全 0；</li>
<li>目的端口：2 字节，必须，也是最重要的字段；</li>
<li>长度：2 字节，长度值包括报头和数据部分；</li>
<li>校验和：2 字节，用于检验 UDP 数据报在传输过程中是否有出错，有错就丢弃。</li>
</ul>
<p><strong>tcpdump 抓取 UDP 报文</strong><br>
现在我们动手实践，尝试抓取一个 UDP 数据报，并解读其内容。</p>
<p>我们需要一个小程序，用于向指定 IP 地址的指定端口发送一个 指定内容 的 UDP 数据报，这个程序已经编写好，依次输入以下命令，下载并编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://labfile.oss.aliyuncs.com/courses/98/test.c</span><br><span class="line">gcc -o <span class="built_in">test</span> test.c</span><br></pre></td></tr></table></figure>
<p>这个 C 程序会向 IP 地址 <code>192.168.1.1</code> 的 <code>7777</code> 端口发送一条 “hello” 消息。你可以用编辑器修改程序，向不同的 IP 发送不同的内容。</p>
<p>编译完成后先别运行，我们还需要使用一个知名的抓包工具 <strong>tcpdump</strong>，依次输入以下命令安装，并运行 tcpdump：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install tcpdump</span><br><span class="line">sudo tcpdump -vvv -X udp port 7777</span><br></pre></td></tr></table></figure>
<p>新开一个终端，输入以下命令运行刚才编译好的 C 程序 test：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./test</span><br></pre></td></tr></table></figure>
<p>test 程序运行结束，返回刚才运行 tcpdump 的终端查看抓包结果：<br>
<img src="/resource/43e5161379384c2c9f266af27e0ef2a0.png" alt="9f3278c6e36d20cf7702e6727101b94b.png"></p>
<p><strong>蓝色框</strong>为 16 进制目的端口，<strong>绿色框</strong>为 16 进制目的 IP，<strong>红色框</strong>为 20 字节 IP 报头，<strong>橘色下划线</strong>为 8 字节 UDP 报头，<strong>红色下划线</strong>为 hello 的 ASCII 码。</p>
<p>从 <code>4500</code> 到 <code>0101</code> 都是 IP 报头，IP 报文在之前已经讲过，这里就不赘述了。后面的部分就是 UDP 报文。</p>
<p>我们知道 UDP 报头一共 8 字节，所以从 <code>eb39</code> 到 <code>ac82</code> 是 UDP 报头的部分。</p>
<ul>
<li><code>eb39</code>：源端口，2 字节，换成十进制也就是 32830</li>
<li><code>1e61</code>：目的端口，2 字节，十进制为 7777</li>
<li><code>001c</code>：包长度，单位为字节，换为十进制可知包长度为 28 字节</li>
<li><code>ac82</code>：校验和</li>
</ul>
<p>后面的就是数据内容的 ASCII 码。</p>
]]></content>
  </entry>
  <entry>
    <title>paddle2.0高层API实现人脸关键点检测(人脸关键点检测综述_自定义网络_paddleHub_趣味ps)</title>
    <url>/2022/06/01/63e0d330af4f4b7c818e86e056b93ab8/</url>
    <content><![CDATA[<p>[toc]</p>
<blockquote>
<p>本文包含了：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- 人脸关键点检测综述</span><br><span class="line">- 人脸关键点检测数据集介绍以及数据处理实现</span><br><span class="line">- 自定义网络实现关键点检测</span><br><span class="line">- paddleHub实现关键点检测</span><br><span class="line">- 基于关键点检测的趣味ps</span><br></pre></td></tr></table></figure>
<p>『深度学习 7 日打卡营·day3』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
</ul>
<h2 id="一、问题定义">一、问题定义</h2>
<p>人脸关键点检测，是输入一张人脸图片，模型会返回人脸关键点的一系列坐标，从而定位到人脸的关键信息。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2aead1c9391422cd84d7d21a1af783ce.png" alt=""></p>
<p>人脸关键点检测是人脸识别和分析领域中的关键一步，它是诸如自动人脸识别、表情分析、三维人脸重建及三维动画等其它人脸相关问题的前提和突破口。近些年来，深度学习方法由于其自动学习及持续学习能力，已被成功应用到了图像识别与分析、语音识别和自然语言处理等很多领域，且在这些方面都带来了很显著的改善。因此，本文针对深度学习方法进行了人脸关键点检测的研究。</p>
<h2 id="人脸关键点检测深度学习方法综述">人脸关键点检测深度学习方法综述</h2>
<h3 id="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection">Deep Convolutional Network Cascade for Facial Point Detection</h3>
<p>2013 年，Sun 等人首次将 CNN 应用到人脸关键点检测，提出一种级联的 CNN(拥有三个层级)——DCNN(Deep Convolutional Network)，此种方法属于级联回归方法。作者通过精心设计拥有三个层级的级联卷积神经网络，不仅改善初始不当导致陷入局部最优的问题，而且借助于 CNN 强大的特征提取能力，获得更为精准的关键点检测。</p>
<p>如图所示，DCNN 由三个 Level 构成。Level-1 由 3 个 CNN 组成;Level-2 由 10 个 CNN 组成(每个关键点采用两个 CNN);Level-3 同样由 10 个 CNN 组成。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6b6a9873d6444caa87181de4c1ca2ed5.png" alt=""></p>
<p>DCNN 采用级联回归的思想，从粗到精的逐步得到精确的关键点位置，不仅设计了三级级联的卷积神经网络，还引入局部权值共享机制，从而提升网络的定位性能。最终在数据集 BioID 和 LFPW 上均获得当时最优结果。速度方面，采用 3.3GHz 的 CPU，每 0.12 秒检测一张图片的 5 个关键点。</p>
<h3 id="Extensive-Facial-Landmark-Localization-with-Coarse-to-fine-Convolutional-Network-Cascade">Extensive Facial Landmark Localization with Coarse-to-fine Convolutional Network Cascade</h3>
<p>2013 年，Face++在 DCNN 模型上进行改进，提出从粗到精的人脸关键点检测算法，实现了 68 个人脸关键点的高精度定位。该算法将人脸关键点分为内部关键点和轮廓关键点，内部关键点包含眉毛、眼睛、鼻子、嘴巴共计 51 个关键点，轮廓关键点包含 17 个关键点。</p>
<p>针对内部关键点和外部关键点，该算法并行的采用两个级联的 CNN 进行关键点检测，网络结构如图所示。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/01a425c3fd5887f0f787ae7967886bca.png" alt=""></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/190e43a69ba567e10d7cb2039f5a2edf.png" alt=""></p>
<p>算法主要创新点由以下三点：</p>
<ul>
<li>把人脸的关键点定位问题，划分为内部关键点和轮廓关键点分开预测，有效的避免了 loss 不均衡问题</li>
<li>在内部关键点检测部分，并未像 DCNN 那样每个关键点采用两个 CNN 进行预测，而是每个器官采用一个 CNN 进行预测，从而减少计算量</li>
<li>相比于 DCNN，没有直接采用人脸检测器返回的结果作为输入，而是增加一个边界框检测层(Level-1)，可以大大提高关键点粗定位网络的精度。</li>
</ul>
<p>Face++版 DCNN 首次利用卷积神经网络进行 68 个人脸关键点检测，针对以往人脸关键点检测受人脸检测器影响的问题，作者设计 Level-1 卷积神经网络进一步提取人脸边界框，为人脸关键点检测获得更为准确的人脸位置信息，最终在当年 300-W 挑战赛上获得领先成绩。</p>
<h3 id="TCDCN-Facial-Landmark-Detection-by-Deep-Multi-task-Learning">TCDCN-Facial Landmark Detection by Deep Multi-task Learning</h3>
<p>优点是快和多任务，不仅使用简单的端到端的人脸关键点检测方法，而且能够做到去分辨人脸的喜悦、悲伤、愤怒等分类标签属性，这样跟文章的标题或者说是文章的主题贴合——多任务。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/a2f5609870bb3b145768c4d999646943.png" alt=""></p>
<h3 id="Joint-Face-Detection-and-Alignment-using-Multi-task-Cascaded-Convolutional-Networks">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</h3>
<p>2016 年，Zhang 等人提出一种多任务级联卷积神经网络(MTCNN, Multi-task Cascaded Convolutional Networks)用以同时处理人脸检测和人脸关键点定位问题。作者认为人脸检测和人脸关键点检测两个任务之间往往存在着潜在的联系，然而大多数方法都未将两个任务有效的结合起来，本文为了充分利用两任务之间潜在的联系，提出一种多任务级联的人脸检测框架，将人脸检测和人脸关键点检测同时进行。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b0445220291b2c637beec82cfbccb870.png" alt=""></p>
<p>MTCNN 包含三个级联的多任务卷积神经网络，分别是 Proposal Network (P-Net)、Refine Network (R-Net)、Output Network (O-Net)，每个多任务卷积神经网络均有三个学习任务，分别是人脸分类、边框回归和关键点定位。网络结构如图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b6e8884f06a3925e30a4a68c29dcc66c.png" alt=""></p>
<p>MTCNN 实现人脸检测和关键点定位分为三个阶段。首先由 P-Net 获得了人脸区域的候选窗口和边界框的回归向量，并用该边界框做回归，对候选窗口进行校准，然后通过非极大值抑制(NMS)来合并高度重叠的候选框。然后将 P-Net 得出的候选框作为输入，输入到 R-Net，R-Net 同样通过边界框回归和 NMS 来去掉那些 false-positive 区域，得到更为准确的候选框;最后，利用 O-Net 输出 5 个关键点的位置。</p>
<h3 id="DAN-Deep-Alignment-Networks">DAN(Deep Alignment Networks)</h3>
<p>2017 年，Kowalski 等人提出一种新的级联深度神经网络——DAN(Deep Alignment Network)，以往级联神经网络输入的是图像的某一部分，与以往不同，DAN 各阶段网络的输入均为整张图片。当网络均采用整张图片作为输入时，DAN 可以有效的克服头部姿态以及初始化带来的问题，从而得到更好的检测效果。之所以 DAN 能将整张图片作为输入，是因为其加入了关键点热图(Landmark Heatmaps)，关键点热图的使用是本文的主要创新点。DAN 基本框架如图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/88517620128a0e38f81d5937b7b10a1e.png" alt=""></p>
<p>DAN 包含多个阶段，每一个阶段含三个输入和一个输出，输入分别是被矫正过的图片、关键点热图和由全连接层生成的特征图，输出是面部形状(Face Shape)。其中，CONNECTION LAYER 的作用是将本阶段得输出进行一系列变换，生成下一阶段所需要的三个输入</p>
<h3 id="PFLD-A-Practical-Facial-Landmark-Detector">PFLD: A Practical Facial Landmark Detector</h3>
<p>这个人脸检测算法 PFLD，全文名称为《PFLD: A Practical Facial Landmark Detector》。作者分别来自天津大学、武汉大学、腾讯 AI 实验室、美国天普大学。该算法对嵌入式设备非常优化，在骁龙 845 的芯片中效率可达 140fps；另外模型大小较小，仅 2.1MB；此外在许多关键点检测的 benchmark 中也取得了相当好的结果。综上，该算法在实际的应用场景中（如低算力的端上设备）有很大的应用空间。</p>
<h4 id="PFLD-模型设计">PFLD 模型设计</h4>
<p>在模型设计上，PFLD 的模型设计上骨干网络没有采用 VGG16、ResNet 等大模型，但是为了增加模型的表达能力，对 Mobilenet 的输出特征进行了结构上的修改。</p>
<h4 id="PFLD-的模型训练策略">PFLD 的模型训练策略</h4>
<p>一开始我们设计的那个简单的网络，采用的损失函数为 MSE，所以为了平衡各种情况的训练数据，我们只能通过增加极端情况下的训练数据、平衡各类情况下的训练数据的比例、控制数据数据的采样形式（非完全随机采样）等方式进行性能调优。</p>
<ul>
<li>
<p>损失函数设计</p>
<p>PFLD 采用了一种很优雅的方式来处理上述各情况样本不均衡的问题，我们先看看其损失函数的设计：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/54ca8b209ee1064728ff2d67770d8272.png" alt=""></p>
<p>上式中 wn 为可调控的权值函数（针对不同的情况选取不同的权值，如正常情况、遮挡情况、暗光情况等等），theta 为人脸姿态的三维欧拉角（K=3），d 为回归的 landmark 和 groundtrue 的度量（一般情况下为 MSE，也可以选 L1 度量）。该损失函数设计的目的是，对于样本量比较大的数据（如正脸，即欧拉角都相对较小的情况），给予一个小的权值，在进行梯度的反向传播的时候，对模型训练的贡献小一些；对于样本量比较少的数据（侧脸、低头、抬头、表情极端），给予一个较大的权值，从而使在进行梯度的反向传播的时候，对模型训练的贡献大一些。该模型的损失函数的设计，非常巧妙的解决了平衡各类情况训练样本不均衡的问题。</p>
</li>
<li>
<p>配合训练的子网络</p>
<p>PFLD 的训练过程中引入了一个子网络，用以监督 PFLD 网络模型的训练。该子网络仅在训练的阶段起作用，在 inference 的时候不参与；该子网络的用处，是对于每一个输入的人脸样本，对该样本进行三维欧拉角的估计，其 groundtruth 由训练数据中的关键点信息进行估计，虽然估计的不够精确，但是作为区分数据分布的依据已经足够了，毕竟还该网络的目的是监督和辅助训练收敛，主要是为了服务关键点检测网络。有一个地方挺有意思的是，该子网络的输入不是训练数据，而是 PFLD 主网络的中间输出，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0c38571527622f32d549ddeb0079f8be.png" alt=""></p>
</li>
</ul>
<p>主网络和姿态估计子网络的详细配置如下表：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/10c85cdd527c1b175e90a68fca999343.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 环境导入</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"></span><br><span class="line">paddle.set_device(<span class="string">&#x27;gpu&#x27;</span>) <span class="comment"># 设置为GPU</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>) <span class="comment"># 忽略 warning</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">paddle.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;2.0.0&#x27;</span><br></pre></td></tr></table></figure>
<h2 id="二、数据准备">二、数据准备</h2>
<p>传统人脸关键点检测数据库为室内环境下采集的数据库，比如<code>Multi-pie</code>、<code>Feret、Frgc</code>、<code>AR、BioID</code> 等人脸数据库。而现阶段人脸关键点检测数据库通常为复杂环境下采集的数据库.LFPW 人脸数据库有 1132 幅训练人脸图像和 300 幅测试人脸图像，大部分为正面人脸图像，每个人脸标定 29 个关键点。<code>AFLW</code> 人脸数据库包含 25993 幅从 <code>Flickr</code> 采集的人脸图像，每个人脸标定 21 个关键点。<code>COFW</code> 人脸数据库包含 <code>LFPW</code> 人脸数据库训练集中的 845 幅人脸图像以及其他 500 幅遮挡人脸图像，而测试集为 507 幅严重遮挡(同时包含姿态和表情的变化)的人脸图像，每个人脸标定 29 个关键点。<code>MVFW</code> 人脸数据库为多视角人脸数据集，包括 2050 幅训练人脸图像和 450 幅测试人脸图像，每个人脸标定 68 个关键点。<code>OCFW</code> 人脸数据库包含 2951 幅训练人脸图像(均为未遮挡人脸)和 1246 幅测试人脸图像(均为遮挡人脸)，每个人脸标定 68 个关键点。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/099f78bf5bee8885edb93b0b2913460a.png" alt=""></p>
<h3 id="2-1-下载数据集">2.1 下载数据集</h3>
<p>本次实验所采用的数据集来源为 github 的<a href="https://github.com/udacity/P1_Facial_Keypoints">开源项目</a></p>
<p>目前该数据集已上传到 AI Studio <a href="https://aistudio.baidu.com/aistudio/datasetdetail/69065">人脸关键点识别</a>，加载后可以直接使用下面的命令解压。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 覆盖且不显示</span></span><br><span class="line"><span class="comment"># !unzip -o -q data/data69065/data.zip</span></span><br></pre></td></tr></table></figure>
<p>解压后的数据集结构为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data/</span><br><span class="line">|—— test</span><br><span class="line">|   |—— Abdel_Aziz_Al-Hakim_00.jpg</span><br><span class="line">    ... ...</span><br><span class="line">|—— test_frames_keypoints.csv</span><br><span class="line">|—— training</span><br><span class="line">|   |—— Abdullah_Gul_10.jpg</span><br><span class="line">    ... ...</span><br><span class="line">|—— training_frames_keypoints.csv</span><br></pre></td></tr></table></figure>
<p>其中，<code>training</code> 和 <code>test</code> 文件夹分别存放训练集和测试集。<code>training_frames_keypoints.csv</code> 和 <code>test_frames_keypoints.csv</code> 存放着训练集和测试集的标签。接下来，我们先来观察一下 <code>training_frames_keypoints.csv</code> 文件，看一下训练集的标签是如何定义的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">key_pts_frame = pd.read_csv(<span class="string">&#x27;data/training_frames_keypoints.csv&#x27;</span>) <span class="comment"># 读取数据集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of images: &#x27;</span>, key_pts_frame.shape[<span class="number">0</span>]) <span class="comment"># 输出数据集大小</span></span><br><span class="line">key_pts_frame.head(<span class="number">5</span>) <span class="comment"># 看前五条数据</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Number of images:  3462</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.dataframe tbody tr th &#123;</span><br><span class="line">    vertical-align: top;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">.dataframe thead th &#123;</span><br><span class="line">    text-align: right;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>126</th>
      <th>127</th>
      <th>128</th>
      <th>129</th>
      <th>130</th>
      <th>131</th>
      <th>132</th>
      <th>133</th>
      <th>134</th>
      <th>135</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Luis_Fonsi_21.jpg</td>
      <td>45.0</td>
      <td>98.0</td>
      <td>47.0</td>
      <td>106.0</td>
      <td>49.0</td>
      <td>110.0</td>
      <td>53.0</td>
      <td>119.0</td>
      <td>56.0</td>
      <td>...</td>
      <td>83.0</td>
      <td>119.0</td>
      <td>90.0</td>
      <td>117.0</td>
      <td>83.0</td>
      <td>119.0</td>
      <td>81.0</td>
      <td>122.0</td>
      <td>77.0</td>
      <td>122.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Lincoln_Chafee_52.jpg</td>
      <td>41.0</td>
      <td>83.0</td>
      <td>43.0</td>
      <td>91.0</td>
      <td>45.0</td>
      <td>100.0</td>
      <td>47.0</td>
      <td>108.0</td>
      <td>51.0</td>
      <td>...</td>
      <td>85.0</td>
      <td>122.0</td>
      <td>94.0</td>
      <td>120.0</td>
      <td>85.0</td>
      <td>122.0</td>
      <td>83.0</td>
      <td>122.0</td>
      <td>79.0</td>
      <td>122.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Valerie_Harper_30.jpg</td>
      <td>56.0</td>
      <td>69.0</td>
      <td>56.0</td>
      <td>77.0</td>
      <td>56.0</td>
      <td>86.0</td>
      <td>56.0</td>
      <td>94.0</td>
      <td>58.0</td>
      <td>...</td>
      <td>79.0</td>
      <td>105.0</td>
      <td>86.0</td>
      <td>108.0</td>
      <td>77.0</td>
      <td>105.0</td>
      <td>75.0</td>
      <td>105.0</td>
      <td>73.0</td>
      <td>105.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Angelo_Reyes_22.jpg</td>
      <td>61.0</td>
      <td>80.0</td>
      <td>58.0</td>
      <td>95.0</td>
      <td>58.0</td>
      <td>108.0</td>
      <td>58.0</td>
      <td>120.0</td>
      <td>58.0</td>
      <td>...</td>
      <td>98.0</td>
      <td>136.0</td>
      <td>107.0</td>
      <td>139.0</td>
      <td>95.0</td>
      <td>139.0</td>
      <td>91.0</td>
      <td>139.0</td>
      <td>85.0</td>
      <td>136.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Kristen_Breitweiser_11.jpg</td>
      <td>58.0</td>
      <td>94.0</td>
      <td>58.0</td>
      <td>104.0</td>
      <td>60.0</td>
      <td>113.0</td>
      <td>62.0</td>
      <td>121.0</td>
      <td>67.0</td>
      <td>...</td>
      <td>92.0</td>
      <td>117.0</td>
      <td>103.0</td>
      <td>118.0</td>
      <td>92.0</td>
      <td>120.0</td>
      <td>88.0</td>
      <td>122.0</td>
      <td>84.0</td>
      <td>122.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 137 columns</p>
</div>
<p>上表中每一行都代表一条数据，其中，第一列是图片的文件名，之后从第 0 列到第 135 列，就是该图的关键点信息。因为每个关键点可以用两个坐标表示，所以 136/2 = 68，就可以看出这个数据集为 68 点人脸关键点数据集。</p>
<p>Tips1: 目前常用的人脸关键点标注，有如下点数的标注</p>
<ul>
<li>5 点</li>
<li>21 点</li>
<li>68 点</li>
<li>98 点</li>
</ul>
<p>Tips2：本次所采用的 68 标注，标注顺序如下：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0933bdce12e4571b6167f81b9554d59b.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算标签的均值和标准差，用于标签的归一化</span></span><br><span class="line">key_pts_values = key_pts_frame.values[:,<span class="number">1</span>:] <span class="comment"># 取出标签信息</span></span><br><span class="line">data_mean = key_pts_values.mean() <span class="comment"># 计算均值</span></span><br><span class="line">data_std = key_pts_values.std()   <span class="comment"># 计算标准差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标签的均值为:&#x27;</span>, data_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;标签的标准差为:&#x27;</span>, data_std)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">标签的均值为: 104.4724870017331</span><br><span class="line">标签的标准差为: 43.17302271754281</span><br></pre></td></tr></table></figure>
<h3 id="2-2-查看图像">2.2 查看图像</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_keypoints</span>(<span class="params">image, key_pts</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        image: 图像信息</span></span><br><span class="line"><span class="string">        key_pts: 关键点信息，</span></span><br><span class="line"><span class="string">    展示图片和关键点信息</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    plt.imshow(image.astype(<span class="string">&#x27;uint8&#x27;</span>))  <span class="comment"># 展示图片信息</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(key_pts)//<span class="number">2</span>,):</span><br><span class="line">        plt.scatter(key_pts[i*<span class="number">2</span>], key_pts[i*<span class="number">2</span>+<span class="number">1</span>], s=<span class="number">20</span>, marker=<span class="string">&#x27;.&#x27;</span>, c=<span class="string">&#x27;b&#x27;</span>)  <span class="comment"># 展示关键点信息 蓝色散点</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 展示单条数据</span></span><br><span class="line"></span><br><span class="line">n = <span class="built_in">int</span>(np.random.randint(<span class="number">1</span>, <span class="number">3462</span>, size=<span class="number">1</span>)) <span class="comment"># n为数据在表格中的索引</span></span><br><span class="line">image_name = key_pts_frame.iloc[n, <span class="number">0</span>] <span class="comment"># 获取图像名称</span></span><br><span class="line">key_pts = key_pts_frame.iloc[n, <span class="number">1</span>:].as_matrix() <span class="comment"># 将图像label格式转为numpy.array的格式</span></span><br><span class="line">key_pts = key_pts.astype(<span class="string">&#x27;float&#x27;</span>).reshape(-<span class="number">1</span>) <span class="comment"># 获取图像关键点信息</span></span><br><span class="line"><span class="built_in">print</span>(key_pts.shape)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>)) <span class="comment"># 展示的图像大小</span></span><br><span class="line">show_keypoints(mpimg.imread(os.path.join(<span class="string">&#x27;data/training/&#x27;</span>, image_name)), key_pts) <span class="comment"># 展示图像与关键点信息</span></span><br><span class="line">plt.show() <span class="comment"># 展示图像</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(136,)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024056525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="2-3-数据集定义-2">2.3 数据集定义</h3>
<p>使用飞桨框架高层 API 的 <code>paddle.io.Dataset</code> 自定义数据集类，具体可以参考官网文档 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/02_paddle2.0_develop/02_data_load_cn.html#id3">自定义数据集</a>。</p>
<h3 id="作业-1：自定义-Dataset，完成人脸关键点数据集定义">作业 1：自定义 Dataset，完成人脸关键点数据集定义</h3>
<p>按照 <code>__init__</code> 中的定义，实现 <code>__getitem__</code> 和 <code>__len__</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按照Dataset的使用规范，构建人脸关键点数据集</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FacialKeypointsDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># 人脸关键点数据集</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    步骤一：继承paddle.io.Dataset类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, root_dir, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        步骤二：实现构造函数，定义数据集大小</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): 带标注的csv文件路径</span></span><br><span class="line"><span class="string">            root_dir (string): 图片存储的文件夹路径</span></span><br><span class="line"><span class="string">            transform (callable, optional): 应用于图像上的数据处理方法</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.key_pts_frame = pd.read_csv(csv_file) <span class="comment"># 读取csv文件</span></span><br><span class="line">        self.root_dir = root_dir <span class="comment"># 获取图片文件夹路径</span></span><br><span class="line">        self.transform = transform <span class="comment"># 获取 transform 方法</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现 __getitem__</span></span><br><span class="line">        image_dir = os.path.join(self.root_dir, self.key_pts_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 读取图像</span></span><br><span class="line">        image = mpimg.imread(image_dir)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去除图像\alpha通道</span></span><br><span class="line">        <span class="keyword">if</span> image.shape[-<span class="number">1</span>] == <span class="number">4</span>:</span><br><span class="line">            image = image[..., <span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取关键点</span></span><br><span class="line">        key_pts = self.key_pts_frame.iloc[idx, <span class="number">1</span>:].as_matrix()  <span class="comment"># 不读取name</span></span><br><span class="line">        key_pts = key_pts.astype(<span class="string">&#x27;float&#x27;</span>).reshape(-<span class="number">1</span>)  <span class="comment"># [136, 1]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据增强</span></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image, key_pts = self.transform([image, key_pts])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_numpy</span></span><br><span class="line">        image = np.array(image, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        key_pts = np.array(key_pts, dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image, key_pts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        步骤四：实现__len__方法，返回数据集总数目</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现 __len__</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.key_pts_frame)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-训练集可视化">2.4 训练集可视化</h3>
<p>实例化数据集并显示一些图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建一个数据集类</span></span><br><span class="line">face_dataset = FacialKeypointsDataset(csv_file=<span class="string">&#x27;data/training_frames_keypoints.csv&#x27;</span>,</span><br><span class="line">                                      root_dir=<span class="string">&#x27;data/training/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出数据集大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据集大小为: &#x27;</span>, <span class="built_in">len</span>(face_dataset))</span><br><span class="line"><span class="comment"># 根据 face_dataset 可视化数据集</span></span><br><span class="line">num_to_display = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_to_display):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义图片大小</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机选择图片</span></span><br><span class="line">    rand_i = np.random.randint(<span class="number">0</span>, <span class="built_in">len</span>(face_dataset))</span><br><span class="line">    sample = face_dataset[rand_i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出图片大小和关键点的数量</span></span><br><span class="line">    <span class="built_in">print</span>(i, sample[<span class="number">0</span>].shape, sample[<span class="number">1</span>].shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置图片打印信息</span></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, num_to_display, i + <span class="number">1</span>)</span><br><span class="line">    ax.set_title(<span class="string">&#x27;Sample #&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出图片</span></span><br><span class="line">    show_keypoints(sample[<span class="number">0</span>], sample[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">数据集大小为:  3462</span><br><span class="line">0 (300, 250, 3) (136,)</span><br><span class="line">1 (140, 142, 3) (136,)</span><br><span class="line">2 (208, 174, 3) (136,)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024109247.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210206024112700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210206024116618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上述代码虽然完成了数据集的定义，但是还有一些问题，如：</p>
<ul>
<li>每张图像的大小不一样，图像大小需要统一以适配网络输入要求</li>
<li>图像格式需要适配模型的格式输入要求</li>
<li>数据量比较小，没有进行数据增强</li>
</ul>
<p>这些问题都会影响模型最终的性能，所以需要对数据进行预处理。</p>
<h3 id="2-5-Transforms">2.5 Transforms</h3>
<p>对图像进行预处理，包括灰度化、归一化、重新设置尺寸、随机裁剪，修改通道格式等等，以满足数据要求；每一类的功能如下：</p>
<ul>
<li>灰度化：丢弃颜色信息，保留图像边缘信息；识别算法对于颜色的依赖性不强，加上颜色后鲁棒性会下降，而且灰度化图像维度下降（3-&gt;1），保留梯度的同时会加快计算。</li>
<li>归一化：加快收敛</li>
<li>重新设置尺寸：数据增强</li>
<li>随机裁剪：数据增强</li>
<li>修改通道格式：改为模型需要的结构</li>
</ul>
<h3 id="作业-2：实现自定义-ToCHW">作业 2：实现自定义 ToCHW</h3>
<p>实现数据预处理方法 ToCHW</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 标准化自定义 transform 方法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformAPI</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    步骤一：继承 object 类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        步骤二：在 __call__ 中定义数据处理方法</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        processed_data = data</span><br><span class="line">        <span class="keyword">return</span>  processed_data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.vision.transforms.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GrayNormalize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 将图片变为灰度图，并将其值放缩到[0, 1]</span></span><br><span class="line">    <span class="comment"># 将 label 放缩到 [-1, 1] 之间</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        image = data[<span class="number">0</span>]   <span class="comment"># 获取图片</span></span><br><span class="line">        key_pts = data[<span class="number">1</span>] <span class="comment"># 获取标签</span></span><br><span class="line"></span><br><span class="line">        image_copy = np.copy(image)</span><br><span class="line">        key_pts_copy = np.copy(key_pts)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 灰度化图片</span></span><br><span class="line">        gray_scale = paddle.vision.transforms.Grayscale(num_output_channels=<span class="number">3</span>)</span><br><span class="line">        image_copy = gray_scale(image_copy)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将图片值放缩到 [0, 1]</span></span><br><span class="line">        image_copy = image_copy / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将坐标点放缩到 [-1, 1]</span></span><br><span class="line">        mean = data_mean <span class="comment"># 获取标签均值</span></span><br><span class="line">        std = data_std   <span class="comment"># 获取标签标准差</span></span><br><span class="line">        key_pts_copy = (key_pts_copy - mean)/std</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_copy, key_pts_copy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Resize</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 将输入图像调整为指定大小</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, output_size</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(output_size, (<span class="built_in">int</span>, <span class="built_in">tuple</span>))</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line"></span><br><span class="line">        image = data[<span class="number">0</span>]    <span class="comment"># 获取图片</span></span><br><span class="line">        key_pts = data[<span class="number">1</span>]  <span class="comment"># 获取标签</span></span><br><span class="line"></span><br><span class="line">        image_copy = np.copy(image)</span><br><span class="line">        key_pts_copy = np.copy(key_pts)</span><br><span class="line"></span><br><span class="line">        h, w = image_copy.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self.output_size, <span class="built_in">int</span>):</span><br><span class="line">            <span class="keyword">if</span> h &gt; w:</span><br><span class="line">                new_h, new_w = self.output_size * h / w, self.output_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_h, new_w = self.output_size, self.output_size * w / h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        new_h, new_w = <span class="built_in">int</span>(new_h), <span class="built_in">int</span>(new_w)</span><br><span class="line"></span><br><span class="line">        img = F.resize(image_copy, (new_h, new_w))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># scale the pts, too</span></span><br><span class="line">        key_pts_copy[::<span class="number">2</span>] = key_pts_copy[::<span class="number">2</span>] * new_w / w</span><br><span class="line">        key_pts_copy[<span class="number">1</span>::<span class="number">2</span>] = key_pts_copy[<span class="number">1</span>::<span class="number">2</span>] * new_h / h</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, key_pts_copy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomCrop</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 随机位置裁剪输入的图像</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, output_size</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(output_size, (<span class="built_in">int</span>, <span class="built_in">tuple</span>))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(output_size, <span class="built_in">int</span>):</span><br><span class="line">            self.output_size = (output_size, output_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(output_size) == <span class="number">2</span></span><br><span class="line">            self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        image = data[<span class="number">0</span>]</span><br><span class="line">        key_pts = data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        image_copy = np.copy(image)</span><br><span class="line">        key_pts_copy = np.copy(key_pts)</span><br><span class="line"></span><br><span class="line">        h, w = image_copy.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        top = np.random.randint(<span class="number">0</span>, h - new_h)</span><br><span class="line">        left = np.random.randint(<span class="number">0</span>, w - new_w)</span><br><span class="line"></span><br><span class="line">        image_copy = image_copy[top: top + new_h,</span><br><span class="line">                      left: left + new_w]</span><br><span class="line"></span><br><span class="line">        key_pts_copy[::<span class="number">2</span>] = key_pts_copy[::<span class="number">2</span>] - left</span><br><span class="line">        key_pts_copy[<span class="number">1</span>::<span class="number">2</span>] = key_pts_copy[<span class="number">1</span>::<span class="number">2</span>] - top</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_copy, key_pts_copy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ToCHW</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 将图像的格式由HWC改为CHW</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现ToCHW，可以使用 paddle.vision.transforms.Transpose 实现</span></span><br><span class="line">        image = data[<span class="number">0</span>]</span><br><span class="line">        key_pts = data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        transpose = paddle.vision.transforms.Transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># [h w c] -&gt; [c h w]</span></span><br><span class="line">        image = transpose(image)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image, key_pts</span><br></pre></td></tr></table></figure>
<p>看一下每种图像预处理方法的的效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 Resize</span></span><br><span class="line">resize = Resize(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 RandomCrop</span></span><br><span class="line">random_crop = RandomCrop(<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 GrayNormalize</span></span><br><span class="line">norm = GrayNormalize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试 Resize + RandomCrop，图像大小变到250*250， 然后截取出224*224的图像块</span></span><br><span class="line">composed = paddle.vision.transforms.Compose([Resize(<span class="number">250</span>), RandomCrop(<span class="number">224</span>)])</span><br><span class="line"></span><br><span class="line">test_num = <span class="number">800</span> <span class="comment"># 测试的数据下标</span></span><br><span class="line">data = face_dataset[test_num]</span><br><span class="line"></span><br><span class="line">transforms = &#123;<span class="string">&#x27;None&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">              <span class="string">&#x27;norm&#x27;</span>: norm,</span><br><span class="line">              <span class="string">&#x27;random_crop&#x27;</span>: random_crop,</span><br><span class="line">              <span class="string">&#x27;resize&#x27;</span>: resize,</span><br><span class="line">              <span class="string">&#x27;composed&#x27;</span>: composed&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, func_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(transforms):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义图片大小</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">40</span>, <span class="number">40</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理图片</span></span><br><span class="line">    <span class="keyword">if</span> transforms[func_name] != <span class="literal">None</span>:</span><br><span class="line">        transformed_sample = transforms[func_name](data)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        transformed_sample = data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置图片打印信息</span></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">5</span>, i + <span class="number">1</span>)</span><br><span class="line">    ax.set_title(<span class="string">&#x27; Transform is #&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func_name))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出图片</span></span><br><span class="line">    show_keypoints(transformed_sample[<span class="number">0</span>], transformed_sample[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024137350.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210206024141275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210206024144318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210206024147748.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210206024150688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="2-6-使用数据预处理的方式完成数据定义">2.6 使用数据预处理的方式完成数据定义</h3>
<p>让我们将 <code>Resize、RandomCrop、GrayNormalize、ToCHW</code> 应用于新的数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> Compose</span><br><span class="line"></span><br><span class="line">data_transform = Compose([Resize(<span class="number">256</span>), RandomCrop(<span class="number">224</span>), GrayNormalize(), ToCHW()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the transformed dataset</span></span><br><span class="line">train_dataset = FacialKeypointsDataset(csv_file=<span class="string">&#x27;data/training_frames_keypoints.csv&#x27;</span>,</span><br><span class="line">                                       root_dir=<span class="string">&#x27;data/training/&#x27;</span>,</span><br><span class="line">                                       transform=data_transform)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of train dataset images: &#x27;</span>, <span class="built_in">len</span>(train_dataset))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    sample = train_dataset[i]</span><br><span class="line">    <span class="built_in">print</span>(i, sample[<span class="number">0</span>].shape, sample[<span class="number">1</span>].shape)</span><br><span class="line"></span><br><span class="line">test_dataset = FacialKeypointsDataset(csv_file=<span class="string">&#x27;data/test_frames_keypoints.csv&#x27;</span>,</span><br><span class="line">                                      root_dir=<span class="string">&#x27;data/test/&#x27;</span>,</span><br><span class="line">                                      transform=data_transform)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of test dataset images: &#x27;</span>, <span class="built_in">len</span>(test_dataset))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Number of train dataset images:  3462</span><br><span class="line">0 (3, 224, 224) (136,)</span><br><span class="line">1 (3, 224, 224) (136,)</span><br><span class="line">Number of test dataset images:  770</span><br></pre></td></tr></table></figure>
<h2 id="3、模型组建">3、模型组建</h2>
<h3 id="3-1-组网可以很简单">3.1 组网可以很简单</h3>
<p>根据前文的分析可知，人脸关键点检测和分类，可以使用同样的网络结构，如 LeNet、Resnet50 等完成特征的提取，只是在原来的基础上，需要修改模型的最后部分，将输出调整为 人脸关键点的数量*2，即每个人脸关键点的横坐标与纵坐标，就可以完成人脸关键点检测任务了，具体可以见下面的代码，也可以参考官网案例:<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/cv_case/landmark_detection/landmark_detection.html">人脸关键点检测</a></p>
<p>网络结构如下：<br>
<img src="https://img-blog.csdnimg.cn/img_convert/90300cac29d44951b09d4ff8cf468a23.png" alt=""></p>
<h3 id="作业-3：根据上图，实现网络结构">作业 3：根据上图，实现网络结构</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> paddle.vision.models <span class="keyword">import</span> resnet50</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequential继承</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(paddle.nn.Sequential):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现 __init__</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_pts</span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, self).__init__(</span><br><span class="line">            paddle.vision.models.resnet50(pretrained=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1000</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, key_pts*<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="3-2-网络结构可视化">3.2 网络结构可视化</h3>
<p>使用<code>model.summary</code>可视化网络结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = paddle.Model(SimpleNet(key_pts=<span class="number">68</span>))</span><br><span class="line">model.summary((-<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">   Layer (type)         Input Shape          Output Shape         Param #</span><br><span class="line">===============================================================================</span><br><span class="line">    Conv2D-107       [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408</span><br><span class="line">  BatchNorm2D-107   [[1, 64, 112, 112]]   [1, 64, 112, 112]         256</span><br><span class="line">      ReLU-37       [[1, 64, 112, 112]]   [1, 64, 112, 112]          0</span><br><span class="line">    MaxPool2D-3     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0</span><br><span class="line">    Conv2D-109       [[1, 64, 56, 56]]     [1, 64, 56, 56]         4,096</span><br><span class="line">  BatchNorm2D-109    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-38        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-110       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-110    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">    Conv2D-111       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-111    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">    Conv2D-108       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-108    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">BottleneckBlock-33   [[1, 64, 56, 56]]     [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-112       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384</span><br><span class="line">  BatchNorm2D-112    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-39        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-113       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-113    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">    Conv2D-114       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-114    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">BottleneckBlock-34   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-115       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384</span><br><span class="line">  BatchNorm2D-115    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-40        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-116       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-116    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">    Conv2D-117       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-117    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">BottleneckBlock-35   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-119       [[1, 256, 56, 56]]    [1, 128, 56, 56]       32,768</span><br><span class="line">  BatchNorm2D-119    [[1, 128, 56, 56]]    [1, 128, 56, 56]         512</span><br><span class="line">      ReLU-41        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-120       [[1, 128, 56, 56]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-120    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-121       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-121    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">    Conv2D-118       [[1, 256, 56, 56]]    [1, 512, 28, 28]       131,072</span><br><span class="line">  BatchNorm2D-118    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-36   [[1, 256, 56, 56]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-122       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-122    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-42        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-123       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-123    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-124       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-124    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-37   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-125       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-125    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-43        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-126       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-126    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-127       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-127    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-38   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-128       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-128    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-44        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-129       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-129    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-130       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-130    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-39   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-132       [[1, 512, 28, 28]]    [1, 256, 28, 28]       131,072</span><br><span class="line">  BatchNorm2D-132    [[1, 256, 28, 28]]    [1, 256, 28, 28]        1,024</span><br><span class="line">      ReLU-45       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-133       [[1, 256, 28, 28]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-133    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-134       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-134   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">    Conv2D-131       [[1, 512, 28, 28]]   [1, 1024, 14, 14]       524,288</span><br><span class="line">  BatchNorm2D-131   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-40   [[1, 512, 28, 28]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-135      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-135    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-46       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-136       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-136    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-137       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-137   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-41  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-138      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-138    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-47       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-139       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-139    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-140       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-140   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-42  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-141      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-141    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-48       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-142       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-142    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-143       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-143   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-43  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-144      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-144    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-49       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-145       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-145    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-146       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-146   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-44  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-147      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-147    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-50       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-148       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-148    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-149       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-149   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-45  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-151      [[1, 1024, 14, 14]]    [1, 512, 14, 14]       524,288</span><br><span class="line">  BatchNorm2D-151    [[1, 512, 14, 14]]    [1, 512, 14, 14]        2,048</span><br><span class="line">      ReLU-51        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-152       [[1, 512, 14, 14]]     [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-152     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">    Conv2D-153        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-153    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">    Conv2D-150      [[1, 1024, 14, 14]]    [1, 2048, 7, 7]       2,097,152</span><br><span class="line">  BatchNorm2D-150    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-46  [[1, 1024, 14, 14]]    [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-154       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-154     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">      ReLU-52        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-155        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-155     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">    Conv2D-156        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-156    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-47   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-157       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-157     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">      ReLU-53        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-158        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-158     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">    Conv2D-159        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-159    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-48   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">AdaptiveAvgPool2D-3  [[1, 2048, 7, 7]]     [1, 2048, 1, 1]           0</span><br><span class="line">     Linear-7           [[1, 2048]]           [1, 1000]          2,049,000</span><br><span class="line">     ResNet-3        [[1, 3, 224, 224]]       [1, 1000]              0</span><br><span class="line">     Linear-8           [[1, 1000]]            [1, 512]           512,512</span><br><span class="line">      ReLU-54            [[1, 512]]            [1, 512]              0</span><br><span class="line">     Linear-9            [[1, 512]]            [1, 136]           69,768</span><br><span class="line">===============================================================================</span><br><span class="line">Total params: 26,192,432</span><br><span class="line">Trainable params: 26,086,192</span><br><span class="line">Non-trainable params: 106,240</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.57</span><br><span class="line">Forward/backward pass size (MB): 261.50</span><br><span class="line">Params size (MB): 99.92</span><br><span class="line">Estimated Total Size (MB): 361.99</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&#x27;total_params&#x27;: 26192432, &#x27;trainable_params&#x27;: 26086192&#125;</span><br></pre></td></tr></table></figure>
<h2 id="四、模型训练">四、模型训练</h2>
<h3 id="4-1-模型配置">4.1 模型配置</h3>
<p>训练模型前，需要设置训练模型所需的优化器，损失函数和评估指标。</p>
<ul>
<li>优化器：Adam 优化器，快速收敛。</li>
<li>损失函数：SmoothL1Loss</li>
<li>评估指标：NME</li>
</ul>
<h3 id="4-2-自定义评估指标">4.2 自定义评估指标</h3>
<p>特定任务的 Metric 计算方式在框架既有的 Metric 接口中不存在，或算法不符合自己的需求，那么需要我们自己来进行 Metric 的自定义。这里介绍如何进行 Metric 的自定义操作，更多信息可以参考官网文档<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/02_paddle2.0_develop/07_customize_cn.html#metric">自定义 Metric</a>；首先来看下面的代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> paddle.metric <span class="keyword">import</span> Metric</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NME</span>(<span class="title class_ inherited__">Metric</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    1. 继承paddle.metric.Metric</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="string">&#x27;nme&#x27;</span>, *args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        2. 构造函数实现，自定义参数即可</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NME, self).__init__(*args, **kwargs)</span><br><span class="line">        self._name = name</span><br><span class="line">        self.rmse = <span class="number">0</span></span><br><span class="line">        self.sample_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">name</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        3. 实现name方法，返回定义的评估指标名字</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self._name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, preds, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        4. 实现update方法，用于单个batch训练时进行评估指标计算。</span></span><br><span class="line"><span class="string">        - 当`compute`类函数未实现时，会将模型的计算输出和标签数据的展平作为`update`的参数传入。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        N = preds.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        preds = preds.reshape((N, -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        labels = labels.reshape((N, -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        self.rmse = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            pts_pred, pts_gt = preds[i, ], labels[i, ]</span><br><span class="line">            interocular = np.linalg.norm(pts_gt[<span class="number">36</span>, ] - pts_gt[<span class="number">45</span>, ])</span><br><span class="line"></span><br><span class="line">            self.rmse += np.<span class="built_in">sum</span>(np.linalg.norm(pts_pred - pts_gt, axis=<span class="number">1</span>)) / (interocular * preds.shape[<span class="number">1</span>])</span><br><span class="line">            self.sample_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rmse / N</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accumulate</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        5. 实现accumulate方法，返回历史batch训练积累后计算得到的评价指标值。</span></span><br><span class="line"><span class="string">        每次`update`调用时进行数据积累，`accumulate`计算时对积累的所有数据进行计算并返回。</span></span><br><span class="line"><span class="string">        结算结果会在`fit`接口的训练日志中呈现。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.rmse / self.sample_num</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        6. 实现reset方法，每个Epoch结束后进行评估指标的重置，这样下个Epoch可以重新进行计算。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.rmse = <span class="number">0</span></span><br><span class="line">        self.sample_num = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="作业-4：实现模型的配置和训练">作业 4：实现模型的配置和训练</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 paddle.Model 封装模型</span></span><br><span class="line">model = paddle.Model(SimpleNet(key_pts=<span class="number">68</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置模型</span></span><br><span class="line">model.prepare(optimizer=paddle.optimizer.Adam(learning_rate=<span class="number">0.001</span>, weight_decay=<span class="number">5e-4</span>, parameters=model.parameters()),</span><br><span class="line">              loss=paddle.nn.SmoothL1Loss(),</span><br><span class="line">              metrics=NME())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练可视化VisualDL工具的回调函数</span></span><br><span class="line">visualdl = paddle.callbacks.VisualDL(log_dir=<span class="string">&#x27;visualdl_log&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          test_dataset,</span><br><span class="line">          epochs=<span class="number">50</span>,</span><br><span class="line">          batch_size=<span class="number">64</span>,</span><br><span class="line">          shuffle=<span class="literal">True</span>,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          save_freq=<span class="number">10</span>,</span><br><span class="line">          save_dir=<span class="string">&#x27;./checkpoints&#x27;</span>,</span><br><span class="line">          callbacks=[visualdl])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0314 - nme: 3.5256e-04 - 519ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1009 - nme: 8.2678e-04 - 392ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 2/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0243 - nme: 3.5679e-04 - 515ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1283 - nme: 9.6968e-04 - 384ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 3/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0842 - nme: 5.4441e-04 - 516ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1667 - nme: 0.0011 - 384ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 4/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0288 - nme: 3.6530e-04 - 508ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0784 - nme: 6.5352e-04 - 386ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 5/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0151 - nme: 2.4205e-04 - 515ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1389 - nme: 9.6945e-04 - 395ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 6/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0566 - nme: 5.3502e-04 - 515ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0740 - nme: 7.7999e-04 - 387ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 7/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0764 - nme: 5.0105e-04 - 527ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1483 - nme: 0.0010 - 394ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 8/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0620 - nme: 5.2153e-04 - 517ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0452 - nme: 5.5858e-04 - 388ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 9/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0319 - nme: 3.1529e-04 - 548ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0614 - nme: 5.9176e-04 - 389ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 10/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0278 - nme: 3.3610e-04 - 535ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0530 - nme: 5.8935e-04 - 395ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 11/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0287 - nme: 3.5150e-04 - 539ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/10</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0630 - nme: 6.0881e-04 - 391ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 12/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0568 - nme: 5.7893e-04 - 519ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1034 - nme: 7.6108e-04 - 391ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 13/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0217 - nme: 3.2146e-04 - 516ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0812 - nme: 7.3246e-04 - 390ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 14/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0303 - nme: 4.3396e-04 - 519ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0300 - nme: 4.4063e-04 - 390ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 15/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0206 - nme: 3.6483e-04 - 512ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1166 - nme: 8.2566e-04 - 384ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 16/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0557 - nme: 4.9904e-04 - 518ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0294 - nme: 4.3339e-04 - 404ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 17/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0132 - nme: 2.2774e-04 - 547ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0422 - nme: 4.7383e-04 - 389ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 18/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0101 - nme: 2.0102e-04 - 526ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0657 - nme: 6.2621e-04 - 401ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 19/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0243 - nme: 3.7471e-04 - 523ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0342 - nme: 4.0499e-04 - 388ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 20/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0149 - nme: 2.2458e-04 - 518ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0324 - nme: 4.5619e-04 - 389ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 21/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0239 - nme: 3.0768e-04 - 522ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/20</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0590 - nme: 6.0212e-04 - 398ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 22/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0136 - nme: 2.5585e-04 - 516ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0358 - nme: 4.5645e-04 - 394ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 23/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0138 - nme: 2.3678e-04 - 517ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0431 - nme: 4.7841e-04 - 403ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 24/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0118 - nme: 2.5309e-04 - 543ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0551 - nme: 5.4791e-04 - 394ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 25/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0194 - nme: 2.6883e-04 - 523ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0606 - nme: 6.2750e-04 - 395ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 26/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.1156 - nme: 8.1230e-04 - 507ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0253 - nme: 4.3194e-04 - 398ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 27/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0211 - nme: 2.9135e-04 - 524ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0242 - nme: 3.5315e-04 - 724ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 28/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0257 - nme: 3.4604e-04 - 527ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0165 - nme: 3.0425e-04 - 389ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 29/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0275 - nme: 3.3530e-04 - 571ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0216 - nme: 3.7149e-04 - 394ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 30/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0183 - nme: 2.9012e-04 - 513ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0315 - nme: 4.3564e-04 - 393ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 31/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0163 - nme: 2.7395e-04 - 510ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/30</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.1089 - nme: 8.2030e-04 - 400ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 32/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0181 - nme: 2.3484e-04 - 516ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0211 - nme: 3.9242e-04 - 401ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 33/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0474 - nme: 5.8619e-04 - 531ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0372 - nme: 4.5803e-04 - 395ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 34/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0225 - nme: 3.2990e-04 - 512ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0412 - nme: 4.8774e-04 - 392ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 35/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0155 - nme: 2.7035e-04 - 517ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0372 - nme: 5.1617e-04 - 396ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 36/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0179 - nme: 3.2274e-04 - 520ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0186 - nme: 3.1128e-04 - 396ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 37/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0360 - nme: 4.2556e-04 - 581ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0138 - nme: 2.6913e-04 - 389ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 38/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0098 - nme: 2.0373e-04 - 523ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0332 - nme: 5.0381e-04 - 390ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 39/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0453 - nme: 4.6348e-04 - 518ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0307 - nme: 4.7557e-04 - 392ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 40/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0291 - nme: 4.4760e-04 - 528ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0161 - nme: 2.9352e-04 - 395ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 41/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0886 - nme: 6.5529e-04 - 562ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/40</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0242 - nme: 3.5008e-04 - 392ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 42/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0636 - nme: 5.7302e-04 - 553ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0232 - nme: 3.2434e-04 - 393ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 43/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0363 - nme: 4.2757e-04 - 521ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0276 - nme: 4.5470e-04 - 383ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 44/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0244 - nme: 3.6430e-04 - 522ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0246 - nme: 3.7649e-04 - 574ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 45/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0382 - nme: 4.3469e-04 - 521ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0280 - nme: 4.2459e-04 - 409ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 46/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0487 - nme: 4.3741e-04 - 523ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0316 - nme: 4.8172e-04 - 390ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 47/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0146 - nme: 2.3825e-04 - 515ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0136 - nme: 2.7338e-04 - 391ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 48/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0224 - nme: 2.9963e-04 - 526ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0150 - nme: 2.8412e-04 - 394ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 49/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0139 - nme: 2.3343e-04 - 514ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0323 - nme: 4.2434e-04 - 388ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">Epoch 50/50</span><br><span class="line">step 55/55 [==============================] - loss: 0.0171 - nme: 2.8570e-04 - 515ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 13/13 [==============================] - loss: 0.0187 - nme: 3.2592e-04 - 385ms/step</span><br><span class="line">Eval samples: 770</span><br><span class="line">save checkpoint at /home/aistudio/checkpoints/final</span><br></pre></td></tr></table></figure>
<p><strong>损失函数的选择</strong>：L1Loss、L2Loss、SmoothL1Loss 的对比</p>
<ul>
<li>L1Loss: 在训练后期，预测值与 ground-truth 差异较小时， 损失对预测值的导数的绝对值仍然为 1，此时如果学习率不变，损失函数将在稳定值附近波动，难以继续收敛达到更高精度。</li>
<li>L2Loss: 在训练初期，预测值与 ground-truth 差异较大时，损失函数对预测值的梯度十分大，导致训练不稳定。</li>
<li>SmoothL1Loss: 在 x 较小时，对 x 梯度也会变小，而在 x 很大时，对 x 的梯度的绝对值达到上限 1，也不会太大以至于破坏网络参数。</li>
</ul>
<h3 id="模型保存">模型保存</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">checkpoints_path = <span class="string">&#x27;./output/models&#x27;</span></span><br><span class="line">model.save(checkpoints_path, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="五、模型预测">五、模型预测</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义功能函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_all_keypoints</span>(<span class="params">image, predicted_key_pts</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    展示图像，预测关键点</span></span><br><span class="line"><span class="string">    Args：</span></span><br><span class="line"><span class="string">        image：裁剪后的图像 [224, 224, 3]</span></span><br><span class="line"><span class="string">        predicted_key_pts: 预测关键点的坐标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 展示图像</span></span><br><span class="line">    plt.imshow(image.astype(<span class="string">&#x27;uint8&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 展示关键点</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(predicted_key_pts), <span class="number">2</span>):</span><br><span class="line">        plt.scatter(predicted_key_pts[i], predicted_key_pts[i+<span class="number">1</span>], s=<span class="number">20</span>, marker=<span class="string">&#x27;.&#x27;</span>, c=<span class="string">&#x27;m&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_output</span>(<span class="params">test_images, test_outputs, batch_size=<span class="number">1</span>, h=<span class="number">20</span>, w=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    展示图像，预测关键点</span></span><br><span class="line"><span class="string">    Args：</span></span><br><span class="line"><span class="string">        test_images：裁剪后的图像 [224, 224, 3]</span></span><br><span class="line"><span class="string">        test_outputs: 模型的输出</span></span><br><span class="line"><span class="string">        batch_size: 批大小</span></span><br><span class="line"><span class="string">        h: 展示的图像高</span></span><br><span class="line"><span class="string">        w: 展示的图像宽</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(test_images.shape) == <span class="number">3</span>:</span><br><span class="line">        test_images = np.array([test_images])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line"></span><br><span class="line">        plt.figure(figsize=(h, w))</span><br><span class="line">        ax = plt.subplot(<span class="number">1</span>, batch_size, i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机裁剪后的图像</span></span><br><span class="line">        image = test_images[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型的输出，未还原的预测关键点坐标值</span></span><br><span class="line">        predicted_key_pts = test_outputs[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 还原后的真实的关键点坐标值</span></span><br><span class="line">        predicted_key_pts = predicted_key_pts * data_std + data_mean</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 展示图像和关键点</span></span><br><span class="line">        show_all_keypoints(np.squeeze(image), predicted_key_pts)</span><br><span class="line"></span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取图像</span></span><br><span class="line">img = mpimg.imread(<span class="string">&#x27;./test.jpeg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关键点占位符</span></span><br><span class="line">kpt = np.ones((<span class="number">136</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># data_transform = Compose([Resize(256), RandomCrop(224), GrayNormalize(), ToCHW()])</span></span><br><span class="line">transform = Compose([Resize(<span class="number">256</span>), RandomCrop(<span class="number">224</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图像先重新定义大小，并裁剪到 224*224的大小</span></span><br><span class="line">rgb_img, kpt = transform([img, kpt])</span><br><span class="line"></span><br><span class="line">norm = GrayNormalize()</span><br><span class="line">to_chw = ToCHW()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图像进行归一化和格式变换</span></span><br><span class="line">img, kpt = norm([rgb_img, kpt])</span><br><span class="line">img, kpt = to_chw([img, kpt])</span><br><span class="line"></span><br><span class="line">img = np.array([img], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载保存好的模型进行预测</span></span><br><span class="line">model = paddle.Model(SimpleNet(key_pts=<span class="number">68</span>))</span><br><span class="line">model.load(checkpoints_path)</span><br><span class="line">model.prepare()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果</span></span><br><span class="line">out = model.predict_batch([img])</span><br><span class="line">out = out[<span class="number">0</span>].reshape((out[<span class="number">0</span>].shape[<span class="number">0</span>], <span class="number">136</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">visualize_output(rgb_img, out, batch_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(847, 700, 3)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024221450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="使用-PaddleHub-进行测试">使用 PaddleHub 进行测试</h2>
<p>便捷地获取 PaddlePaddle 生态下的预训练模型，完成模型的管理和一键预测。配合使用 Fine-tune API，可以基于大规模预训练模型快速完成迁移学习，让预训练模型能更好地服务于用户特定场景的应用</p>
<p><code>图像 - 关键点检测</code>-&gt;<code>face_landmark_localization</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!hub install face_landmark_localization==<span class="number">1.0</span><span class="number">.2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py:3: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  from collections import MutableMapping</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  from collections import Iterable, Mapping</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  from collections import Sized</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly</span><br><span class="line">  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: &quot;&quot;</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from &#x27;collections&#x27; instead of from &#x27;collections.abc&#x27; is deprecated, and in 3.8 it will stop working</span><br><span class="line">  from collections import Sequence, defaultdict</span><br><span class="line">Downloading face_landmark_localization</span><br><span class="line">[==================================================] 100.00%</span><br><span class="line">Uncompress /home/aistudio/.paddlehub/tmp/tmp2uffqr16/face_landmark_localization</span><br><span class="line">[==================================================] 100.00%</span><br><span class="line">Successfully installed face_landmark_localization-1.0.2</span><br><span class="line">[0m</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddlehub <span class="keyword">as</span> hub</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">face_landmark = hub.Module(name=<span class="string">&quot;face_landmark_localization&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace face detection module to speed up predictions but reduce performance</span></span><br><span class="line"><span class="comment"># face_landmark.set_face_detector_module(hub.Module(name=&quot;ultra_light_fast_generic_face_detector_1mb_320&quot;))</span></span><br><span class="line"></span><br><span class="line">result = face_landmark.keypoint_detection(images=[cv2.imread(<span class="string">&#x27;./test.jpeg&#x27;</span>)])</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># result = face_landmark.keypoint_detection(paths=[&#x27;/PATH/TO/IMAGE&#x27;])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[2021-02-06 01:30:35,589] [    INFO] - Installing face_landmark_localization module</span><br><span class="line">[2021-02-06 01:30:35,704] [    INFO] - Module face_landmark_localization already installed in /home/aistudio/.paddlehub/modules/face_landmark_localization</span><br><span class="line">[2021-02-06 01:30:35,707] [    INFO] - Installing ultra_light_fast_generic_face_detector_1mb_640 module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Downloading ultra_light_fast_generic_face_detector_1mb_640</span><br><span class="line">[==================================================] 100.00%</span><br><span class="line">Uncompress /home/aistudio/.paddlehub/tmp/tmp_7n8dm48/ultra_light_fast_generic_face_detector_1mb_640</span><br><span class="line">[==================================================] 100.00%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[2021-02-06 01:30:40,971] [    INFO] - Successfully installed ultra_light_fast_generic_face_detector_1mb_640-1.1.2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image = cv2.imread(<span class="string">&#x27;./test.jpeg&#x27;</span>)</span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">key_pts = np.array(result[<span class="number">0</span>][<span class="string">&#x27;data&#x27;</span>])</span><br><span class="line">key_pts = key_pts.reshape((<span class="number">136</span>, -<span class="number">1</span>))</span><br><span class="line">key_pts.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(136, 1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置画幅</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">16</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图像</span></span><br><span class="line">plt.imshow(image.astype(<span class="string">&#x27;uint8&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示关键点</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(key_pts), <span class="number">2</span>):</span><br><span class="line">    plt.scatter(key_pts[i], key_pts[i+<span class="number">1</span>], s=<span class="number">20</span>, marker=<span class="string">&#x27;.&#x27;</span>, c=<span class="string">&#x27;m&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024242293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="六、趣味应用">六、趣味应用</h2>
<p>当我们得到关键点的信息后，就可以进行一些趣味的应用。</p>
<h2 id="装饰预览">装饰预览</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">star_image = cv2.imread(<span class="string">&#x27;ps.jpeg&#x27;</span>)</span><br><span class="line">star_image = cv2.cvtColor(star_image, cv2.COLOR_BGR2RGB)</span><br><span class="line"><span class="comment"># star_image = cv2.resize(star_image, (40, 40))</span></span><br><span class="line"><span class="built_in">print</span>(star_image.shape)</span><br><span class="line">plt.imshow(star_image)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(524, 650, 3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;matplotlib.image.AxesImage at 0x7fc5567f18d0&gt;</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024233796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义功能函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_fu</span>(<span class="params">image, predicted_key_pts, size=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    展示加了贴纸的图像</span></span><br><span class="line"><span class="string">    Args：</span></span><br><span class="line"><span class="string">        image：裁剪后的图像 [224, 224, 3]</span></span><br><span class="line"><span class="string">        predicted_key_pts: 预测关键点的坐标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 3-32, 36-15</span></span><br><span class="line">    <span class="comment"># 计算坐标，15 和 34点的中间值</span></span><br><span class="line">    x1 = (<span class="built_in">int</span>(predicted_key_pts[<span class="number">28</span>]) + <span class="built_in">int</span>(predicted_key_pts[<span class="number">70</span>]))//<span class="number">2</span></span><br><span class="line">    y1 = (<span class="built_in">int</span>(predicted_key_pts[<span class="number">29</span>]) + <span class="built_in">int</span>(predicted_key_pts[<span class="number">71</span>]))//<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    x2 = (<span class="built_in">int</span>(predicted_key_pts[<span class="number">4</span>]) + <span class="built_in">int</span>(predicted_key_pts[<span class="number">62</span>]))//<span class="number">2</span></span><br><span class="line">    y2 = (<span class="built_in">int</span>(predicted_key_pts[<span class="number">5</span>]) + <span class="built_in">int</span>(predicted_key_pts[<span class="number">63</span>]))//<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># star_image = cv2.imread(&#x27;rat.jpg&#x27;)</span></span><br><span class="line">    star_image = cv2.imread(<span class="string">&#x27;ps.jpeg&#x27;</span>)</span><br><span class="line">    star_image = cv2.cvtColor(star_image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    <span class="comment"># resize</span></span><br><span class="line">    star_image = cv2.resize(star_image, (size, size))</span><br><span class="line">    <span class="comment"># 处理通道</span></span><br><span class="line">    <span class="keyword">if</span>(star_image.shape[<span class="number">2</span>] == <span class="number">4</span>):</span><br><span class="line">        star_image = star_image[:,:,<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将小图放到原图上</span></span><br><span class="line">    image[y1:y1+<span class="built_in">len</span>(star_image[<span class="number">0</span>]), x1:x1+<span class="built_in">len</span>(star_image[<span class="number">1</span>]),:] = star_image</span><br><span class="line"></span><br><span class="line">    image[y2:y2+<span class="built_in">len</span>(star_image[<span class="number">0</span>]), x2:x2+<span class="built_in">len</span>(star_image[<span class="number">1</span>]),:] = star_image</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 展示处理后的图片</span></span><br><span class="line">    plt.imshow(image.astype(<span class="string">&#x27;uint8&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 展示关键点信息</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predicted_key_pts)//<span class="number">2</span>,):</span><br><span class="line">        plt.scatter(predicted_key_pts[i*<span class="number">2</span>], predicted_key_pts[i*<span class="number">2</span>+<span class="number">1</span>], s=<span class="number">20</span>, marker=<span class="string">&#x27;.&#x27;</span>, c=<span class="string">&#x27;m&#x27;</span>) <span class="comment"># 展示关键点信息</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">custom_output</span>(<span class="params">test_images, test_outputs, batch_size=<span class="number">1</span>, h=<span class="number">20</span>, w=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    展示图像，预测关键点</span></span><br><span class="line"><span class="string">    Args：</span></span><br><span class="line"><span class="string">        test_images：裁剪后的图像 [224, 224, 3]</span></span><br><span class="line"><span class="string">        test_outputs: 模型的输出</span></span><br><span class="line"><span class="string">        batch_size: 批大小</span></span><br><span class="line"><span class="string">        h: 展示的图像高</span></span><br><span class="line"><span class="string">        w: 展示的图像宽</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(test_images.shape) == <span class="number">3</span>:</span><br><span class="line">        test_images = np.array([test_images])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line"></span><br><span class="line">        plt.figure(figsize=(h, w))</span><br><span class="line">        ax = plt.subplot(<span class="number">1</span>, batch_size, i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机裁剪后的图像</span></span><br><span class="line">        image = test_images[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型的输出，未还原的预测关键点坐标值</span></span><br><span class="line">        predicted_key_pts = test_outputs[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 还原后的真实的关键点坐标值</span></span><br><span class="line">        predicted_key_pts = predicted_key_pts * data_std + data_mean</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 展示图像和关键点</span></span><br><span class="line">        show_fu(np.squeeze(image), predicted_key_pts)</span><br><span class="line"></span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="作业-6：实现趣味-PS">作业 6：实现趣味 PS</h3>
<p>根据人脸检测的结果，实现趣味 PS。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取图像</span></span><br><span class="line">img = mpimg.imread(<span class="string">&#x27;./test.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关键点占位符</span></span><br><span class="line">kpt = np.ones((<span class="number">136</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">transform = Compose([Resize(<span class="number">256</span>), RandomCrop(<span class="number">224</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图像先重新定义大小，并裁剪到 224*224的大小</span></span><br><span class="line">rgb_img, kpt = transform([img, kpt])</span><br><span class="line"></span><br><span class="line">norm = GrayNormalize()</span><br><span class="line">to_chw = ToCHW()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对图像进行归一化和格式变换</span></span><br><span class="line">img, kpt = norm([rgb_img, kpt])</span><br><span class="line">img, kpt = to_chw([img, kpt])</span><br><span class="line"></span><br><span class="line">img = np.array([img], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载保存好的模型进行预测</span></span><br><span class="line"><span class="comment"># model = paddle.Model(SimpleNet())</span></span><br><span class="line"><span class="comment"># model.load(checkpoints_path)</span></span><br><span class="line"><span class="comment"># model.prepare()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果</span></span><br><span class="line">out = model.predict_batch([img])</span><br><span class="line">out = out[<span class="number">0</span>].reshape((out[<span class="number">0</span>].shape[<span class="number">0</span>], <span class="number">136</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化1</span></span><br><span class="line">custom_output(rgb_img, out, batch_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024256453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># paddleHub 效果可视化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置画幅</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># paddleHub 效果可视化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置画幅</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">20</span>))</span><br><span class="line"></span><br><span class="line">show_fu(image, key_pts, size=<span class="number">40</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210206024549531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
  </entry>
  <entry>
    <title>算法面试Python相关知识点</title>
    <url>/2022/05/30/8f191998df9746e1aeeae97ca644236b/</url>
    <content><![CDATA[<p>[toc]</p>
<h1>算法面试 Python 相关知识点</h1>
<h2 id="什么是解释性语言，什么是编译性语言？">什么是解释性语言，什么是编译性语言？</h2>
<p>计算机不能直接理解高级语言，只能直接理解机器语言，所以必须要把高级语言翻译成机器语言，计算机才能执行高级语言编写的程序。</p>
<ul>
<li>
<p>解释性语言在运行程序的时候才会进行翻译。</p>
</li>
<li>
<p>编译型语言写的程序在执行之前，需要一个专门的编译过程，把程序编译成机器语言（可执行文件）。</p>
</li>
</ul>
<h2 id="Python-程序运行过程？">Python 程序运行过程？</h2>
<p>Python 程序在解释器上执行分两个过程：</p>
<ul>
<li>
<p>编译：<br>
首先把程序的字节码保存为一个以.pyc 为扩展名的文件。作为一种启动速度的优化。下一次运行程序时，如果上没有修改过源码的话，Python 将会加载.pyc 文件并跳过编译这个步骤。</p>
</li>
<li>
<p>执行：<br>
当程序编译成字节码后，发送到 Python 虚拟机上来执行。虚拟机是 Python 的运行引擎。是 Python 解释器的最后一步。</p>
</li>
</ul>
<p>注：解释器即让其他程序运行起来的程序，是代码与机器的计算机硬件之间的软件逻辑层。Python 也是一个名为解释器的软件包。</p>
<h2 id="Python-的作用域？">Python 的作用域？</h2>
<p>Python 中的作用域分 4 种情况：</p>
<ul>
<li>L：local，局部作用域，即函数中定义的变量；</li>
<li>E：enclosing，嵌套的父级函数的局部作用域，即包含此函数的上级函数的局部作用域；</li>
<li>G：global，全局变量，就是模块级别定义的变量；</li>
<li>B：built-in，系统固定模块里面的变量，比如 int, bytearray 等。 搜索变量的优先级顺序依次是：作用域局部&gt;外层作用域&gt;当前模块中的全局&gt;python 内置作用域，也就是 LEGB。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="built_in">int</span>(<span class="number">2.9</span>)  <span class="comment"># int buiLt-in</span></span><br><span class="line">g_count = <span class="number">0</span> <span class="comment"># GLobaL</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    o_count = <span class="number">1</span> <span class="comment"># EncLosing</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        i_count = <span class="number">2</span> <span class="comment"># LocaL</span></span><br><span class="line">        <span class="built_in">print</span>(o_count)</span><br><span class="line">    <span class="comment"># print(i_count)  找不到</span></span><br><span class="line">    inner()</span><br><span class="line">outer()</span><br><span class="line"><span class="comment"># print(o_count)  # 找不到</span></span><br></pre></td></tr></table></figure>
<h2 id="Python-的数据结构？">Python 的数据结构？</h2>
<p>Python 中的绝大部分数据结构可以被最终分解为三种类型：集合（Set），序列（Sequence），映射（Mapping）。</p>
<ul>
<li>
<ol>
<li>集合是独立于标量，序列和映射之外的特殊数据结构，它支持数学理论的各种集合的运算。它的存在使得用程序代码实现数学理论变得方便。</li>
</ol>
</li>
<li>
<ol start="2">
<li>序列是 Python 中最为基础的内建类型。它分为七种类型：列表、字符串、元组、Unicode 字符串、字节数组、缓冲区和 xrange 对象。常用的是：列表（List）、字符串（String）、元组（Tuple）。</li>
</ol>
</li>
<li>
<ol start="3">
<li>映射在 Python 的实现是数据结构字典（Dictionary）。作为第三种基本单位，映射的灵活使得它在多种场合中都有广泛的应用和良好的可拓展性。</li>
</ol>
</li>
</ul>
<p>Python 可变类型是列表、集合、字典，不可变有字符串、元组、数字。</p>
<h2 id="进程和线程">进程和线程</h2>
<ul>
<li>进程：一个运行的程序（代码）就是一个进程，没有运行的代码叫程序，进程是<strong>系统资源分配的最小单位</strong>，进程拥有自己独立的内存空间，所有进程间数据不共享，开销大。</li>
<li>线程：<strong>CPU 调度执行的最小单位</strong>，也叫执行路径，不能独立存在，依赖进程存在，一个进程至少有一个线程，叫主线程，而多个线程共享内存（数据共享，共享全局变量),从而极大地提高了程序的运行效率。</li>
<li>多线程
<ul>
<li>Python 在任意时刻，只有一个线程在解释器中运行。对 Python 虚拟机的访问由全局解释器锁（GIL）来控制，正是这个锁能保证同一时刻只有一个线程在运行。</li>
<li>多线程共享主进程的资源，所以可能还会改变其中的变量，这个需加上线程锁，每次执行完一个线程再执行下一个线程。</li>
<li>一个 CPU 在同一个时刻只能执行一个线程，但是当遇到 IO 操作或者运行一定的代码量的时候就会释放全局解释器锁，执行另外一个线程。</li>
</ul>
</li>
<li>互斥锁和死锁
<ul>
<li>互斥锁：即确保某段关键代码的数据只能又一个线程从头到尾完整执行，保证了这段代码数据的安全性，但是这样就会导致死锁。</li>
<li>死锁：多个子线程在等待对方解除占用状态，但是都不先解锁，互相等待，这就是死锁。</li>
</ul>
</li>
</ul>
<h2 id="Lambda">Lambda</h2>
<p>lambda 函数是一个可以接收任意多个参数(包括可选参数)并且返回单个表达式值的函数。</p>
<ul>
<li>1.lambda 函数比较轻便，即用即仍，很适合需要完成一项功能，但是此功能只在此一处使用，连名字都很随意的情况下。</li>
<li>2.匿名函数，一般用来给 filter，map 这样的函数式编程服务。</li>
<li>3.作为回调函数，传递给某些应用，比如消息处理。</li>
</ul>
<h2 id="浅拷贝和深拷贝">浅拷贝和深拷贝</h2>
<ul>
<li>浅拷贝(copy)：创建新对象，其内容是原对象的引用。拷贝父对象，不会拷贝对象的内部的子对象。</li>
<li>深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。深拷贝出来的对象是一个全新的对象，不再与原来的对象有任何关联。</li>
</ul>
<p>解析：</p>
<ul>
<li>变量：是一个系统表的元素，拥有指向对象的连接空间</li>
<li>对象：被分配的一块内存，存储其所代表的值</li>
<li>引用：是自动形成的从变量到对象的指针</li>
<li>类型：属于对象，而非变量</li>
<li>不可变对象：一旦创建就不可修改的对象，包括字符串、元组、数值类型，不可变类型，不管是深拷贝还是浅拷贝，地址值和拷贝后的值都是一样的。（该对象所指向的内存中的值不能被改变。当改变某个变量时候，由于其所指的值不能被改变，相当于把原来的值复制一份后再改变，这会开辟一个新的地址，变量再指向这个新的地址。）</li>
<li>可变对象：可以修改的对象，包括列表、字典、集合（该对象所指向的内存中的值可以被改变。变量（准确的说是引用）改变后，实际上是其所指的值直接发生改变，并没有发生复制行为，也没有开辟新的地址，通俗点说就是原地改变。）</li>
</ul>
<p>当写 a = ‘python’，Python 解释器干的事情：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.创建变量a</span><br><span class="line">2.创建一个对象(分配一块内存)，来存储值 ‘python’</span><br><span class="line">3.将变量与对象，通过指针连接起来，从变量到对象的连接称之为引用(变量引用对象)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<ol>
<li><strong>赋值</strong>：只是复制了新对象的引用，不会开辟新的内存空间。并不会产生一个独立的对象单独存在，只是将原有的数据块打上一个新标签，所以当其中一个标签被改变的时候，数据块就会发生变化，另一个标签也会随之改变。</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>浅拷贝</strong>：创建新对象，其内容是原对象的引用。</li>
</ol>
<p>浅拷贝有三种形式： 切片操作，工厂函数，copy 模块中的 copy 函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">如： lst = [1,2,[3,4]]</span><br><span class="line">    切片操作：lst1 = lst[:] 或者 lst1 = [each for each in lst]</span><br><span class="line">    工厂函数：lst1 = list(lst)</span><br><span class="line">    copy函数：lst1 = copy.copy(lst)</span><br></pre></td></tr></table></figure>
<p>浅拷贝之所以称为浅拷贝，是它仅仅只拷贝了一层，拷贝了最外围的对象本身，内部的元素都只是拷贝了一个引用而已，在 lst 中有一个嵌套的 list[3,4]，如果修改了它，情况就不一样了。</p>
<ul>
<li>
<p>浅拷贝要分两种情况进行讨论：</p>
</li>
<li>
<ol>
<li>当浅拷贝的值是不可变对象（字符串、元组、数值类型）时和“赋值”的情况一样，对象的 id 值（id()函数用于获取对象的内存地址）与浅拷贝原来的值相同。<br>
 - 2. 当浅拷贝的值是可变对象（列表、字典、集合）时会产生一个“不是那么独立的对象”存在。有两种情况：<strong>第一种情况</strong>：复制的对象中无复杂子对象[1,2,3,4,5,6]，原来值的改变并不会影响浅复制的值，同时浅复制的值改变也并不会影响原来的值。原来值的 id 值与浅复制原来的值不同。<strong>第二种情况</strong>：复制的对象中有复杂子对象（例如列表中的一个子元素是一个列表[1,2,3,[4,5],6]），如果不改变其中复杂子对象，浅复制的值改变并不会影响原来的值。 但是改变原来的值中的复杂子对象的值会影响浅复制的值。</li>
</ol>
</li>
</ul>
</li>
<li>
<ol start="3">
<li><strong>深拷贝</strong>：和浅拷贝对应，深拷贝拷贝了对象的所有元素，包括多层嵌套的元素。深拷贝出来的对象是一个全新的对象，不再与原来的对象有任何关联。所以改变原有被复制对象不会对已经复制出来的新对象产生影响。</li>
</ol>
</li>
</ul>
<h2 id="Python-多线程是否能用多个-CPU，为什么？">Python 多线程是否能用多个 CPU，为什么？</h2>
<p>Python 的多线程不能利用多核 CPU。<br>
因为 Python 解释器使用了 GIL（Global Interpreter Lock），在任意时刻中只允许单个 Python 线程运行。无论系统有多少个 CPU 核心，Python 程序都只能在一个 CPU 上运行 。<br>
注：GIL 的功能是：在 CPython 解释器中执行的每一个 Python 线程，都会先锁住自己，以阻止别的线程执行。</p>
<h2 id="Python-垃圾回收机制">Python 垃圾回收机制</h2>
<p>Python 的垃圾回收机制是以：引用计数器为主，标记清除和分代回收为辅。</p>
<ul>
<li>
<ol>
<li>引用计数：每个对象内部都维护了一个值，该值记录这此对象被引用的次数，如果次数为 0，则 Python 垃圾回收机制会自动清除此对象。</li>
</ol>
</li>
<li>
<ol start="2">
<li>标记-清除（Mark—Sweep）：被分配对象的计数值与被释放对象的计数值之间的差异累计超过某个阈值，则 Python 的收集机制就启动</li>
</ol>
<ul>
<li>标记阶段，遍历所有的对象，如果是可达的（reachable），也就是还有对象引用它，那么就标记该对象为可达。</li>
<li>清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。</li>
</ul>
</li>
<li>
<ol start="3">
<li>“分代回收”(Generational Collection)。当代码中主动执行 gc.collect() 命令时，Python 解释器就会进行垃圾回收。</li>
</ol>
</li>
</ul>
<p>总体来说，在 Python 中，主要通过引用计数进行垃圾回收；通过 “标记-清除” 解决容器对象可能产生的循环引用问题；通过 “分代回收” 以空间换时间的方法提高垃圾回收效率。</p>
<h2 id="Python-生成器">Python 生成器</h2>
<p>生成器是一种可以简单有效的创建迭代器的工具。像常规函数一样撰写，但是在需要返回数据时使用 yield 语句。每当对它调用 next()函数，生成器从它上次停止的地方重新开始（它会记住所有的数据值和上次执行的语句）。</p>
<h2 id="Python-迭代器和生成器的区别">Python 迭代器和生成器的区别</h2>
<ul>
<li>迭代器有两个方法 next 方法和 iter 方法，iter 方法获取对象的迭代器，next 方法返回下一个迭代器。</li>
<li>生成器：使用了 yield 的函数被称为生成器（generator），在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。调用一个生成器函数，返回的是一个迭代器对象。</li>
</ul>
<p>区别：</p>
<ul>
<li>1.语法上：生成器是通过函数的形式中调用 yield 或（）的形式创建的；迭代器可以通过 iter（） 内置函数创建</li>
<li>2.用法上：生成器在调用 next（）函数或 for 循环中，所有过程被执行，且返回值；迭代器在调用 next（）函数或 for 循环中，所有值被返回，没有其他过程或说动作。</li>
</ul>
<p>总结：</p>
<ul>
<li>迭代器：在循环遍历自定义容器对象时，会使用 python 内置函数 iter()调用遍历对象的_iter_(self)获得一个迭代器，之后再循环对这个迭代器使用 next()调用迭代器对象的_next_(self) 。</li>
<li>生成器：只能遍历一次，是一类特殊的迭代器。生成器能做到迭代器能做的所有事，而且因为自动创建了 iter()和 next()方法，生成器显得特别简洁，而且生成器也是高效的，使用生成器表达式取代列表解析可以同时节省内存。除了创建和保存程序状态的自动方法，当发生器终结时，还会自动抛出 StopIteration 异常。</li>
</ul>
<h2 id="什么是闭包">什么是闭包</h2>
<p>在一个内部函数中，对外部作用域的变量进行引用，(并且一般外部函数的返回值为内部函数)，那么内部函数就被认为是闭包。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>(<span class="params">y</span>):</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(outer(<span class="number">6</span>)(<span class="number">5</span>))</span><br><span class="line">-----------------------------</span><br><span class="line">&gt;&gt;&gt;<span class="number">11</span></span><br></pre></td></tr></table></figure>
<p>如代码所示，在 outer 函数内，又定义了一个 inner 函数，并且 inner 函数又引用了外部函数 outer 的变量 x，这就是一个闭包了。在输出时，outer(6)(5)，第一个括号传进去的值返回 inner 函数，其实就是返回 6 + y，所以再传第二个参数进去，就可以得到返回值，6 + 5。</p>
<h2 id="Python-装饰器">Python 装饰器</h2>
<p>装饰器本质上是一个 Python 函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能。<br>
装饰器的返回值也是一个函数对象，它经常用于有切面需求的场景，比如：插入日志 性能测试 事务处理 缓存 权限校验等场景 装饰器是解决这类问题的绝佳设计。<br>
有了装饰器，就可以抽离出大量与函数功能本身无关的雷同代码并继续重用，概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。</p>
<h3 id="装饰器">装饰器</h3>
<p>函数 use_logging 就是装饰器，它把执行真正业务方法的 func 包裹在函数里面，看起来像 bar 被 use_logging 装饰了。@符号是装饰器的语法糖，在定义函数的时候使用，避免再一次赋值操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_logging</span>(<span class="params">func</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">    	logging.warn(<span class="string">&quot;%s is running&quot;</span> % func.__name__)</span><br><span class="line">    	<span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_logging</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;i am bar&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_logging</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;i am foo&#x27;</span>)</span><br><span class="line"></span><br><span class="line">bar()</span><br><span class="line">------------------------</span><br><span class="line">&gt;&gt;&gt;bar <span class="keyword">is</span> running</span><br><span class="line">&gt;&gt;&gt;i am bar</span><br></pre></td></tr></table></figure>
<h3 id="带参数的装饰器">带参数的装饰器</h3>
<p>装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_logging</span>(<span class="params">level</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        	<span class="keyword">if</span> level == <span class="string">&quot;warn&quot;</span>:</span><br><span class="line">        		logging.warn(<span class="string">&quot;%s is running&quot;</span> % func.__name__)</span><br><span class="line">        	<span class="keyword">return</span> func(*args)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@use_logging(<span class="params">level=<span class="string">&quot;warn&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">name=<span class="string">&#x27;foo&#x27;</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;i am %s&quot;</span> % name)</span><br><span class="line"></span><br><span class="line">foo()</span><br><span class="line">------------------------</span><br><span class="line">&gt;&gt;&gt;foo <span class="keyword">is</span> running</span><br><span class="line">&gt;&gt;&gt;i am foo</span><br></pre></td></tr></table></figure>
<h3 id="类装饰器">类装饰器</h3>
<p>相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器还可以依靠类内部的__call__方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Foo</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func</span>):</span><br><span class="line">        self._func = func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;class decorator runing&#x27;</span>)</span><br><span class="line">        self._func()</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;class decorator ending&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@Foo</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>():</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;bar&#x27;</span>)</span><br><span class="line"></span><br><span class="line">bar()</span><br><span class="line">------------------------</span><br><span class="line">&gt;&gt;&gt;<span class="keyword">class</span> <span class="title class_">decorator</span> runing</span><br><span class="line">&gt;&gt;&gt;bar</span><br><span class="line">&gt;&gt;&gt;<span class="keyword">class</span> <span class="title class_">decorator</span> ending</span><br></pre></td></tr></table></figure>
<h2 id="Python-中-yield-和-return-的区别">Python 中 yield 和 return 的区别</h2>
<ul>
<li>共同点：<code>return</code>和<code>yield</code>都用来返回值；在一次性地返回所有值场景中<code>return</code>和<code>yield</code>的作用是一样的。</li>
<li>不同点：如果要返回的数据是通过<code>for</code>等循环生成的迭代器类型数据（如列表、元组），<code>return</code>只能在循环外部一次性地返回，<code>yeild</code>则可以在循环内部逐个元素返回（<code>yield</code> 函数会暂停并保存当前所有的运行信息，返回 <code>yield</code> 的值, 并在下一次执行 <code>next()</code> 方法时从当前位置继续运行）</li>
</ul>
<h2 id="Python-中-set-的底层实现">Python 中 set 的底层实现</h2>
<p>散列表/哈希表。set 只是默认键和值是相同的。<br>
注：散列表是根据关键字而直接进行访问值的数据结构。也就是说散列表建立了关键字和存储地址之间的一种直接映射关系。</p>
<h2 id="Python-中字典与-set-区别？">Python 中字典与 set 区别？</h2>
<ul>
<li>1.字典是一系列无序的键值对的组合；集合 set()里的元素默认键值是一样的，是单一的一个元素。</li>
<li>2.从 python3.6 后，字典有序；集合无序。</li>
<li>3.字典键不能重复；集合 set()元素不能重复。</li>
</ul>
<h2 id="Python-中-init-和-new-和-call-的区别？">Python 中__init__和__new__和__call__的区别？</h2>
<ul>
<li><code>__init__</code>是初始化方法</li>
<li><code>__new__</code>实例化对象</li>
<li><code>__call__</code>允许一个类的实例像函数一样被调用。实质上说，这意味着 x() 与 x.call() 是相同。</li>
</ul>
<p>构造方法 = 创建对象 + 初始化对象 = <code>new</code> + <code>init</code><br>
<code>__new__</code>方法是在实例创建之前被调用，是一个静态方法，主要的功能就是创建一个类的实例并返回<br>
<code>__init__</code>方法是在实例创建之后被调用，主要的功能是设置实例的一些属性初始值</p>
<p>实际测试：<code>__new__</code>在<code>__init__</code>之前被调用，<code>__new__</code>的返回值（实例）将传递给<code>__init__</code>方法的第一个参数（self），然后<code>__init__</code>给这个实例(self)设置一些参数。</p>
<h2 id="Python-中的内存管理">Python 中的内存管理</h2>
<h3 id="什么是内存管理器（what）">什么是内存管理器（what）</h3>
<p>Python 作为一个高层次的结合了解释性、编译性、互动性和面向对象的脚本语言，与大多数编程语言不同，Python 中的变量无需事先申明，变量无需指定类型，程序员无需关心内存管理，Python 解释器给你自动回收。开发人员不用过多的关心内存管理机制，这一切全部由 Python 内存管理器承担了复杂的内存管理工作。</p>
<p>内存不外乎创建和销毁两部分，本文将围绕 python 的内存池和垃圾回收两部分进行分析。</p>
<h3 id="Python-内存池">Python 内存池</h3>
<p><strong>为什么要引入内存池（why）</strong></p>
<p>当创建大量消耗小内存的对象时，频繁调用 new/malloc 会导致大量的内存碎片，致使效率降低。内存池的作用就是预先在内存中申请一定数量的，大小相等的内存块留作备用，当有新的内存需求时，就先从内存池中分配内存给这个需求，不够之后再申请新的内存。这样做最显著的优势就是能够减少内存碎片，提升效率。</p>
<p>Python 中的内存管理机制为<strong>Pymalloc</strong>。</p>
<p><strong>内存池是如何工作的（how）</strong><br>
首先，我们看一张 CPython(python 解释器)的内存架构图：</p>
<p><img src="/resource/e6695e0f6d82420ba3686e5bed395bc8.png" alt="2022-05-16-21-36-24.png"></p>
<ul>
<li>Python 的对象管理主要位于 Level+1~Level+3 层。</li>
<li>Level+3 层：对于 Python 内置的对象（比如 int,dict 等）都有独立的私有内存池，对象之间的内存池不共享，即 int 释放的内存，不会被分配给 float 使用。</li>
<li>Level+2 层：当申请的内存大小 &lt; 256KB 时，内存分配主要由 Python 对象分配器（Python’s object allocator）实施。</li>
<li>Level+1 层：当申请的内存大小 &gt; 256KB 时，由 Python 原生的内存分配器进行分配，本质上是调用 C 标准库中的 malloc/realloc 等函数</li>
</ul>
<p><strong>内存释放</strong></p>
<p>关于释放内存方面，当一个对象的<strong>引用计数</strong>变为 0 时，Python 就会调用它的<strong>析构函数</strong>。调用析构函数并不意味着最终一定会调用 free 来释放内存空间，如果真是这样的话，那频繁地申请、释放内存空间会使 Python 的执行效率大打折扣。因此在析构时也采用了内存池机制，从内存池申请到的内存会被归还到内存池中，以避免频繁地申请和释放动作。</p>
<h3 id="垃圾回收机制">垃圾回收机制</h3>
<p>Python 的垃圾回收机制采用<strong>引用计数</strong>机制为主，<strong>标记-清除</strong>和<strong>分代回收</strong>机制为辅的策略。其中，<strong>标记-清除</strong>机制用来解决计数引用带来的循环引用而无法释放内存的问题，<strong>分代回收</strong>机制是为提升垃圾回收的效率。</p>
<h4 id="引用计数">引用计数</h4>
<p>Python 通过引用计数来保存内存中的变量追踪，即记录该对象被其他使用的对象引用的次数。</p>
<p>Python 中有个内部跟踪变量叫做引用计数器，每个变量有多少个引用，简称引用计数。当某个对象的引用计数为 0 时，就列入了垃圾回收队列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> sys</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)  <span class="comment"># 获取对象a的引用次数</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> b  <span class="comment"># 删除b的引用</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = <span class="built_in">list</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.append(a) <span class="comment"># 加入到容器中</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> c  <span class="comment"># 删除容器，引用-1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">3</span>, <span class="number">4</span>]  <span class="comment"># 重新赋值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">注意：当把a作为参数传递给getrefcount时，会产生一个临时的引用，因此得出来的结果比真实情况 + 1</span><br></pre></td></tr></table></figure>
<p><strong>引用计数增加的情况</strong>：</p>
<ul>
<li>一个对象被分配给一个新的名字（例如：<code>a = [1, 2]</code>）</li>
<li>将其放入一个容器中（如列表、元组或字典）（例如：<code>c.append(a)</code>）</li>
</ul>
<p><strong>引用计数减少的情况</strong>：</p>
<ul>
<li>使用<code>del</code>语句对对象别名显式的销毁(例如：<code>del b</code>)</li>
<li>对象所在的容器被销毁或从容器中删除对象（例如：<code>del c</code> ）</li>
<li>引用超出作用域或被重新赋值（例如：<code>a = [3,4]</code>）</li>
</ul>
<p>引用计数能够解决大多数垃圾回收的问题，但是遇到<strong>两个对象相互引用</strong>的情况，del 语句可以减少引用次数，但是引用计数不会归 0，对象也就不会被销毁，从而造成了内存泄漏问题。针对该情况，Python 引入了<strong>标记-清除</strong>机制。</p>
<h4 id="标记-清除（Mark-and-Sweep）">标记-清除（Mark-and-Sweep）</h4>
<p><strong>标记-清除</strong>用来解决引用计数机制产生的<strong>循环引用</strong>，进而导致内存泄漏的问题。循环引用只有在容器对象才会产生，比如字典，元组，列表等。</p>
<p>顾名思义，该机制在进行垃圾回收时分成了两步，分别是：</p>
<ul>
<li><strong>标记阶段</strong>，遍历所有的对象，如果是可达的（reachable），也就是还有对象引用它，那么就标记该对象为可达。</li>
<li><strong>清除阶段</strong>，再次遍历对象，如果发现某个对象没有标记为可达（即为 Unreachable），则就将其回收。</li>
</ul>
<p>当一个对象被创建时，它的标记位被设置为 <code>0（False）</code>。<br>
在标记阶段，从根结点出发，也就是可以直接访问的局部变量，采用图的遍历算法，比如 <code>DFS</code>，将所有可达对象的标记位设置为 <code>1（True）</code>。<br>
在清除阶段，线性扫描堆内存，将不可达对象直接释放，将可达对象的标记位重新设置为 <code>0</code>，为下一次标记清除做准备。<br>
标记清除的优点是可以处理循环引用，缺点是算法会暂停程序的执行。为了优化垃圾回收的性能，可以使用分代回收算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = [<span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(b)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.append(b)</span><br><span class="line">* * *</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(b)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.append(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sys.getrefcount(a)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> b</span><br></pre></td></tr></table></figure>
<ul>
<li><code>a</code>引用<code>b</code>，<code>b</code>引用<code>a</code>, 此时两个对象各自被引用了 2 次（去除 getrefcout()的临时引用）</li>
<li>执行<code>del</code>之后，对象<code>a</code>, <code>b</code>的引用次数都减 1，此时各自的引用计数器都为 1，陷入循环引用。</li>
<li>标记：找到其中的一端<code>a</code>，因为它有一个对<code>b</code>的引用，则将<code>b</code>的引用计数减 1。</li>
<li>标记：再沿着引用到<code>b</code>，<code>b</code>有一个<code>a</code>的引用，将<code>a</code>的引用计数减 1，此时对象<code>a</code>和<code>b</code>的引用次数全部为 0，被标记为不可达（Unreachable）。</li>
<li>清除: 被标记为不可达的对象就是真正需要被释放的对象</li>
</ul>
<p>上面描述的垃圾回收的阶段，会暂停整个应用程序，等待标记清除结束后才会恢复应用程序的运行。为了减少应用程序暂停的时间，Python 通过**“分代回收”(Generational Collection)**以空间换时间的方法提高垃圾回收效率。</p>
<h4 id="分代回收">分代回收</h4>
<p><strong>分代回收</strong>是基于这样的一个统计事实，对于程序，存在一定比例的内存块的生存周期比较短；而剩下的内存块，生存周期会比较长，甚至会从程序开始一直持续到程序结束。生存期较短对象的比例通常在 80%～ 90%之间。 因此，简单地认为：对象存在时间越长，越可能不是垃圾，应该越少去收集。这样在执行标记-清除算法时可以有效减小遍历的对象数，从而提高垃圾回收的速度，是一种<strong>以空间换时间的方法策略</strong>。</p>
<p>Python 将所有的对象分为年轻代（第 0 代）、中年代（第 1 代）、老年代（第 2 代）三代。所有的新建对象默认是 第 0 代对象。当在第 0 代的 gc 扫描中存活下来的对象将被移至第 1 代，在第 1 代的 gc 扫描中存活下来的对象将被移至第 2 代。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gc扫描次数（第0代&gt;第1代&gt;第2代）</span><br></pre></td></tr></table></figure>
<p>当某一代中被分配的对象与被释放的对象之差达到某一阈值时，就会触发当前一代的<strong>gc 扫描</strong>。当某一代被扫描时，比它年轻的一代也会被扫描，因此，第 2 代的 gc 扫描发生时，第 0，1 代的 gc 扫描也会发生，即为<strong>全代扫描</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> gc</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gc.get_threshold()  <span class="comment"># 分代回收机制的参数阈值设置</span></span><br><span class="line">(<span class="number">700</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 700=新分配的对象数量-释放的对象数量，第0代gc扫描被触发</span></span><br><span class="line"><span class="comment"># 第一个10：第0代gc扫描发生10次，则第1代的gc扫描被触发</span></span><br><span class="line"><span class="comment"># 第二个10：第1代的gc扫描发生10次，则第2代的gc扫描被触发</span></span><br></pre></td></tr></table></figure>
<p>总体而言，Python 通过<strong>内存池</strong>来减少内存碎片化，提高执行效率。主要通过<strong>引用计数</strong>来完成垃圾回收，通过<strong>标记-清除</strong>解决容器对象循环引用造成的问题，通过<strong>分代回收</strong>提高垃圾回收的效率。</p>
<h2 id="类方法和静态方法的区别">类方法和静态方法的区别</h2>
<p>Python 类方法和实例方法相似，它最少也要包含一个参数，只不过类方法中通常将其命名为 cls，Python 会自动将类本身绑定给 cls 参数（注意，绑定的不是类对象）。也就是说，在调用类方法时，无需显式为 cls 参数传参。</p>
<p>静态方法没有类似 self、cls 这样的特殊参数，因此 Python 解释器不会对它包含的参数做任何类或对象的绑定。也正因为如此，类的静态方法中无法调用任何类属性和类方法。</p>
<p>实例方法只能被实例对象调用(Python3 中，如果类调用实例方法，需要显示的传 self, 也就是实例对象自己)，静态方法(由@staticmethod 装饰的方法)、类方法(由@classmethod 装饰的方法)，可以被类或类的实例对象调用。</p>
<ul>
<li>实例方法，第一个参数必须要默认传实例对象，一般习惯用 self。</li>
<li>静态方法，参数没有要求。</li>
<li>类方法，第一个参数必须要默认传类，一般习惯用 cls。</li>
</ul>
<h2 id="Python-的错误和异常处理">Python 的错误和异常处理</h2>
<p>Python 中会发生两种类型的错误。</p>
<ul>
<li>
<ol>
<li>语法错误：如果未遵循正确的语言语法，则会引发语法错误。</li>
</ol>
</li>
<li>
<ol start="2">
<li>逻辑错误（异常）：在运行时中，通过语法测试后发生错误的情况称为异常或逻辑类型。</li>
</ol>
</li>
</ul>
<p><strong>异常处理</strong><br>
通过使用 <code>try</code>…<code>except</code> 来处理异常状况。一般来说会把通常的语句放在 <code>try</code> 代码块中，将错误处理器代码放置在 <code>except</code> 代码块中。</p>
<ul>
<li>try…else 语句：else 语句是在 try 语句中的代码没有任何异常的情况下，再执行 else 语句下的代码。</li>
<li>try…finally 语句：finally 语句就是不管上面有没有异常，都要执行 finally 语句下的代码，通常是做一些必须要释放的资源的代码，最典型的就是文件操作和数据库操作。</li>
<li>抛出异常 raise：raise 语句是抛出一个指定的异常。</li>
</ul>
<h2 id="Python-中的-is-和-的区别">Python 中的 is 和==的区别</h2>
<ul>
<li><code>is</code>比较的是两个对象的 id 值是否相等，也就是比较两个对象是否为同一个实例对象，是否指向同一个内存地址。</li>
<li><code>==</code>比较的是两个对象的内容是否相等，默认会调用对象的<code>eq()</code>方法。</li>
</ul>
<h2 id="GBK-和-UTF-8-的区别">GBK 和 UTF-8 的区别</h2>
<ul>
<li>GBK 是在国家标准 GB2312 基础上扩容后兼容 GB2312 的标准。GBK 编码专门用来解决中文编码的，是双字节的。不论中英文都是双字节的。</li>
<li>UTF－8 编码是用以解决国际上字符的一种多字节编码，它对英文使用 8 位（即一个字节），中文使用 24 位（三个字节）来编码。对于英文字符较多的论坛则用 UTF－8 节省空间。另外，如果是外国人访问 GBK 网页，需要下载中文语言包支持。访问 UTF-8 编码的网页则不出现这问题。可以直接访问。</li>
<li>GBK 包含全部中文字符；UTF-8 则包含全世界所有国家需要用到的字符。</li>
</ul>
<h2 id="Python-遍历字典的方法">Python 遍历字典的方法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dic1 = &#123;<span class="string">&#x27;date&#x27;</span>:<span class="string">&#x27;2018.11.2&#x27;</span>,<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;carlber&#x27;</span>,<span class="string">&#x27;work&#x27;</span>:<span class="string">&quot;遍历&quot;</span>,<span class="string">&#x27;number&#x27;</span>:<span class="number">3</span>&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dic1:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> dic1.keys():   <span class="comment">#遍历字典中的键</span></span><br><span class="line">    <span class="built_in">print</span>(key)</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> dic1.values():  <span class="comment">#遍历字典中的值</span></span><br><span class="line">    <span class="built_in">print</span>(value)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> dic1.items():  <span class="comment">#遍历字典中的元素</span></span><br><span class="line">    <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure>
<h2 id="init-py文件的作用以及意义"><code>__init__.py</code>文件的作用以及意义</h2>
<ul>
<li>这个文件定义了包的属性和方法，它可以什么也不定义；可以只是一个空文件，但是必须存在。</li>
<li>如果 <code>__init__.py</code> 不存在，这个目录就仅仅是一个目录，而不是一个包，它就不能被导入或者包含其它的模块和嵌套包。</li>
<li>或者可以这样理解。这样，当导入这个包的时候，<code>__init__.py</code>文件自动运行。帮导入了这么多个模块，就不需要将所有的<code>import</code>语句写在一个文件里了，也可以减少代码量。</li>
</ul>
<h2 id="函数调用参数的传递方式是值传递还是引用传递？">函数调用参数的传递方式是值传递还是引用传递？</h2>
<p>Python 的参数传递有：<strong>位置参数</strong>, <strong>默认参数</strong>, <strong>可变参数</strong>, <strong>关键字参数</strong>.</p>
<p>函数的传值到底是<strong>值传递</strong>还是<strong>引用传递</strong>, 要分情况：</p>
<ul>
<li><strong>不可变参数用值传递</strong>：像整数和字符串这样的不可变对象，是通过拷贝进行传递的，因为你无论如何都不可能在原处改变不可变对象.</li>
<li><strong>可变参数是引用传递</strong>：比如像列表, 字典这样的对象是通过引用传递, 和 C 语言里面的用指针传递数组很相似, 可变对象能在函数内部改变.</li>
</ul>
<h2 id="Python-的缺省参数">Python 的缺省参数</h2>
<p>缺省参数指在调用函数的时候没有传入参数的情况下, 调用默认的参数, 在调用函数的同时赋值时, 所传入的参数会替代默认参数.</p>
<ul>
<li><code>*args</code>是不定长参数，它可以表示输入参数是不确定的，可以是任意多个。</li>
<li><code>**kwargs</code>是关键字参数，赋值的时候是以键值对的方式，参数可以是任意多对在定义函数的时候</li>
<li>不确定会有多少参数会传入时，就可以使用两个参数。</li>
</ul>
<h2 id="map与reduce函数"><code>map</code>与<code>reduce</code>函数</h2>
<ul>
<li><code>map()</code>是将传入的函数依次作用到序列的每个元素，每个元素都是独自被函数“作用”一次 。</li>
<li><code>reduce()</code>是将传入的函数作用在序列的第一个元素得到结果后，把这个结果继续与下一个元素作用（累</li>
</ul>
<p>积计算）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])   <span class="comment"># 使用 lambda</span></span><br><span class="line"><span class="comment"># [1, 4, 9, 16]</span></span><br><span class="line">reduce(<span class="keyword">lambda</span> x, y: x * y, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 相当于 ((1 * 2) * 3) * 4</span></span><br><span class="line"><span class="comment"># 24</span></span><br></pre></td></tr></table></figure>
<h2 id="hasattr-getattr-setattr-函数使用详解？"><code>hasattr()</code>, <code>getattr()</code>, <code>setattr()</code>函数使用详解？</h2>
<ol>
<li><code>hasattr(object, name)</code>函数：</li>
</ol>
<p>判断一个对象里面是否有 name 属性或者 name 方法，返回 bool 值，有 name 属性（方法）返回 True，否则返回 False。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">function_demo</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	name = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;hello function&quot;</span></span><br><span class="line">functiondemo = function_demo()</span><br><span class="line">res = <span class="built_in">hasattr</span>(functiondemo, <span class="string">&quot;name&quot;</span>) <span class="comment"># 判断对象是否有name属性，True</span></span><br><span class="line">res = <span class="built_in">hasattr</span>(functiondemo, <span class="string">&quot;run&quot;</span>)  <span class="comment"># 判断对象是否有run方法，True</span></span><br><span class="line">res = <span class="built_in">hasattr</span>(functiondemo, <span class="string">&quot;age&quot;</span>)  <span class="comment"># 判断对象是否有age属性，False</span></span><br></pre></td></tr></table></figure>
<ol start="2">
<li><code>getattr(object, name[,default])</code>函数：</li>
</ol>
<p>获取对象 object 的属性或者方法，如果存在则打印出来，如果不存在，打印默认值，默认值可选。<br>
注意：如果返回的是对象的方法，则打印结果是：方法的内存地址，如果需要运行这个方法，可以在后面添加括号()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">function_demo</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	name = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;hello function&quot;</span></span><br><span class="line">functiondemo = function_demo()</span><br><span class="line"><span class="built_in">getattr</span>(functiondemo, <span class="string">&quot;name&quot;</span>)<span class="comment"># 获取name属性，存在就打印出来 --- demo</span></span><br><span class="line"><span class="built_in">getattr</span>(functiondemo, <span class="string">&quot;run&quot;</span>) <span class="comment"># 获取run 方法，存在打印出方法的内存地址</span></span><br><span class="line"><span class="built_in">getattr</span>(functiondemo, <span class="string">&quot;age&quot;</span>) <span class="comment"># 获取不存在的属性，报错</span></span><br><span class="line"><span class="built_in">getattr</span>(functiondemo, <span class="string">&quot;age&quot;</span>, <span class="number">18</span>) <span class="comment"># 获取不存在的属性，返回一个默认值</span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li><code>setattr(object, name, values)</code>函数：</li>
</ol>
<p>给对象的属性赋值，若属性不存在，先创建再赋值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">function_demo</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	name = <span class="string">&quot;demo&quot;</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">			<span class="keyword">return</span> <span class="string">&quot;hello function&quot;</span></span><br><span class="line">functiondemo = function_demo()</span><br><span class="line">res = <span class="built_in">hasattr</span>(functiondemo, <span class="string">&quot;age&quot;</span>) <span class="comment"># 判断age属性是否存在，False</span></span><br><span class="line"><span class="built_in">print</span>(res)</span><br><span class="line"><span class="built_in">setattr</span>(functiondemo, <span class="string">&quot;age&quot;</span>, <span class="number">18</span>) <span class="comment"># 对age属性进行赋值，无返回值</span></span><br><span class="line">res = <span class="built_in">hasattr</span>(functiondemo, <span class="string">&quot;age&quot;</span>) <span class="comment"># 再次判断属性是否存在，True</span></span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>综合使用</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">function_demo</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	name = <span class="string">&quot;demo&quot;</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;hello function&quot;</span></span><br><span class="line">functiondemo = function_demo()</span><br><span class="line">res = <span class="built_in">hasattr</span>(functiondemo, <span class="string">&quot;addr&quot;</span>) <span class="comment"># 先判断是否存在</span></span><br><span class="line"><span class="keyword">if</span> res:</span><br><span class="line">	addr = <span class="built_in">getattr</span>(functiondemo, <span class="string">&quot;addr&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(addr)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	addr = <span class="built_in">getattr</span>(functiondemo, <span class="string">&quot;addr&quot;</span>, <span class="built_in">setattr</span>(functiondemo, <span class="string">&quot;addr&quot;</span>, <span class="string">&quot;北京首</span></span><br><span class="line"><span class="string">都&quot;</span>))</span><br><span class="line">	<span class="built_in">print</span>(addr)</span><br></pre></td></tr></table></figure>
<h2 id="什么是断言-assert-？">什么是断言(assert)？</h2>
<p><strong>assert 断言</strong>——声明其布尔值必须为真判定，发生异常则为假。</p>
<h2 id="Python-是如何进行类型转换的？">Python 是如何进行类型转换的？</h2>
<p>内建函数(build-in)封装了各种转换函数，可以使用目标类型关键字强制类型转换<br>
进制之间的转换可以用<code>int(str, base='n')</code>将特定进制的字符串转换为十进制，再用相应的进制转换函数将十进制转换为目标进制。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hex to decimal</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(<span class="string">&#x27;0xf&#x27;</span>, <span class="number">16</span>))  <span class="comment"># 15</span></span><br><span class="line"><span class="comment"># binary to decimal</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(<span class="string">&#x27;10100111110&#x27;</span>, <span class="number">2</span>))  <span class="comment"># 1342</span></span><br><span class="line"><span class="comment"># octonary to decimal</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(<span class="string">&#x27;17&#x27;</span>, <span class="number">8</span>))  <span class="comment"># 15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># decimal to hex</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">hex</span>(<span class="number">1033</span>))  <span class="comment"># 0x409</span></span><br><span class="line"><span class="comment"># binary to hex</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">hex</span>(<span class="built_in">int</span>(<span class="string">&#x27;101010&#x27;</span>, <span class="number">2</span>))) <span class="comment"># 0x2a</span></span><br><span class="line"><span class="comment"># oct to hex</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">hex</span>(<span class="built_in">int</span>(<span class="string">&#x27;17&#x27;</span>, <span class="number">8</span>)))  <span class="comment"># 0xf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># decimal to binary</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">bin</span>(<span class="number">10</span>))  <span class="comment"># 0b1010</span></span><br><span class="line"><span class="comment"># hex to bin</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">bin</span>(<span class="built_in">int</span>(<span class="string">&#x27;0xff&#x27;</span>, <span class="number">16</span>)))  <span class="comment"># 0b11111111</span></span><br><span class="line"><span class="comment"># oct to bin</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">bin</span>(<span class="built_in">int</span>(<span class="string">&#x27;17&#x27;</span>,<span class="number">8</span>)))  <span class="comment"># 0b1111</span></span><br></pre></td></tr></table></figure>
<h2 id="提高-Python-运行效率的方法？">提高 Python 运行效率的方法？</h2>
<ol>
<li>
<p>使用生成器，因为可以节约大量内存</p>
</li>
<li>
<p>循环代码优化，避免过多重复代码的执行</p>
</li>
<li>
<p>核心模块用 Cython PyPy 等，提高效率</p>
</li>
<li>
<p>多进程, 多线程, 协程</p>
</li>
<li>
<p>多个 if elif 条件判断，可以把最有可能先发生的条件放到前面写，这样可以减少程序判断的次数，提高效率</p>
</li>
</ol>
<h2 id="Python-中any-和all-方法">Python 中<code>any()</code>和<code>all()</code>方法</h2>
<ul>
<li><code>any()</code>：只要可迭代对象中有一个元素为真就为真</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">any</span>(<span class="params">iterable</span>):</span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> iterable:</span><br><span class="line">        <span class="keyword">if</span> element:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>all()</code>：可迭代对象中所有的判断项返回都是真，结果才为真</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">all</span>(<span class="params">iterable</span>):</span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> iterable:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> element:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h2 id="Python-为假的变量">Python 为假的变量</h2>
<p>0，空字符串，空列表，空字典，空元组，None，False</p>
]]></content>
  </entry>
  <entry>
    <title>深度学习知识点汇总</title>
    <url>/2022/06/01/7bb85f0eb3694e50b292fbf368c0b35e/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="为什么二分类不用-MSE-损失函数？">为什么二分类不用 MSE 损失函数？</h2>
<p>对于二分类问题，损失函数不采用均方误差（Mean Squared Error，MSE）至少可以从两个角度来分析。</p>
<p>均方差 Mean Squared Error (MSE) 损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。其基本形式如下</p>
<p>$$<br>
J_{M S E}=\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>
$$</p>
<h3 id="数据分布角度">数据分布角度</h3>
<p>首先，使用 MSE 意味着假设数据采样误差是遵循正态分布的。用贝叶斯门派的观点来看，意味着作了高斯先验的假设。实际上，可以分为两类（即二分类）的数据集是遵循伯努利分布。</p>
<p>如果假设误差遵循正态分布，并使用最大似然估计（Maximum Likelihood Estimation，MLE），我们将得出 MSE 正是用于优化模型的损失函数。</p>
<p>首先，正态/高斯分布$\mathcal{N}$由两个参数$(\mu, \sigma)$定义，训练数据$(x, y)$包括特征$x$和实际观测值$y$。简单来说，每当我们采样数据时，观测值有时会与真实值相匹配，有时观测值会因某些误差而失真。我们假设所有观测到的数据都带有一定的误差（即$\epsilon$），并且误差遵循均值为$0$，方差未知的正态分布。我们可以这样来看，实际观测值$y$通常围绕待预测的目标值$\hat{y}$呈正态分布。</p>
<p>在一定的假设下， 我们可以通过最大化似然（MLE）推导出均方差损失的形式。假设模型预测值与真实值之间的误差服从高斯分布$(\mu = 0, \sigma = 1)$，则给定一个$x_i$，模型输出真实值$y_i$的概率为:</p>
<p>$$<br>
p\left(y_{i} \mid x_{i}\right)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}-\hat{y_{i}}\right)^{2}}{2}\right)<br>
$$</p>
<p>进一步我们假设数据集中 N 个样本点之间相互独立，则给定所有$x$输出所有真实值$y$的概率，即似然 Likelihood，为所有$p(y_i \mid x_i)$的累乘:</p>
<p>$$<br>
L(x, y)=\prod_{i=1}^{N} \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}\right)<br>
$$</p>
<p>通常为了方便计算，我们最大化对数似然函数（maximize log-likelihood）</p>
<p>$$<br>
LL(x, y) = log(L(x,y)) = -\frac{N}{2} \log 2 \pi-\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>
$$</p>
<p>去掉无关的第一项，然后转化为最小化负对数似然（Minimize Negative Log-Likelihood）</p>
<p>$$<br>
N L L(x, y)=\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}<br>
$$</p>
<p>可以看到这个实际上就是均方差损失的形式。也就是说在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
<p>对于平均绝对误差 Mean Absolute Error (MAE) 是另一类常用的损失函数，也称为 L1 Loss。其基本形式如下:</p>
<p>$$<br>
J_{M A E}=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|<br>
$$</p>
<p>我们也可以通过类似的过程进行推导：<br>
假设模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution$(\mu = 0, b = 1)$，则给定一个 $x_i$ 模型输出真实值$y_i$的概率为:</p>
<p>$$<br>
p\left(y_{i} \mid x_{i}\right)=\frac{1}{2} \exp \left(-\left|y_{i}-\hat{y}_{i}\right|\right)<br>
$$</p>
<p>$$<br>
L(x, y)=\prod_{i=1}^{N} \frac{1}{2} \exp \left(-\left|y_{i}-\hat{y}<em>{i}\right|\right) \<br>
L L(x, y)=N \ln \frac{1}{2}-\sum</em>{i=1}^{N}\left|y_{i}-\hat{y}<em>{i}\right| \<br>
N L L(x, y)=\sum</em>{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|<br>
$$</p>
<h3 id="优化角度">优化角度</h3>
<p>逻辑回归是分类的一种，输出包含 sigmoid（也可以是其他非线性激活函数），而如果还用 mse 做损失函数的话：</p>
<p>$$<br>
\text { Loss }=\frac{1}{2} \sum_{i=1}^{n}\left(y_{i}-\hat{y}<em>{i}\right)^{2}=\frac{1}{2} \sum</em>{i=1}^{n}\left(y_{i}-\sigma\left(w x_{i}+b\right)\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\frac{1}{1+e^{-\left(w x_{i}+b\right)}}\right)^{2}<br>
$$</p>
<p>MSE 函数对于二分类问题来说是非凸的，有多个极值点，所以不适用做损失函数了。<code>sigmoid</code>激活函数的输入很可能直接就在平坦区域，那么导数就几乎是 0，梯度就几乎不会被反向传递，梯度直接消失了。所以 mse 做损失函数的时候最后一层不能用 sigmoid 做激活函数，其他层可以用 sigmoid 做激活函数。。简而言之，如果使用 MSE 损失函数训练二分类模型，则不能保证将损失函数最小化。这是因为 MSE 函数期望实数输入在范围$(-\infin, \infin)$中，而二分类模型通过 <code>Sigmoid</code> 函数输出范围为$(0, 1)$的概率。</p>
<p>当将一个无界的值传递给 MSE 函数时，在目标值 处有一个明确最小值的情况下，会形成一条漂亮的 U 形（凸）曲线。另一方面，当将来自 Sigmoid 等函数的有界值传递给 MSE 函数时，可能会导致结果并不是凸的。</p>
<p>当然，用其他损失函数只能保证在第一步不会直接死掉，反向传播如果激活函数和归一化做得不好，同样会梯度消失。所以从梯度这个原因说 mse 不好不是很正确。</p>
<h3 id="用交叉熵损失函数后还会有梯度消失的问题吗？">用交叉熵损失函数后还会有梯度消失的问题吗？</h3>
<p>梯度消失问题存在 2 个地方：</p>
<ul>
<li>
<p>1.损失函数对权值 w 求导，这是误差反向传播的第一步，mse 的损失函数会在损失函数求导这一个层面上就导致梯度消失；所以使用交叉熵损失函数。</p>
</li>
<li>
<p>2.误差反向传播时，链式求导也会使得梯度消失。使用交叉熵损失函数也不能避免反向传播带来的梯度消失，此时规避梯度消失的方法：</p>
<ul>
<li>ReLU 等激活函数；</li>
<li>输入归一化、每层归一化；</li>
<li>网络结构上调整，比如 LSTM、GRU 等深度神经网络，不管用什么损失函数，隐含层的激活函数如果用 sigmoid，肯定会梯度消失，训练无效。如果是浅层神经网络，影响可能不是很大，和神经网络的输入有关，如果输入经过了归一化，结果靠谱，如果没有归一化，梯度也直接消失，训练肯定失败。</li>
</ul>
</li>
</ul>
<p>损失函数和激活函数决定的是模型会不会收敛，也影响训练速度；优化器决定的是模型能不能跳出局部极小值、跳出鞍点、能不能快速下降这些问题的。</p>
<h2 id="LSTM-原理">LSTM 原理</h2>
<p>LSTM 是循环神经网络 RNN 的变种，其包含三个门，分别是<code>输入门</code>、<code>遗忘门</code>和<code>输出门</code>。</p>
<h3 id="LSTM-与-GRU-的区别">LSTM 与 GRU 的区别</h3>
<ul>
<li>GRU 只有两个门(update 和 reset)，LSTM 有三个门(forget, input, output)，GRU 直接将<code>hidden state</code>传给下一个单元，而 LSTM 用 memory cell 把<code>hidden state</code>包装起来。</li>
<li>LSTM 和 GRU 的性能在很多任务上不分伯仲。</li>
<li>GRU 的参数更少，因此更容易收敛，但在大数据集上，LSTM 性能表现更好。</li>
</ul>
<h2 id="Transformer-的原理">Transformer 的原理</h2>
<p>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的 Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization 层。</p>
<h3 id="Transformer-的计算公式，QKV-怎么得到">Transformer 的计算公式，QKV 怎么得到</h3>
<p>QKV 分别是由输入 X 通过线性变换得到的。</p>
<p>$$<br>
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>
$$</p>
<h3 id="multi-head-的作用">multi-head 的作用</h3>
<p>多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用多个卷积核的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。</p>
<h3 id="residual-连接">residual 连接</h3>
<p>在 transformer 的 encoder 和 decoder 中，各用到了 6 层的 attention 模块，每一个 attention 模块又和一个 FeedForward 层（简称 FFN）相接。对每一层的 attention 和 FFN，都采用了一次残差连接，即把每一个位置的输入数据和输出数据相加，使得 Transformer 能够有效训练更深的网络。在残差连接过后，再采取 Layer Nomalization 的方式。</p>
<p>resnet 的思想：残差模块能让训练变得更加简单，如果输入值和输出值的差值过小，那么可能梯度会过小，导致出现梯度小时的情况，残差网络的好处在于当残差为 0 时，改成神经元只是对前层进行一次线性堆叠，使得网络梯度不容易消失，性能不会下降。</p>
<h2 id="relu-的公式和优缺点，relu-在-0-的位置可导吗，不可导怎么处理">relu 的公式和优缺点，relu 在 0 的位置可导吗，不可导怎么处理</h2>
<p>$$<br>
relu(x) = max(x, 0)<br>
$$</p>
<p>优点：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快，求导方便</li>
<li>加速网络训练</li>
</ul>
<p>缺点包括：</p>
<ul>
<li>由于负数部分恒为 0，会导致一些神经元无法激活</li>
<li>输出不是以 0 为中心</li>
</ul>
<p>由于 relu 函数的左导数和右导数不相等，所以其不可导。针对这种类型的激活函数，可以使用次梯度来解决。</p>
<p>次梯度方法(subgradient method)是传统的梯度下降方法的拓展，用来处理不可导的凸函数。它的优势是比传统方法处理问题范围大，劣势是算法收敛速度慢。但是，由于它对不可导函数有很好的处理方法，所以学习它还是很有必要的。</p>
<p>$$<br>
c \le \frac{f(x) - f(x_0)}{x - x_0}<br>
$$</p>
<p>对于 relu 函数，当 x&gt;0 时，导数为 1，当 x&lt;0 时导数为 0。因此 relu 函数在 x=0 的次梯度 c ∈ [ 0 , 1 ]，c 可以取[0,1]之间的任意值。</p>
<h2 id="如何处理神经网络中的过拟合问题？">如何处理神经网络中的过拟合问题？</h2>
<ul>
<li>L1/L2 正则化</li>
<li>dropout</li>
<li>data argumentation</li>
<li>early stop</li>
</ul>
<h3 id="L1-L2-正则化的区别">L1/L2 正则化的区别</h3>
<p>在 L1 规范化中，权重通过一个常量向 0 进行缩小；在 L2 规范化中，权重通过一个和 w 成比例的量进行缩小<br>
当一个特定的权重绝对值|w|很大时，L1 规范化的权重缩小要比 L2 规范化小很多；当一个特定的权重绝对值|w|很小时，L1 规范化的权重缩小要比 L2 规范化大很多<br>
L1 规范化倾向于聚集网络的权重在相对少量的高重要度的连接上，而其他权重会被驱使向 0 接近<br>
在 w=0 处偏导数不存在，此时使用无规范化的随机梯度下降规则，因为规范化的效果是缩小权重，不能对一个已经是 0 的权重进行缩小</p>
<h2 id="神经网络都有哪些正则化操作？BN-和-LN-分别用在哪？">神经网络都有哪些正则化操作？BN 和 LN 分别用在哪？</h2>
<p>最常用的正则化技术是 dropout，随机的丢掉一些神经元。还有数据增强，早停，L1 正则化，L2 正则化等。</p>
<ul>
<li>Batch Normalization 是对这批样本的同一维度特征做归一化.</li>
<li>Layer Normalization 是对这单个样本的所有维度特征做归一化。</li>
</ul>
<p>BN 用在图像较多，LN 用在文本较多。</p>
<h2 id="Attention-和全连接的区别是啥？">Attention 和全连接的区别是啥？</h2>
<p>Attention 的最终输出可以看成是一个“在关注部分权重更大的全连接层”。但是它与全连接层的区别在于，注意力机制可以利用输入的特征信息来确定哪些部分更重要。</p>
<p>全连接的作用的是对一个实体进行从一个特征空间到另一个特征空间的映射，而注意力机制是要对来自同一个特征空间的多个实体进行整合。全连接的权重对应的是一个实体上的每个特征的重要性，而注意力机制的输出结果是各个实体的重要性。比如说，一个单词“love”在从 200 维的特征空间转换到 100 维的特征空间时，使用的是全连接，不需要注意力机制，因为特征空间每一维的意义是固定的。而如果我们面对的是词组“I love you”，需要对三个 200 维的实体特征进行整合，整合为一个 200 维的实体，此时就要考虑到实体间的位置可能发生变化，我们下次收到的句子可能是“love you I”，从而需要一个与位置无关的方案。</p>
<h2 id="BP-算法及推导">BP 算法及推导</h2>
<p><strong>BP 算法是“误差反向传播”的简称</strong>，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。</p>
<p>反向传播要求有<strong>对每个输入值期望得到的已知输出，来计算损失函数的梯度</strong>。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的 Delta 规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的<strong>激励函数可微</strong>。</p>
<h3 id="BP-算法">BP 算法</h3>
<p>BP 网络的结构降法的基础上。BP 网络的输入输出关系实质上是一种映射关系：一个 输入 m 输出的 BP 神经网络所完成的功能是从 一维欧氏空间向 m 维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是 BP 算法得以应用的基础。</p>
<p>反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。</p>
<p>BP 算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。</p>
<h3 id="推导">推导</h3>
<p>$$<br>
z^l = W^la^{l-1} + b^l \<br>
a^l = \sigma(z^l)<br>
$$</p>
<p>其中，$z^l$表示第$l$层$(l=1,2,…,L)$经过激活函数之前的输出，而$a^l$表示第 l 层经过激活函数之后的输出，$\sigma$表示激活函数。</p>
<p>定义误差函数:</p>
<p>$$<br>
C=\frac{1}{2}\left|a^{L}-y\right|_{2}^{2}<br>
$$</p>
<p>其中，$L$代表多层感知机总的层数， $a^L$表示多层感知机第$L$层经过激活函数后的输出，即神经网络所预测的输出值。而 y 是训练数据中对应输入 x 实际的输出值 y。</p>
<p>为了方便进一步的计算推导，以及避免重复计算，我们引入一个中间量$\delta^l$，我们称它为第 l 层的 delta 误差，表示误差函数对于神经网络第 l 层激活前输出值的偏导数，即$\delta^l = \frac{\partial C}{\partial z^l}$。</p>
<p>$$<br>
\delta^{L}=\frac{\partial C}{\partial z^{L}}=\frac{\partial C}{\partial a^{L}} \frac{\partial a^{L}}{\partial z^{L}}=\left(a^{L}-y\right) \odot \sigma^{\prime}\left(z^{L}\right)<br>
$$</p>
<p>求得了输出层的 delta 误差，误差函数 C 对于输出层参数的导数，即对权重矩阵以及偏置项的导数可通过输出层的 delta 误差求得如下，这里使用了求导的链式法则</p>
<p>$$<br>
\frac{\partial C}{\partial W^{L}}=\frac{\partial C}{\partial z^{L}} \frac{\partial z^{L}}{\partial W^{L}}=\delta^{L}\left(a^{L-1}\right)^{T} \<br>
\frac{\partial C}{\partial b^{L}}=\frac{\partial C}{\partial z^{L}} \frac{\partial z^{L}}{\partial b^{L}}=\delta^{L} \odot \mathbf{1}=\delta^{L}<br>
$$</p>
<p>我们可以很容易看到，一旦求出了当前层的 delta 误差，误差函数对当前层各参数的导数便可以相应的求出。</p>
<p>得到了最后一层的 delta 误差，我们接下来需要将 delta 误差逆向传播，即不断地根据后一层的 delta 误差求得前一层的 delta 误差，最终求得每一层的 delta 误差。其实在这里我们主要利用的是求导的链式法则。假设我们已经求得第 l+1 层的 delta 误差，我们可以将第 l 层的 delta 误差表示如下</p>
<p>$$<br>
\delta^{l}=\frac{\partial C}{\partial z^{l}}=\frac{\partial C}{\partial z^{l+1}} \frac{\partial z^{l+1}}{\partial z^{l}}=\delta^{l+1} \frac{\partial z^{l+1}}{\partial z^{l}}<br>
$$</p>
<p>因为</p>
<p>$$<br>
z^{l + 1} = W^{l + 1} a^l + b^{l + 1} = W^{l + 1}\sigma(z^l) + b^{l + 1}<br>
$$</p>
<p>所以</p>
<p>$$<br>
\delta^{l} = (W^{l + 1})^T\delta^{l + 1} \odot\sigma^{\prime}(z^l)<br>
$$</p>
<p>在求得每一层的 delta 误差后，我们可以很容易地求出误差函数 C 对于每一层参数的梯度：</p>
<p>$$<br>
\frac{\partial C}{\partial W^{l}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial W^{l}}=\delta^{l}\left(a^{l-1}\right)^{T} \<br>
\frac{\partial C}{\partial b^{l}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial b^{l}}=\delta^{l} 1=\delta^{l}<br>
$$</p>
<p>最后我们可以通过梯度下降法来对每一层的参数进行更新：</p>
<p>$$<br>
W^{l} =W^{l}-\eta \frac{\partial C}{\partial W^{l}} \<br>
b^{l} =b^{l}-\eta \frac{\partial C}{\partial b^{l}}<br>
$$</p>
<h2 id="梯度消失和梯度爆炸的问题是如何产生的？如何解决？">梯度消失和梯度爆炸的问题是如何产生的？如何解决？</h2>
<p>由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用 sigmoid, tanh 激活函数的话，由于导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 w 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。</p>
<p>因为 sigmoid 导数最大为 1/4，故只有当 abs(w)&gt;4 时才可能出现梯度爆炸，因此最普遍发生的是梯度消失问题。</p>
<p>解决方法通常包括</p>
<ul>
<li>使用 ReLU 等激活函数，梯度只会为 0 或者 1，每层的网络都可以得到相同的更新速度</li>
<li>采用 LSTM</li>
<li>进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内</li>
<li>使用正则化，这样会限制参数 w 的大小，从而防止梯度爆炸</li>
<li>设计网络层数更少的网络进行模型训练</li>
<li>batch normalization</li>
</ul>
<h2 id="语言模型中，Bert-为什么在-masked-language-model-中采用了-80-、10-、10-的策略？">语言模型中，Bert 为什么在 masked language model 中采用了 80%、10%、10%的策略？</h2>
<p>如果训练的时候 100%都是 Mask，那么在 fine-tune 的时候，所有的词时候已知的，不存在[Mask]，那么模型就只知道根据其他词的信息来预测当前词，而不会直接利用这个词本身的信息，会凭空损失一部分信息，对下游任务不利。</p>
<p>还有 10% random token 是因为如果都用原 token，模型在预训练时可能会偷懒，不去建模单词间的依赖关系，直接照抄当前词</p>
<p>[MASK] 是以一种显式的方式告诉模型『这个词我不告诉你，你自己从上下文里猜』，从而防止信息泄露。如果 [MASK] 以外的部分全部都用原 token，模型会学到『如果当前词是 [MASK]，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』。这样一来，在 finetune 阶段，所有词都是正常单词，模型就照抄所有词，不提取单词间的依赖关系了。</p>
<p>以一定的概率填入 random token，就是让模型时刻堤防着，在任意 token 的位置都需要把当前 token 的信息和上下文推断出的信息相结合。这样一来，在 finetune 阶段的正常句子上，模型也会同时提取这两方面的信息，因为它不知道它所看到的『正常单词』到底有没有被动过手脚的。</p>
<h2 id="Bert-现有的问题有哪些？">Bert 现有的问题有哪些？</h2>
<ul>
<li>Bert 模型过于庞大，参数太多，无论是 feature-based approach 还是 fine-tune approach 都很慢；而且因为表示是上下文相关的，上线的应用需要实时处理，时间消耗太大；</li>
<li>Bert 给出来的中文模型中，是以字为基本单位的，很多需要词向量的应用无法直接使用；同时该模型无法识别很多生僻词，都是 UNK；</li>
<li>Bert 模型作为自回归模型，由于模型结构的问题，无法给出句子概率值</li>
</ul>
<h2 id="非平衡数据集的处理方法有哪些？">非平衡数据集的处理方法有哪些？</h2>
<ul>
<li>采用更好的评价指标，例如 F1、AUC 曲线等，而不是 Recall、Precision</li>
<li>进行过采样，随机重复少类别的样本来增加它的数量；</li>
<li>进行欠采样，随机对多类别样本降采样</li>
<li>通过在已有数据上添加噪声来生成新的数据</li>
<li>修改损失函数，添加新的惩罚项，使得小样本的类别被判断错误的损失增大，迫使模型重视小样本的数据</li>
<li>使用组合/集成方法解决样本不均衡，在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果；</li>
</ul>
<h2 id="交叉熵损失与-KL-散度的区别">交叉熵损失与 KL 散度的区别</h2>
<p>KL 散度是相对熵(relative entropy)，用来衡量两个概率分布之间的差异，对于两个概率分布 $p(x), q(x)$ ,其中 $p(x)$ 是真实概率分布，而 $q(x)$ 是数据计算得到的概率分布，其相对熵的计算公式为:</p>
<p>$$<br>
K L(p | q)=-\int p(x) \ln q(x) d x-\left(\int p(x) \ln p(x) d x\right)<br>
$$</p>
<p>当且仅当$p(x) == q(x)$时，其值为 0；其前半部分$-\int p(x) \ln q(x) d x$为交叉熵的表达公式，或者说相对熵等于交叉熵减去数据真实分布的熵。<br>
由于真实的概率分布是固定的，因此公式中的后半部分是常数，所以优化交叉熵损失也等效于优化相对熵损失。</p>
<h2 id="熵、条件熵、互信息、相对熵">熵、条件熵、互信息、相对熵</h2>
<h3 id="熵">熵</h3>
<p>熵是一个随机变量不确定性的度量。对于一个离散型变量，定义为：</p>
<p>$$<br>
H(x) = -\sum_{x\in X}p(x)\log{p(x)}<br>
$$</p>
<p>一个随机性变量的熵越大，就表示不确定性越大，也就是说随机变量包含的信息量越大。<br>
熵只依赖于$X$的分布，与$X$的取值无关。</p>
<h3 id="条件熵">条件熵</h3>
<p>条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性，$H(Y|X)$ 定义为在给定条件 $X$ 下，$Y$ 的条件概率分布的熵对 $X$ 的数学期望：</p>
<p>$$<br>
H(Y\mid X) = \sum_{x\in X}p(x)H(Y\mid X=x)<br>
$$</p>
<h3 id="互信息">互信息</h3>
<p>互信息表示在得知 $Y$ 后，原来信息量减少了多少。</p>
<p>$$<br>
I(X;Y) = \sum_{x\in X}\sum_{y\in Y}p(x, y)\log{\frac{p(x, y)}{p(x)p(y)}} \<br>
I(X;Y) = H(X) - H(X\mid Y) = H(Y) - H(Y\mid X)<br>
$$</p>
<p>如果$X$与$Y$相互独立，则互信息为 0。</p>
<h3 id="相对熵">相对熵</h3>
<p>KL 散度（Kullback-Leibler divergence）和相对熵是等价的，KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度越小表示两个分布越接近。也就是说 KL 散度是不对称的，且 KL 散度的值是非负数。</p>
<p>$$<br>
D_{KL}(P//Q) = -\int{p(x)\log{q(x)}dx} - (- \int{p(x)\log{q(x)}dx}) = -\int{p(x)\log[\frac{q(x)}{p(x)}]dx}<br>
\<br>
D_{KL}(P//Q) = - \sum_{x\in X}p(x)log(q(x)) - (-\sum_{x\in X}p(x)log(p(x)))<br>
$$</p>
<p>显然$D_{KL}(P//Q)$不等于$D_{KL}(Q//P)$，即 KL 散度不是一个对称量。</p>
<p>JS 散度是基于 KL 散度的变种，度量了两个概率分布的相似度，解决了 KL 散度的非对称问题。如果两个分配 P,Q 离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 0。梯度消失了。</p>
<p>$$<br>
J S\left(P_{1} | P_{2}\right)=\frac{1}{2} K L\left(P_{1} | \frac{P_{1}+P_{2}}{2}\right)+\frac{1}{2} K L\left(P_{2} | \frac{P_{1}+P_{2}}{2}\right)<br>
$$</p>
<h2 id="机器学习泛化能力评测指标">机器学习泛化能力评测指标</h2>
<p>泛化能力是模型对未知数据的预测能力。</p>
<h3 id="分类问题">分类问题</h3>
<ul>
<li>
<p>准确率：分类正确的样本占总样本的比例<br>
准确率的缺陷：当正负样本不平衡比例时，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。</p>
</li>
<li>
<p>召回率：分类正确的正样本个数占实际的正样本个数的比例。</p>
</li>
<li>
<p>F1 score：是精确率和召回率的调和平均数，综合反应模型分类的性能。<br>
Precision 值和 Recall 值是既矛盾又统一的两个指标，为了提高 Precision 值，分类器需要尽量在“更有把握”时才把样本预测为正样本，但此时往往会因为过于保 守而漏掉很多“没有把握”的正样本，导致 Recall 值降低。</p>
<p>ROC 曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性 率（True Positive Rate，TPR）。FPR 和 TPR 的计算方法分别为</p>
</li>
<li>
<p>精确度（precision）/查准率：TP/（TP+FP）=TP/P 预测为真中，实际为正样本的概率 s</p>
</li>
<li>
<p>召回率（recall）/查全率：TP/（TP+FN） 正样本中，被识别为真的概率</p>
</li>
<li>
<p>假阳率（False positive rate）：FPR = FP/(FP+TN) 负样本中，被识别为真的概率</p>
</li>
<li>
<p>真阳率（True positive rate）：TPR = TP/（TP+FN） 正样本中，能被识别为真的概率</p>
</li>
<li>
<p>准确率（accuracy）：ACC =（TP+TN）/(P+N) 所有样本中，能被正确识别的概率</p>
<p>上式中，P 是真实的正样本的数量，N 是真实的负样本的数量，TP 是 P 个正样本中被分类器预测为正样本的个数，FP 是 N 个负样本中被分类器预测为正样本的个数。</p>
<p>AUC：AUC 是 ROC 曲线下面的面积，AUC 可以解读为从所有正例中随机选取一个样本 A，再从所有负例中随机选取一个样本 B，分类器将 A 判为正例的概率比将 B 判为正例的概率大的可能性。AUC 反映的是分类器对样本的排序能力。AUC 越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</p>
</li>
<li>
<p>F1-score：在多分类问题中，如果要计算模型的 F1-score，则有两种计算方式，分别为<code>微观micro-F1</code>和<code>宏观macro-F1</code>，这两种计算方式在二分类中与 F1-score 的计算方式一样，所以在二分类问题中，计算 micro-F1=macro-F1=F1-score，micro-F1 和 macro-F1 都是多分类 F1-score 的两种计算方式。</p>
<p>micro-F1：计算方法：先计算所有类别的总的 Precision 和 Recall，然后计算出来的 F1 值即为 micro-F1；</p>
<p>使用场景：在计算公式中考虑到了每个类别的数量，所以适用于数据分布不平衡的情况；但同时因为考虑到数据的数量，所以在数据极度不平衡的情况下，数量较多数量的类会较大的影响到 F1 的值；</p>
<p>marco-F1：计算方法：将所有类别的 Precision 和 Recall 求平均，然后计算 F1 值作为 macro-F1；</p>
<p>使用场景：没有考虑到数据的数量，所以会平等的看待每一类（因为每一类的 precision 和 recall 都在 0-1 之间），会相对受高 precision 和高 recall 类的影响较大。</p>
</li>
</ul>
<h3 id="回归问题">回归问题</h3>
<ul>
<li>
<p>RMSE(Root Mean Square Error, 均方根误差)：</p>
<p>$$<br>
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i} - y_i)^2}<br>
$$</p>
<p>RMSE 经常被用来衡量回归模型的好坏。RMSE 能够很好地反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的离群点（Outlier）时，即使离群点 数量非常少，也会让 RMSE 指标变得很差。</p>
</li>
<li>
<p>MAPE(Mean Absolute Percentage Error)：</p>
<p>$$<br>
M A P E=\frac{100 %}{n} \sum_{i=1}^{n}\left|\frac{\hat{y}<em>{i}-y</em>{i}}{y_{i}}\right|<br>
$$</p>
<p>引入别的评价指标，MAPE，平均绝对百分比误差。相比 RMSE，MAPE 相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。</p>
</li>
</ul>
<h2 id="bagging、boosting、stacking-的异同">bagging、boosting、stacking 的异同</h2>
<h3 id="Bagging-算法-套袋发">Bagging 算法(套袋发)</h3>
<p>bagging 的算法过程如下：</p>
<p>从原始样本集中使用 Bootstraping 方法随机抽取 n 个训练样本，共进行 k 轮抽取，得到 k 个训练集（k 个训练集之间相互独立，元素可以有重复）。</p>
<p>对于 n 个训练集，我们训练 k 个模型，（这个模型可根据具体的情况而定，可以是决策树，knn 等）</p>
<p>对于分类问题：由投票表决产生的分类结果；对于回归问题，由 k 个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。</p>
<h3 id="Boosting（提升法）">Boosting（提升法）</h3>
<p>boosting 的算法过程如下：</p>
<p>对于训练集中的每个样本建立权值 wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。</p>
<p>同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost 给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。)</p>
<h3 id="Bagging-和-Boosting-的主要区别">Bagging 和 Boosting 的主要区别</h3>
<p>样本选择上: Bagging 采取 Bootstraping 的是随机有放回的取样，Boosting 的每一轮训练的样本是固定的，改变的是买个样的权重。</p>
<p>样本权重上：Bagging 采取的是均匀取样，且每个样本的权重相同，Boosting 根据错误率调整样本权重，错误率越大的样本权重会变大</p>
<p>预测函数上：Bagging 所以的预测函数权值相同，Boosting 中误差越小的预测函数其权值越大。</p>
<p>并行计算: Bagging 的各个预测函数可以并行生成;Boosting 的各个预测函数必须按照顺序迭代生成。</p>
<h2 id="Focal-loss">Focal loss</h2>
<p>针对样本不均衡问题提的出损失函数。</p>
<p>二分类交叉熵损失函数：</p>
<p>$$<br>
CE = \left{\begin{array}{rlr}<br>
-\log §, &amp; \text { if } &amp; y=1 \<br>
-\log (1-p), &amp; \text { if } &amp; y=0<br>
\end{array}\right.<br>
$$</p>
<p>为了解决正负样本不平衡的问题，在交叉熵损失的前面加上一个参数$\alpha$：</p>
<p>$$<br>
CE = \left{\begin{array}{rlr}<br>
-\alpha\log §, &amp; \text { if } &amp; y=1 \<br>
-(1-\alpha)\log (1-p), &amp; \text { if } &amp; y=0<br>
\end{array}\right.<br>
$$</p>
<p>尽管$\alpha$平衡了正负样本，但对难易样本的不平衡没有任何帮助。而实际上，目标检测中大量的候选目标都是像下图一样的易分样本。这些样本的损失很低，但是由于数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是 GHM 的主要改进对象）</p>
<p>Focal Loss 的思想：把高置信度§样本的损失再降低一些。</p>
<p>$$<br>
FL = \left{\begin{array}{rll}<br>
-\alpha(1-p)^{\gamma} \log §, &amp; \text { if } &amp; y=1 \<br>
-(1-\alpha) p^{\gamma} \log (1-p), &amp; \text { if } &amp; y=0<br>
\end{array}\right.<br>
$$</p>
]]></content>
  </entry>
  <entry>
    <title>超参数网络搜索</title>
    <url>/2022/06/01/85ae70fa5bc34120a61a5026911d6152/</url>
    <content><![CDATA[<p>[toc]</p>
<p>由于各个新模型在执行交叉验证的过程中间是相互独立的，所以我们可以充分利用多核处理器（Multicore processor）甚至是分布式的计算资源来从事并行搜索，节省运算时间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入20类新闻文本抓取器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">news = fetch_20newsgroups(subset=<span class="string">&#x27;all&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分割数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(news.data[:<span class="number">3000</span>], news.target[:<span class="number">3000</span>], test_size=<span class="number">0.25</span>, random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入支持向量机（分类）模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入TF-IDF文本抽取器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用Pipeline进行系统搭建</span></span><br><span class="line">clf = Pipeline([(<span class="string">&#x27;vect&#x27;</span>, TfidfVectorizer(stop_words=<span class="string">&#x27;english&#x27;</span>)), (<span class="string">&#x27;svc&#x27;</span>, SVC())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置 4*3=12组</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;svc__gamma&#x27;</span>:np.logspace(-<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>), <span class="string">&#x27;svc__C&#x27;</span>:np.logspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化配置，n_job=-1代表使用计算机的全部CPU</span></span><br><span class="line">gs = GridSearchCV(estimator=clf, param_grid=parameters, verbose=<span class="number">2</span>, refit=<span class="literal">True</span>, cv=<span class="number">3</span>, n_jobs=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="网络搜索函数参数：">网络搜索函数参数：</h2>
<p><code>class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’)</code></p>
<ul>
<li>
<p>estimator</p>
<p>选择使用的分类器，并且传入除需要确定最佳的参数之外的其他参数。每一个分类器都需要一个 scoring 参数，或者 score 方法：</p>
<p>estimator=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features=‘sqrt’,random_state=10),</p>
</li>
<li>
<p>param_grid</p>
<p>需要最优化的参数的取值，值为字典或者列表，例如：param_grid =param_test1，param_test1 = {‘n_estimators’:range(10,71,10)}。</p>
</li>
<li>
<p>scoring=None</p>
<p>模型评价标准，默认 None,这时需要使用 score 函数；或者如 scoring=‘roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是 None，则使用 estimator 的误差估计函数。具体值的选取看本篇第三节内容。</p>
</li>
<li>
<p>fit_params=None</p>
</li>
<li>
<p>n_jobs=1</p>
<p>n_jobs: 并行数，int：个数,-1：跟 CPU 核数一致, 1:默认值</p>
</li>
<li>
<p>iid=True</p>
<p>iid:默认 True,为 True 时，默认为各个样本 fold 概率分布一致，误差估计为所有样本之和，而非各个 fold 的平均。</p>
</li>
<li>
<p>refit=True</p>
<p>默认为 True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次 fit 一遍全部数据集。</p>
</li>
<li>
<p>cv=None</p>
<p>交叉验证参数，默认 None，使用三折交叉验证。指定 fold 数量，默认为 3，也可以是 yield 训练/测试数据的生成器。</p>
</li>
<li>
<p>verbose=0, scoring=None</p>
<p>verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出。</p>
</li>
<li>
<p>pre_dispatch=‘2*n_jobs’</p>
<p>指定总共分发的并行任务数。当 n_jobs 大于 1 时，数据将在每个运行点进行复制，这可能导致 OOM，而设置 pre_dispatch 参数，则可以预先划分总共的 job 数量，使数据最多被复制 pre_dispatch 次</p>
</li>
<li>
<p>error_score=’raise’</p>
</li>
<li>
<p>return_train_score=’warn’</p>
<p>如果“False”，cv_results_属性将不包括训练分数</p>
<p>回到 sklearn 里面的 GridSearchCV，GridSearchCV 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%time</span><br><span class="line">gs_result = gs.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Wall time: 0 ns</span><br><span class="line">Fitting 3 folds for each of 12 candidates, totalling 36 fits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.</span><br><span class="line">[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  1.5min finished</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210113161419243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="返回结果">返回结果</h2>
<ul>
<li>
<p>cv_results_ : dict of numpy (masked) ndarrays</p>
<p>具有键作为列标题和值作为列的 dict，可以导入到 DataFrame 中。注意，“params”键用于存储所有参数候选项的参数设置列表。</p>
</li>
<li>
<p>best_estimator_ : estimator</p>
<p>通过搜索选择的估计器，即在左侧数据上给出最高分数（或指定的最小损失）的估计器。 如果 refit = False，则不可用。</p>
</li>
<li>
<p>best_score_ : float</p>
<p>best_estimator 的分数</p>
</li>
<li>
<p>best_params_ : dict</p>
<p>在保存数据上给出最佳结果的参数设置</p>
</li>
<li>
<p>best_index_ : int</p>
<p>对应于最佳候选参数设置的索引（cv_results_数组）。<br>
search.cv_results _ [‘params’] [search.best_index_]中的 dict 给出了最佳模型的参数设置，给出了最高的平均分数（search.best_score_）。</p>
</li>
<li>
<p>scorer_ : function</p>
<p>Scorer function used on the held out data to choose the best parameters for the model.</p>
</li>
<li>
<p>n_splits_ : int</p>
<p>The number of cross-validation splits (folds/iterations).</p>
</li>
<li>
<p>grid_scores_：</p>
<p>给出不同参数情况下的评价结果</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(gs_result.score(X_test, y_test))</span><br><span class="line"><span class="built_in">print</span>(gs_result.best_params_, gs.best_score_)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.8226666666666667</span><br><span class="line">&#123;&#x27;svc__C&#x27;: 10.0, &#x27;svc__gamma&#x27;: 0.1&#125; 0.7888888888888889</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>链路层介绍</title>
    <url>/2022/05/31/9296969af88e42cdbeabe44e49cd49d3/</url>
    <content><![CDATA[<p>[toc]</p>
<p>网络层协议的数据单元是 <strong>IP 数据报</strong>，而数据链路层的工作就是把网络层交下来的 <strong>IP 数据报</strong> 封装为<strong>帧（frame）</strong> 发送到链路上，以及把接收到的帧中的数据取出并上交给网络层。 为达到这一目的，数据链路必须具备一系列相应的功能，主要有：</p>
<ul>
<li>将数据封装为帧（frame），帧是数据链路层的传送单位；</li>
<li>控制帧的传输，包括处理传输差错，调节发送速率与接收方相匹配；</li>
<li>在两个网络实体之间提供数据链路通路的建立、维持和释放的管理。</li>
</ul>
<p>数据帧的结构是这样的：</p>
<p><img src="/resource/04f183eb7ab64e18bfab433065d25f46.png" alt="2022-05-16-22-49-24.png"></p>
<p><strong>知识点</strong>:</p>
<ul>
<li>控制帧的传输</li>
<li>以太网</li>
<li>PPP（点对点协议）</li>
<li>SLIP 与 PPP</li>
<li>MTU</li>
</ul>
<h2 id="控制帧的传输">控制帧的传输</h2>
<p><strong>差错控制</strong><br>
通信系统必须具备发现差错的能力，并采取措施纠正之，使差错控制在所能允许的尽可能小的范围内，这就是差错控制过程，也是数据链路层的主要功能之一。</p>
<p><strong>反馈重发</strong><br>
接收方通过对差错编码(奇偶校验码或 CRC 码)的检查，可以判定一帧在传输过程中是否发生了差错。一旦发现差错，一般可以采用<strong>反馈重发</strong>的方法来纠正。这就要求接受方收完一帧后，向发送方反馈一个接收是否正确的信息，使发送方据此做出是否需要重新发送的决定。发送方仅当收到接收方已正确接收的反馈信号后才能认为该帧已经正确发送完毕，否则需要重发直至正确为止。</p>
<p><strong>计时器</strong><br>
如果某一帧发送出现问题，一直不能发送成功，为了避免传输过程停滞不前，通常引入计时器（Timer）来限定接收方发回反馈消息的时间间隔。当发送方发送一帧的同时也启动计时器，若在限定时间间隔内未能收到接收方的反馈信息，即<strong>计时器超时（Timeout）</strong>，则可认为传出的帧已出错或丢失，就要重新发送。</p>
<p><strong>序号</strong><br>
由于同一帧数据可能被重复发送多次，就可能引起接收方多次收到同一帧并将其递交给网络层的情况。为了防止这种情况，可以采用对发送的<strong>帧编号</strong>的方法，即赋予每帧一个序号，从而使接收方能从该序号来区分是新发送来的帧还是重发的帧，以此来确定要不要将接收到的帧递交给网络层。</p>
<p><strong>流量控制</strong><br>
由于收发双方各自使用的设备工作速率和缓冲存储空间的差异，可能出现发送方的发送能力大于接收方接收能力的现象，此时若不对发送方的发送速率做适当的限制，前面来不及接收的帧将被后面不断发送来的帧“淹没”，从而造成帧的丢失而出错。</p>
<p>由此可见，流量控制实际上是<strong>对发送方数据流量的控制</strong>，使其发送速率不超过接收方的速率。所以需要一些规则使得发送方知道在什么情况下可以接着发送下一帧，而在什么情况下必须暂停发送，以等待收到某种反馈信息后再继续发送。这就是流量控制。</p>
<h2 id="以太网">以太网</h2>
<p>以太网(Ether-net)是指 DEC 公司、Intel 公司和 Xerox 公司在 1982 年联合公布的一个<strong>标准</strong>，这个标准里面使用了一种称作 <code>CSMA/CD</code> 的接入方法。而 IEEE802 提供的标准集 802.3(还有一部分定义到了 802.2 中)也提供了一个 <code>CSMA/CD</code> 的标准。</p>
<h2 id="PPP-点对点协议">PPP 点对点协议</h2>
<p><strong>PPP（点到点协议）</strong> 是为在<strong>同等单元</strong>之间传输数据设计的<strong>链路层协议</strong>。这种链路提供<strong>全双工操作</strong>，并按照顺序传递数据。设计目的主要是用来通过拨号或专线方式建立点对点连接发送数据，使其成为各种主机、网桥和路由器之间简单连接的一种共通的解决方案。</p>
<p><strong>点对点协议（PPP）</strong> 为在点对点连接上传输多协议数据包提供了一个标准方法。PPP 最初设计是为两个对等节点之间的 IP 流量传输提供一种封装协议。在 TCP/IP 协议集中它是一种用来同步调制连接的数据链路层协议。</p>
<h2 id="SLIP-与-PPP">SLIP 与 PPP</h2>
<p><strong>SLIP 协议</strong><br>
SLIP 的全称为 Serial Line IP（串行线路 IP）。它是一种对 IP 数据报进行封装的简单形式。</p>
<p>SLIP 协议规定的帧格式规则：</p>
<ul>
<li>IP 数据报以一个称作 END（0xc0）的特殊字符结束。同时为了防止数据报传输之前的线路噪音被误认为是数据报内容，在数据报开始处添加一个 END 字符；</li>
<li>如果 IP 数据报中含有 END 字符，就连续传输 0xdb 和 0xdc 来取代它。0xdb 是 SLIP 的 ESC 字符，但它的值与 ASCⅡ 码中的 ESC（0x1b）不同；</li>
<li>如果 IP 数据报中含有 ESC 字符，就连续传输 0xdb 和 0xdd 来取代它。</li>
</ul>
<p><img src="/resource/6750b44e985f44bc80b68e104f4e85b7.png" alt="2022-05-17-23-07-06.png"></p>
<p>SLIP 的缺陷：</p>
<ul>
<li>每一端必须知道对端的 IP 地址，没有办法把本端 IP 地址传递给对端；</li>
<li>数据帧中无类型字段，当一条串行线路使用 SLIP 时则不能使用其它协议；</li>
<li>SLIP 数据帧中无 checksum，只能依靠上层协议来发现和纠正错误。</li>
</ul>
<p><strong>PPP 协议</strong><br>
PPP 协议修改了 SLIP 协议中的缺陷，包括以下三个部分：</p>
<ul>
<li>PPP 封装 IP 数据报既支持数据为 8 位和无奇偶校验的异步模式，又支持面向比特的同步链接；</li>
<li>通过 LCP（链路控制协议）允许双方进行协商；</li>
<li>通过 NCP（网络控制协议）允许双方在网络层上进行协商。</li>
</ul>
<p>PPP 协议的字符规则与 SLIP 有所不同：</p>
<ul>
<li>PPP 帧以标志字符 0x7e 开始和结束，紧接着是一个值为 0xff 的地址字节，然后是一个值为 0x03 的控制字节；</li>
<li>由于标志字符是 0x7e，当它出现在信息字段中时，需要连续传送 0x7d 和 0x5e 来替代它；</li>
<li>当在信息字段中遇到 0x7d 时，需要连续传送 0x7d 和 0x5d 来替代它。</li>
<li>默认情况下，如果字符的值小于 0x20，需要连续传送 0x7d 和 0x21 来替代它。</li>
</ul>
<p>PPP 与 SLIP 相比具有下列优点：</p>
<ul>
<li>PPP 支持在单根串行线路上运行多种网络层协议；</li>
<li>每一帧都有 CRC 校验；</li>
<li>通信双方可以用 NCP 进行 IP 地址的动态协商；</li>
<li>可以类似于 CSLIP 对 TCP 和 IP 首部进行压缩；</li>
<li>LCP 可以对多个数据链路选项进行设置。</li>
</ul>
<h2 id="MTU">MTU</h2>
<p>为了提供足够快的响应时间，以太网和 IEEE802.3 对数据帧长度都有限制，其最大值分别为 1500 字节和 1492 字节，链路层的这个特性称作 <strong>MTU</strong>，即<strong>最大传输单元</strong>。</p>
<p>当网络层传下来一个 IP 数据报，并且其长度比链路层的 MTU 大，那么网络层就需要对数据报进行分片，使每一片都小于 MTU。</p>
<p>MTU 分为接口 MTU 和路径 MTU：</p>
<ul>
<li>接口 MTU 是指定的接口所允许发送的最大数据长度；</li>
<li>路径 MTU 指两台通信主机路径中最小的 MTU 值。路径 MTU 是不对称的，它在两个方向上不一定一致。</li>
</ul>
<p>用命令 <code>netstat -in</code> 可以查看网络接口的 <strong>MTU</strong></p>
]]></content>
  </entry>
  <entry>
    <title>传输层：TCP协议</title>
    <url>/2022/05/31/969f2855c2214e66b5f04c2877b56977/</url>
    <content><![CDATA[<p>[toc]</p>
<p>TCP 和 UDP 处在同一层——传输层，但是它们有很多的不同。TCP 是 TCP/IP 系列协议中最复杂的部分，它具有以下特点：</p>
<ul>
<li>TCP 提供 可靠的 数据传输服务，TCP 是 <strong>面向连接</strong>的 。应用程序在使用 TCP 通信之前，先要建立连接，这是一个类似“打电话”的过程，通信结束后还要“挂电话”。</li>
<li>TCP 连接是 <strong>点对点</strong> 的，一条 TCP 连接只能连接两个端点。</li>
<li>TCP 提供 <strong>可靠传输</strong>，无差错、不丢失、不重复、按顺序。</li>
<li>TCP 提供 <strong>全双工 通信</strong>，允许通信双方任何时候都能发送数据，因为 TCP 连接的两端都设有发送缓存和接收缓存。</li>
<li>TCP 面向 <strong>字节流</strong> 。TCP 并不知道所传输的数据的含义，仅把数据看作一连串的字节序列，它也不保证接收方收到的数据块和发送方发出的数据块具有大小对应关系。</li>
</ul>
<p>使用 <code>netstat -s</code> 查看数据包统计信息：<br>
以下截图截取 tcp 部分<br>
<img src="/resource/a51814711521447987a3a74422dc1c84.png" alt="6eb8de528d1ef29f2e94cfedb44ea500.png"></p>
<p>截图中每行所表示含义依次是：主动开放的连接数，被动开放的连接数，失败的连接尝试，重置连接数，当前连接数，接收的 分段数，发送的分段数，重新传输的分段数。</p>
<h2 id="TCP-报文段结构">TCP 报文段结构</h2>
<p>TCP 是面向字节流的，而 TCP 传输数据的单元是 <strong>报文段</strong> 。一个 TCP 报文段可分为两部分：<strong>报头</strong>和<strong>数据部分</strong>。数据部分是上层应用交付的数据，而报头则是 TCP 功能的关键。</p>
<p>TCP 报文段的报头有前 20 字节的固定部分，后面 4n 字节是根据需要而添加的字段。如图则是 TCP 报文段结构：<br>
<img src="/resource/ebb48f96211d4c3db193bb20f0ac013e.png" alt="6a0a5cd59a3e6cac77eccbf8e3a1412b.png"></p>
<p>20 字节的固定部分，各字段功能说明：</p>
<ul>
<li><strong>源端口和目的端口</strong>:各占 2 个字节，分别写入源端口号和目的端口号。这和 UDP 报头有类似之处，因为都是传输层协议。</li>
<li><strong>序号</strong>:占 4 字节序，序号范围$[0, 2^{32}-1]$，序号增加到 $2^{32}-1$ 后，下个序号又回到 $0$。TCP 是面向字节流的，通过 TCP 传送的字节流中的每个字节都按顺序编号，而报头中的序号字段值则指的是本报文段数据的第一个字节的序号。</li>
<li><strong>确认序号</strong>:占 4 字节，期望收到对方下个报文段的第一个数据字节的序号。</li>
<li><strong>数据偏移</strong>:占 4 位，指 TCP 报文段的报头长度，包括固定的 20 字节和选项字段。</li>
<li><strong>保留</strong>:占 6 位，保留为今后使用，目前为 0。</li>
<li><strong>控制位</strong>:共有 6 个控制位，说明本报文的性质，意义如下：
<ul>
<li><strong>URG 紧急</strong>:当 URG=1 时，它告诉系统此报文中有紧急数据，应优先传送(比如紧急关闭)，这要与紧急指针字段配合使用。</li>
<li><strong>ACK 确认</strong>:仅当 ACK=1 时确认号字段才有效。建立 TCP 连接后，所有报文段都必须把 ACK 字段置为 1。</li>
<li><strong>PSH 推送</strong>:若 TCP 连接的一端希望另一端立即响应，PSH 字段便可以“催促”对方，不再等到缓存区填满才发送。</li>
<li><strong>RST 复位</strong>:若 TCP 连接出现严重差错，RST 置为 1，断开 TCP 连接，再重新建立连接。</li>
<li><strong>SYN 同步</strong>:用于建立和释放连接，稍后会详细介绍。</li>
<li><strong>FIN 终止</strong>:用于释放连接，当 FIN=1，表明发送方已经发送完毕，要求释放 TCP 连接。</li>
</ul>
</li>
<li><strong>窗口</strong>:占 2 个字节。窗口值是指发送者自己的接收窗口大小，因为接收缓存的空间有限。</li>
<li><strong>检验和</strong>:2 个字节。和 UDP 报文一样，有一个检验和，用于检查报文是否在传输过程中出差错。</li>
<li><strong>紧急指针</strong>:2 字节。当 URG=1 时才有效，指出本报文段紧急数据的字节数。</li>
<li><strong>选项</strong>:长度可变，最长可达 40 字节。具体的选项字段，需要时再做介绍。</li>
</ul>
<p>我们下面再用 <code>tcpdump</code> 命令试着抓取一下:<br>
<img src="/resource/f3361cd43893422c80dca5d1a1fcd197.png" alt="966ad0b240c2111fe96801c9e48c242c.png"></p>
<p>其实输出结果中还包含着 TCP 协议的报文，试着回顾一下，相信你能很快找到哪部分是 IP 协议的首部。IP 报文头紧接着的一部分就是 TCP 报文头，从 170d 开始。</p>
<ul>
<li>源端口：<code>0x170d</code>，转换为十进制为 5901。</li>
<li>目的端口：<code>0x9d86</code>，即为 40326。</li>
<li>序号：<code>0xba42638b</code>，即为 3124913035，这和图中开头的 seq 是一致的。</li>
<li>确认序号：<code>0x4c1ad749</code>，即为 1276827465，这和图中开头的 ack 是一致的。</li>
<li>数据偏移：<code>0x8</code>，<code>8*4=32B</code>。</li>
<li>其他可依次类推。</li>
</ul>
<h2 id="连接的建立与释放">连接的建立与释放</h2>
<p>刚才说过，TCP 是面向连接的，在传输 TCP 报文段之前先要创建连接，发起连接的一方被称为客户端，而响应连接请求的一方被称为服务端，而这个创建连接的过程被称为<strong>三次握手</strong>：<br>
<img src="/resource/0fd490c6ec7447cf8e33afdadb80827a.png" alt="216180853134e568e92c7730a7f6ba80.png"></p>
<ol>
<li>客户端发出请求连接报文段，其中报头控制位 SYN=1，初始序号 seq=x。客户端进入 SYN-SENT(同步已发送)状态。</li>
<li>服务端收到请求报文段后，向客户端发送确认报文段。确认报文段的首部中 SYN=1，ACK=1，确认号是 ack=x+1，同时为自己选择一个初始序号 seq=y。服务端进入 SYN-RCVD（同步收到）状态。</li>
<li>客户端收到服务端的确认报文段后，还要给服务端发送一个确认报文段。这个报文段中 ACK=1，确认号 ack=y+1，而自己的序号 seq=x+1。<strong>这个报文段已经可以携带数据，如果不携带数据则不消耗序号，则下一个报文段序号仍为 seq=x+1。</strong></li>
</ol>
<p>至此 TCP 连接已经建立，客户端进入 ESTABLISHED（已建立连接）状态，当服务端收到确认后，也进入 ESTABLISHED 状态，它们之间便可以正式传输数据了。</p>
<hr>
<p>当传输数据结束后，通信双方都可以释放连接，这个释放连接过程被称为<strong>释放连接</strong>:<br>
<img src="/resource/c0191af2b180461f9779d2709007a231.png" alt="6f2fad15dec30e057625c94471109f01.png"></p>
<ol>
<li>此时 TCP 连接两端都还处于 ESTABLISHED 状态，客户端停止发送数据，并发出一个 FIN 报文段。首部 FIN=1，序号 seq=u（u 等于客户端传输数据最后一字节的序号加 1）。客户端进入 FIN-WAIT-1（终止等待 1）状态。</li>
<li>服务端回复确认报文段，确认号 ack=u+1，序号 seq=v（v 等于服务端传输数据最后一字节的序号加 1），服务端进入 CLOSE-WAIT（关闭等待）状态。现在 TCP 连接处于半开半闭状态，服务端如果继续发送数据，客户端依然接收。</li>
<li>客户端收到确认报文，进入 FIN-WAIT-2 状态，服务端发送完数据后，发出 FIN 报文段，FIN=1，确认号 ack=u+1，然后进入 LAST-ACK(最后确认)状态。</li>
<li>客户端回复确认报文段，ACK=1，确认号 ack=w+1（w 为半开半闭状态时，收到的最后一个字节数据的编号） ，序号 seq=u+1，然后进入 TIME-WAIT（时间等待）状态。</li>
</ol>
<p>注意此时连接还没有释放，需要时间等待状态结束后（4 分钟）连接两端才会 CLOSED。设置时间等待是因为，有可能最后一个确认报文丢失而需要重传。</p>
<p><img src="/resource/e78c78a914a8414e9c9150657e8b3b31.png" alt="f4dfa9e1adc4f5e62dbb3980f10fac54.png"><br>
输出中展示了三次握手的过程。红色为第一次，黄框是第二次，绿框是第三次。</p>
<h2 id="TCP-可靠传输的实现">TCP 可靠传输的实现</h2>
<ul>
<li>TCP 报文段的长度可变，根据收发双方的缓存状态、网络状态而调整。</li>
<li>当 TCP 收到发自 TCP 连接另一端的数据，它将发送一个确认。</li>
<li>当 TCP 发出一个报文段后，它启动一个定时器，等待目的端确认收到这个报文段，如果不能及时收到一个确认，将重发这个报文段。这就是稍后介绍的超时重传。</li>
<li>TCP 将保持它首部和数据的检验和。如果通过检验和发现报文段有差错，这个报文段将被丢弃，等待超时重传。</li>
<li>TCP 将数据按字节排序，报文段中有序号，以确保顺序的正确性。</li>
<li>TCP 还能提供流量控制。TCP 连接的每一方都有收发缓存。TCP 的接收端只允许另一端发送接收端缓冲区所能接纳的数据。这将防止较快主机致使较慢主机的缓冲区溢出。</li>
</ul>
<p>可见超时重发机制是 TCP 可靠性的关键，只要没有得到确认报文段，就重新发送数据报，直到收到对方的确认为止。</p>
<h2 id="超时重传">超时重传</h2>
<p>TCP 规定，接收者收到数据报文段后，需回复一个确认报文段，以告知发送者数据已经收到。而发送者如果一段时间内（<strong>超时计时器</strong>）没有收到确认报文段，便重复发送。</p>
<p>为了实现超时间重传，需要注意：</p>
<ul>
<li>发送者发送一个报文段后，<strong>暂时保存该报文段的副本</strong>，为发生超时重传时使用，收到确认报文后删除该报文段。</li>
<li>确认报文段也需要序号，才能明确是发出去的哪个数据报得到了确认。</li>
<li>超时计时器比传输往返时间略长，但具体值是不确定的，根据网络情况而变。</li>
</ul>
<h2 id="连续-ARQ-协议">连续 ARQ 协议</h2>
<p>也许你也发现了，按上面的介绍，超时重传机制很费时间，每发送一个数据报都要等待确认。</p>
<p>在实际应用中的确不是这样的，真实情况是，采用了<strong>流水线传输</strong>：发送方可以连续发送多个报文段(连续发送的数据长度叫做窗口)，而不必每发完一段就停下来等待确认。</p>
<p>实际应用中，接收方也不必对收到的每个报文都做回复，而是采用<strong>累积确认</strong>方式：接收者收到多个连续的报文段后，只回复确认最后一个报文段，表示在这之前的数据都已收到。</p>
<p>这样，传输效率得到了很大的提升。</p>
<p><img src="/resource/d5ebb566355d40f495b29f76f3699cd7.png" alt="0a950f802d5e487d2ee9590c1fe5fee6.png"></p>
<h2 id="流量控制和拥塞控制">流量控制和拥塞控制</h2>
<p>由于接收方缓存的限制，发送窗口不能大于接收方接收窗口。在报文段首部有一个字段就叫做<strong>窗口(rwnd)</strong>，这便是用于告诉对方自己的接收窗口，可见窗口的大小是可以变化的。</p>
<p>那么窗口的大小是如何变化的呢？TCP 对于拥塞的控制总结为“慢启动、加性增、乘性减”，如图所示：<br>
<img src="/resource/4d3c302464a14a1faa01469dca822e8d.png" alt="a7a03705122b7cefa60b6e7408604bf2.png"></p>
<ul>
<li><strong>慢启动</strong> ：初始的窗口值很小，但是按指数规律渐渐增长，直到达到<strong>慢开始门限(ssthresh)</strong>。</li>
<li><strong>加性增</strong> ：窗口值达到慢开始门限后，每发送一个报文段，窗口值增加一个单位量。</li>
<li><strong>乘性减</strong> ：无论什么阶段，只要出现超时，则把窗口值减小一半。</li>
</ul>
<h2 id="tcpdump-抓取-TCP-报文段">tcpdump 抓取 TCP 报文段</h2>
<p>针对这次实验，需要下载对应的代码文件，是基于 TCP 的聊天小程序，分为 server（服务端—）和 client（客户端）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://labfile.oss.aliyuncs.com/courses/98/client.c</span><br><span class="line">wget https://labfile.oss.aliyuncs.com/courses/98/server.c</span><br><span class="line">gcc -o server server.c</span><br><span class="line">gcc -o client client.c</span><br></pre></td></tr></table></figure>
<p>编译完成后先不要运行，先打开 tcpdump，使用命令安装并运行 tcpdump：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install tcpdump</span><br><span class="line">sudo tcpdump -vvv -X -i lo tcp port 7777</span><br></pre></td></tr></table></figure>
<p>新开一个终端，运行 server 程序：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./server 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>然后再新开第三个终端，运行 client 程序：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./client 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>现在，使用 client 和 server 聊天，轮流互发几条简短的消息（比如 hello、hi、wei 之类的）便可以关闭 client 和 server，回到运行 tcpdump 的终端查看抓取的报文段内容：<br>
<img src="/resource/b3548203a00a4091998c41ed8c734881.png" alt="d0dd4b6e66bd9893d30740ab73942d78.png"></p>
<p>通过抓取的报文，还可以清晰的看到建立连接三次握手和断开连接四次握手的过程。</p>
]]></content>
  </entry>
  <entry>
    <title>ResNet</title>
    <url>/2022/06/01/9d8547900901416593ba964e6757c6c3/</url>
    <content><![CDATA[<h2 id="ResNet-引入">ResNet 引入</h2>
<p>在<code>VGG-19</code>中，卷积网络达到了 19 层，在<code>GoogLeNet</code>中，网络史无前例的达到了 22 层。</p>
<p>网络层数越高包含的函数空间也就越大，理论上网络的加深会让模型更有可能找到合适的函数。</p>
<p>但实际上，网络的精度会随着网络的层数增多而增多吗？在深度学习中，网络层数增多一般会伴着下面几个问题</p>
<ol>
<li>计算资源的消耗</li>
<li>模型容易过拟合</li>
<li>梯度消失/梯度爆炸问题的产生</li>
</ol>
<p>根据实验表明，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了。这是由于网络的加深会造成梯度爆炸和梯度消失的问题。</p>
<p><img src="https://img-blog.csdnimg.cn/20210203012841886.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>作者发现，随着网络层数的增加，网络发生了退化（degradation）的现象：随着网络层数的增多，训练集 loss 逐渐下降，然后趋于饱和，当你再增加网络深度的话，训练集 loss 反而会增大。注意这并不是过拟合，因为在过拟合中训练 loss 是一直减小的。</p>
<p>当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把低层的特征传到高层，那么效果应该至少不比浅层的网络效果差，或者说如果一个 VGG-100 网络在第 98 层使用的是和 VGG-16 第 14 层一模一样的特征，那么 VGG-100 的效果应该会和 VGG-16 的效果相同。所以，我们可以在 VGG-100 的 98 层和 14 层之间添加一条直接映射（Identity Mapping）来达到此效果。</p>
<p>从信息论的角度讲，由于 DPI（数据处理不等式）的存在，在前向传输的过程中，随着层数的加深，Feature Map 包含的图像信息会逐层减少，而 ResNet 的直接映射的加入，保证了 $l+1$ 层的网络一定比 $l$ 层包含更多的图像信息。</p>
<p>基于这种使用直接映射来连接网络不同层直接的思想，残差网络应运而生。</p>
<p><img src="https://img-blog.csdnimg.cn/20210203012848979.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>ResNet 是一种残差网络，咱们可以把它理解为一个子网络，这个子网络经过堆叠可以构成一个很深的网络。</p>
<p>为了让更深的网络也能训练出好的效果，何凯明大神提出了一个新的网络结构——ResNet。这个网络结构的想法主要源于 VLAD（残差的想法来源）和 Highway Network（跳跃连接的想法来源）。</p>
<p><img src="https://img-blog.csdnimg.cn/20210203154413891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="ResNet-结构">ResNet 结构</h2>
<p><img src="https://img-blog.csdnimg.cn/20210203012858311.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Model, Sequential</span><br></pre></td></tr></table></figure>
<h3 id="定义-BasicBlock-和-Bottleneck-ResNet50-101-152">定义 BasicBlock 和 Bottleneck(ResNet50/101/152)</h3>
<p><img src="https://img-blog.csdnimg.cn/2021020301290637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(layers.Layer):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, down_sample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__(**kwargs)</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=strides, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv1/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv2/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.down_sample = down_sample</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.down_sample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.down_sample(inputs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = inputs</span><br><span class="line"></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line"></span><br><span class="line">        x = self.add([identity, x])</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(layers.Layer): <span class="comment"># 瓶颈 两边粗中间细</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, down_sample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__(**kwargs)</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv1/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=strides, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.conv3 = layers.Conv2D(out_channel*self.expansion, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv3&#x27;</span>)</span><br><span class="line">        self.bn3 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv3/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.down_sample = down_sample</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.down_sample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.down_sample(inputs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = inputs</span><br><span class="line"></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.bn3(x, training=training)</span><br><span class="line"></span><br><span class="line">        x = self.add([identity, x])</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>在 block 之间衔接处，由于 feature map 的尺寸不一致，所以需要进行下采样的操作。论文中是通过$1\times 1 , strides=2$的卷积实现的。</p>
<p><img src="https://img-blog.csdnimg.cn/2021020301291660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>在 resnet50/101/152 中结构稍微有一些不同</p>
<p><img src="https://img-blog.csdnimg.cn/20210203012922219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, blocks_num, num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__(**kwargs)</span><br><span class="line">        self.include_top = include_top</span><br><span class="line"></span><br><span class="line">        self.conv1 = layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv1/BatchNorm&#x27;</span>)</span><br><span class="line">        self.relu1 = layers.ReLU(name=<span class="string">&#x27;relu1&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.maxpool1 = layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="string">&#x27;SAME&#x27;</span>, name=<span class="string">&#x27;maxpool1&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.block1 = self._make_layer(block, <span class="literal">True</span>, <span class="number">64</span>, blocks_num[<span class="number">0</span>], name=<span class="string">&#x27;block1&#x27;</span>)</span><br><span class="line">        self.block2 = self._make_layer(block, <span class="literal">False</span>, <span class="number">128</span>, blocks_num[<span class="number">1</span>], strides=<span class="number">2</span>, name=<span class="string">&#x27;block2&#x27;</span>)</span><br><span class="line">        self.block3 = self._make_layer(block, <span class="literal">False</span>, <span class="number">256</span>, blocks_num[<span class="number">2</span>], strides=<span class="number">2</span>, name=<span class="string">&#x27;block3&#x27;</span>)</span><br><span class="line">        self.block4 = self._make_layer(block, <span class="literal">False</span>, <span class="number">512</span>, blocks_num[<span class="number">3</span>], strides=<span class="number">2</span>, name=<span class="string">&#x27;block4&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">if</span> self.include_top == <span class="literal">True</span>:</span><br><span class="line">            self.avgpool = layers.GlobalAveragePooling2D(name=<span class="string">&#x27;avgpool1&#x27;</span>)</span><br><span class="line">            self.fc = layers.Dense(num_classes, name=<span class="string">&#x27;logits&#x27;</span>)</span><br><span class="line">            self.softmax = layers.Softmax()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line"></span><br><span class="line">        x = self.block1(x, training=training)</span><br><span class="line">        x = self.block2(x, training=training)</span><br><span class="line">        x = self.block3(x, training=training)</span><br><span class="line">        x = self.block4(x, training=training)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.include_top == <span class="literal">True</span>:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line">            x = self.softmax(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, first_block, channel, block_num, name=<span class="literal">None</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        down_sample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> strides != <span class="number">1</span> <span class="keyword">or</span> first_block <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            down_sample = Sequential([</span><br><span class="line">                layers.Conv2D(channel*block.expansion, kernel_size=<span class="number">1</span>, strides=strides, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv1&#x27;</span>),</span><br><span class="line">                layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;BatchNorm&#x27;</span>)</span><br><span class="line">            ], name=<span class="string">&#x27;shortcut&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        layers_list = []</span><br><span class="line">        layers_list.append(block(channel, down_sample=down_sample, strides=strides, name=<span class="string">&quot;unit_1&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers_list.append(block(channel, name=<span class="string">&#x27;unit_&#x27;</span> + <span class="built_in">str</span>(index + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Sequential(layers_list, name=name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="resnet-block-v2">resnet_block_v2</h2>
<p>最初的 resnet 残差块可以详细展开如<code>Fig.4(a)</code>，即在卷积之后使用了 BN 做归一化，然后在和直接映射单位加之后使用了 ReLU 作为激活函数。</p>
<p><code>Fig.4(c)</code>反应到网络里即将激活函数移到残差部分使用，这种在卷积之后使用激活函数的方法叫做 post-activation。然后，作者通过调整 ReLU 和 BN 的使用位置得到了几个变种，即<code>Fig.4(d)</code>中的 ReLU-only pre-activation 和<code>Fig.4(e)</code>中的 full pre-activation。</p>
<p><img src="https://img-blog.csdnimg.cn/20210203012934276.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
作者通过对照试验对比了这几种变异模型，结果见<code>Tabel 2</code>。</p>
<p><img src="https://img-blog.csdnimg.cn/20210203012939445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="搭建-resnet-block-v2">搭建 resnet_block_v2</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (e)full pre-activation</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock_v2</span>(layers.Layer):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, down_sample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock_v2, self).__init__(**kwargs)</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=strides, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv1/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv2/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.down_sample = down_sample</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.down_sample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.down_sample(inputs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = inputs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># v2</span></span><br><span class="line">        x = self.bn1(inputs, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.add([identity, x])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck_v2</span>(layers.Layer): <span class="comment"># 瓶颈 两边粗中间细</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, strides=<span class="number">1</span>, down_sample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck_v2, self).__init__(**kwargs)</span><br><span class="line">        self.conv1 = layers.Conv2D(out_channel, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.bn1 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv1/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.conv2 = layers.Conv2D(out_channel, kernel_size=<span class="number">3</span>, strides=strides, padding=<span class="string">&#x27;SAME&#x27;</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.conv3 = layers.Conv2D(out_channel*self.expansion, kernel_size=<span class="number">1</span>, use_bias=<span class="literal">False</span>, name=<span class="string">&#x27;conv3&#x27;</span>)</span><br><span class="line">        self.bn3 = layers.BatchNormalization(momentum=<span class="number">0.9</span>, epsilon=<span class="number">1e-5</span>, name=<span class="string">&#x27;conv3/BatchNorm&#x27;</span>)</span><br><span class="line">        <span class="comment"># ------------------------------------------------------------------</span></span><br><span class="line">        self.down_sample = down_sample</span><br><span class="line">        self.add = layers.Add()</span><br><span class="line">        self.relu = layers.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.down_sample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.down_sample(inputs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = inputs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># v2</span></span><br><span class="line">        x = self.bn1(inputs, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line"></span><br><span class="line">        x = self.bn2(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line"></span><br><span class="line">        x = self.bn3(x, training=training)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line"></span><br><span class="line">        x = self.add([identity, x])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>下面定义了 resnet_v1 版本 34/50/101 三种深度的网络模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_34</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    block = BasicBlock</span><br><span class="line">    block_num = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> ResNet(block, block_num, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    block = Bottleneck</span><br><span class="line">    blocks_num = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> ResNet(block, blocks_num, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    block = Bottleneck</span><br><span class="line">    blocks_num = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> ResNet(block, blocks_num, num_classes, include_top)</span><br></pre></td></tr></table></figure>
<p>下面定义了 resnet_v2 版本 34/50/101 三种深度的网络模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_v2_34</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    block = BasicBlock_v2</span><br><span class="line">    block_num = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> ResNet(block, block_num, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_v2_50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    block = Bottleneck_v2</span><br><span class="line">    blocks_num = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> ResNet(block, blocks_num, num_classes, include_top)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_v2_101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    block = Bottleneck_v2</span><br><span class="line">    blocks_num = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> ResNet(block, blocks_num, num_classes, include_top)</span><br></pre></td></tr></table></figure>
<p>主函数模型测试部分（每个实测通过）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line"><span class="comment">#     model = resnet_34(num_classes=1000, include_top=True)</span></span><br><span class="line"><span class="comment">#     model = resnet_50(num_classes=1000, include_top=True)</span></span><br><span class="line"><span class="comment">#     model = resnet_101(num_classes=1000, include_top=True)</span></span><br><span class="line"><span class="comment">#     model = resnet_v2_34(num_classes=1000, include_top=True)</span></span><br><span class="line"><span class="comment">#     model = resnet_v2_50(num_classes=1000, include_top=True)</span></span><br><span class="line">    model = resnet_v2_101(num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x_data = np.random.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>).astype(np.float32)</span><br><span class="line">    x_label = np.random.rand(<span class="number">4</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>).astype(np.int64)</span><br><span class="line">    model.build((<span class="literal">None</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    model.trainable = <span class="literal">False</span></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;Adam&quot;</span>, loss=<span class="string">&quot;mse&quot;</span>, metrics=[<span class="string">&quot;mae&quot;</span>, <span class="string">&quot;acc&quot;</span>])</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    pred = model.predict(x_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input shape:&quot;</span>, x_data.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output shape:&quot;</span>, pred.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Model: &quot;res_net_21&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #</span><br><span class="line">=================================================================</span><br><span class="line">conv1 (Conv2D)               multiple                  702464</span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1/BatchNorm (BatchNormal multiple                  256</span><br><span class="line">_________________________________________________________________</span><br><span class="line">relu1 (ReLU)                 multiple                  0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">maxpool1 (MaxPooling2D)      multiple                  0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">block1 (Sequential)          (None, 1, 56, 256)        217856</span><br><span class="line">_________________________________________________________________</span><br><span class="line">block2 (Sequential)          (None, 1, 28, 512)        1225728</span><br><span class="line">_________________________________________________________________</span><br><span class="line">block3 (Sequential)          (None, 1, 14, 1024)       26161152</span><br><span class="line">_________________________________________________________________</span><br><span class="line">block4 (Sequential)          (None, 1, 7, 2048)        14983168</span><br><span class="line">_________________________________________________________________</span><br><span class="line">avgpool1 (GlobalAveragePooli multiple                  0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">logits (Dense)               multiple                  2049000</span><br><span class="line">_________________________________________________________________</span><br><span class="line">softmax_20 (Softmax)         multiple                  0</span><br><span class="line">=================================================================</span><br><span class="line">Total params: 45,339,624</span><br><span class="line">Trainable params: 0</span><br><span class="line">Non-trainable params: 45,339,624</span><br><span class="line">_________________________________________________________________</span><br><span class="line">WARNING:tensorflow:11 out of the last 11 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x0000024FDB34EC18&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.</span><br><span class="line">input shape: (4, 3, 224, 224)</span><br><span class="line">output shape: (4, 1000)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>IP网际协议</title>
    <url>/2022/05/31/b20a04577c9d4a0e8a320ab78b918d79/</url>
    <content><![CDATA[<p>[toc]</p>
<p><strong>IP 数据报</strong>：IP 协议位于网络层，它是 TCP/IP 协议族中最为核心的协议，所有的 TCP、UDP、ICMP 及 IGMP 数据都以 IP 数据报格式传输。IP 协议提供的是不可靠、无连接的数据报传送服务。</p>
<p>本节实验我们将学习关于 IP 网际协议的进一步知识。</p>
<p><strong>知识点</strong></p>
<ul>
<li>IP 数据报</li>
<li>IP 地址分类</li>
<li>子网划分</li>
<li>IP 路由选择</li>
<li>NAT 技术</li>
<li>IP 的未来</li>
</ul>
<h2 id="IP-数据报">IP 数据报</h2>
<p><strong>IP 协议</strong>提供的数据传送服务是不可靠和无连接的，具体表现如下：</p>
<ul>
<li><code>不可靠（unreliable）</code>：IP 协议不能保证数据报能成功地到达目的地，它仅提供传输服务。当发生某种错误时，IP 协议会丢弃该数据报。传输的可靠性全由上层协议来提供。</li>
<li><code>无连接（connectionless）</code>：IP 协议对每个数据报的处理是相互独立的。这也说明，IP 数据报可以不按发送顺序接收。如果发送方向接收方发送了两个连续的数据报（先是 A，然后是 B），每个数据报可以选择不同的路线，因此 B 可能在 A 到达之前先到达。</li>
</ul>
<p><strong>IP 数据报格式</strong>：</p>
<p><img src="/resource/92ac35c862b94dfc8b4778debb4c5e5d.png" alt="2022-05-17-23-21-41.png"></p>
<p>如上图所示，普通的 IP 数据报的报头长度 20 字节(除非有选项字段)，各个部分的作用：</p>
<ul>
<li>版本号：4 位，用于标明 IP 版本号，0100 表示 IPv4，0110 表示 IPv6。目前常见的是 IPv4。</li>
<li>首部长度：4 位，表示 IP 报头长度，包括选项字段。</li>
<li>服务类型(TOS)：分别有：最小时延、最大吞吐量、最高可靠性、最小花费 4 种服务，如下图所示。4 个标识位只能有一个被置为 1。</li>
<li>总长度：16 位，报头长度加上数据部分长度，便是数据报的总长度。IP 数据报最长可达 65535 字节。</li>
<li>标识：16 位，接收方根据分片中的标识字段相不相同来判断这些分片是不是同一个数据报的分片，从而进行分片的重组。通常每发送一份报文它的值就会加 1。</li>
<li>标志：3 位，用于标识数据报是否分片。其中的第 2 位是不分段（DF）位。当 DF 位被设置为 1 时，则不对数据报进行分段处理；第 3 位是分段（MF）位，除了最后一个分段的 MF 位被设置为 0 外，其他的分段的 MF 位均设置为 1。</li>
<li>偏移：13 位，在接收方进行数据报重组时用来标识分片的顺序。</li>
<li>生存时间(TTL)：8 位，用于设置数据报可以经过的最多的路由器个数。TTL 的初始值由源主机设置（通常为 32 或 64），每经过一个处理它的路由器，TTL 值减 1。如果一个数据报的 TTL 值被减至 0，它将被丢弃。</li>
<li>协议：8 位，用来标识是哪个协议向 IP 传送数据。ICMP 为 1，IGMP 为 2，TCP 为 6，UDP 为 17，GRE 为 47，ESP 为 50。</li>
<li>首部校验和：根据 IP 首部计算的校验和码。</li>
<li>源 IP 和目的 IP ：数据报头还会包含该数据报的发送方 IP 和接收方 IP。</li>
<li>选项：是数据报中的一个可变长、可选的信息，不常用，多用于安全、军事等领域。</li>
</ul>
<p><code>tcpdump</code>抓包工具：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -n ：显示 IP 地址而非域名地址</span></span><br><span class="line"><span class="comment"># -t ：不显示时间戳</span></span><br><span class="line"><span class="comment"># -x ：以十六进制显示包内内容</span></span><br><span class="line"><span class="comment"># -c ：tcpdump 将在接受到几个数据包后退出</span></span><br><span class="line">sudo tcpdump -ntx -c 1</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shiyanlou:project/ $ sudo tcpdump -ntx -c 1</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">IP 192.168.42.3.3000 &gt; 172.16.6.54.55306: Flags [P.], <span class="built_in">seq</span> 364040095:364040353, ack 2089209678, win 931, options [nop,nop,TS val 1523918276 ecr 3113910914], length 258</span><br><span class="line">        0x0000:  4500 0136 8da1 4000 4006 0f2f c0a8 2a03</span><br><span class="line">        0x0010:  ac10 0636 0bb8 d80a 15b2 cf9f 7c86 cf4e</span><br><span class="line">        0x0020:  8018 03a3 9e1a 0000 0101 080a 5ad5 25c4</span><br><span class="line">        0x0030:  b99a 8282 817e 00fe 7b22 6b69 6e64 223a</span><br><span class="line">        0x0040:  2264 6174 6122 2c22 6964 223a 3139 2c22</span><br><span class="line">        0x0050:  636f 6e74 656e 7422 3a22 7b5c 226a 736f</span><br><span class="line">        0x0060:  6e72 7063 5c22 3a5c 2232 2e30 5c22 2c5c</span><br><span class="line">        0x0070:  226d 6574 686f 645c 223a 5c22 6f6e 4461</span><br><span class="line">        0x0080:  7461 5c22 2c5c 2270 6172 616d 735c 223a</span><br><span class="line">        0x0090:  5c22 7463 7064 756d 703a 2076 6572 626f</span><br><span class="line">        0x00a0:  7365 206f 7574 7075 7420 7375 7070 7265</span><br><span class="line">        0x00b0:  7373 6564 2c20 7573 6520 2d76 206f 7220</span><br><span class="line">        0x00c0:  2d76 7620 666f 7220 6675 6c6c 2070 726f</span><br><span class="line">        0x00d0:  746f 636f 6c20 6465 636f 6465 5c5c 725c</span><br><span class="line">        0x00e0:  5c6e 6c69 7374 656e 696e 6720 6f6e 2065</span><br><span class="line">        0x00f0:  7468 302c 206c 696e 6b2d 7479 7065 2045</span><br><span class="line">        0x0100:  4e31 304d 4220 2845 7468 6572 6e65 7429</span><br><span class="line">        0x0110:  2c20 6361 7074 7572 6520 7369 7a65 2032</span><br><span class="line">        0x0120:  3632 3134 3420 6279 7465 735c 5c72 5c5c</span><br><span class="line">        0x0130:  6e5c 227d 227d</span><br><span class="line">1 packet captured</span><br><span class="line">3 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure>
<p>首先看到开头的 <code>192.168.42.3.3000</code> &gt; <code>172.16.6.54.55306</code> 代表的是源 ip 为 <code>192.168.42.3</code>，端口 <code>3000</code>，目的 ip 为 <code>172.16.6.54</code>，端口 <code>55306</code>。</p>
<p>然后看到 <code>0x0000</code> 那行：</p>
<ul>
<li>协议版本： <code>0x4</code> 表示的是协议版本为 <code>IPv4</code>；</li>
<li>首部长度： <code>0x5</code>，<code>5*4=20</code>，表示 IP 报头长度为 20 字节。一个字节通常等于 8 位，所以这里可以知道 IP 报头为 <code>4500</code> 到 <code>2a02</code>；</li>
<li>TOS 服务类型：<code>0x00</code>，意味着是一般服务；</li>
<li>总长度：<code>0x0136</code>，换算下来为 <code>310</code> 字节；</li>
<li>标识：<code>0x8da1</code>；</li>
<li>3bit 标志 + 13bit 片偏移：<code>0x4000</code>；</li>
<li>生存时间：<code>0x40</code>，值为 64；</li>
<li>协议：<code>0x06</code>，代表 TCP 协议；</li>
<li>首部校验和：<code>0x0f2f</code>。</li>
</ul>
<p>其他信息可依次类推。</p>
<h2 id="IP-地址分类">IP 地址分类</h2>
<p>为了便于寻址以及层次化构造网络，每个 IP 地址可被看作是分为两部分，即<strong>网络号</strong>和<strong>主机号</strong>。同一个区域的所有主机有相同的网络号（即 IP 地址的前半部分相同），区域内的每个主机（包括路由器）都有一个主机号与其对应。</p>
<p>IP 地址被分为 A、B、C、D、E 五类：</p>
<ul>
<li>A 类给大型网络或政府机构等；</li>
<li>B 类分配给中型网络、跨国企业等；</li>
<li>C 类分配给小型网络；</li>
<li>D 类用于多播；</li>
<li>E 类用于实验。</li>
</ul>
<p>各类可容纳的地址数目不同，其中我们最常见的为 A、B、C 这三类。</p>
<p>IP 地址用 32 位二进制数字表示的时候，A、B、C 类 IP 的网络号长度分别为 8 位、16 位、24 位：</p>
<p><img src="/resource/578f417eb26140fe93cf9d2c08ff67a7.png" alt="2022-05-17-23-36-31.png"></p>
<p><strong>A 类地址</strong>：</p>
<ul>
<li>A 类地址网络号范围：1.0.0.0—127.0.0.0；</li>
<li>A 类 IP 地址范围：1.0.0.0—127.255.255.255；</li>
<li>A 类 IP 的私有地址范围：10.0.0.0—10.255.255.255 （所谓的私有地址就是在互联网上不使用，而被用在局域网络中的地址）；</li>
<li>127.X.X.X 是保留地址，用做循环测试用的；</li>
<li>因为主机号有 24 位，所以一个 A 类网络号可以容纳 224-2=16777214 个主机号。</li>
</ul>
<p><strong>B 类地址</strong>：</p>
<ul>
<li>B 类地址网络号范围：128.0.0.0—191.255.0.0；</li>
<li>B 类 IP 地址范围：128.0.0.0—191.255.255.255；</li>
<li>B 类 IP 的私有地址范围：172.16.0.0—172.31.255.255；</li>
<li>1- 69.254.X.X 是保留地址；191.255.255.255 是广播地址；</li>
<li>因为主机号有 16 位，所以一个 B 类网络号可以容纳 216-2=65534 个主机号。</li>
</ul>
<p><strong>C 类地址</strong>：</p>
<ul>
<li>C 类地址网络号范围：192.0.0.0—223.255.255.0；</li>
<li>C 类 IP 地址范围：192.0.0.0—223.255.255.255；</li>
<li>C 类 IP 的私有地址范围：192.168.0.0—192.168.255.255；</li>
<li>因为主机号有 8 位，所以一个 C 类网络号可以容纳 28-2=254 个主机号。</li>
</ul>
<h2 id="子网划分">子网划分</h2>
<p>IP 地址如果只使用 ABCDE 类来划分，会造成大量的浪费：一个有 500 台主机的网络，无法使用 C 类地址。但如果使用一个 B 类地址，6 万多个主机地址只有 500 个被使用，造成 IP 地址的大量浪费。</p>
<p>因此，可以在 ABC 类网络的基础上，进一步划分子网：占用主机号的前几个位，用于表示子网号。</p>
<p>这样 IP 地址就可看作 IP = 网络号 + 子网号 + 主机号。</p>
<p>子网号的位数没有硬性规定，于是我们用子网掩码来确定一个 IP 地址中哪几位是主机号，具体使用方法如图：</p>
<p><img src="/resource/cd7f6f5e6d1e4bcaad3863cfe168735c.png" alt="2022-05-30-19-22-13.png"></p>
<p>子网掩码中的 1 标识了 IP 地址中相应的<strong>网络号</strong>和<strong>子网号</strong>，0 标识了<strong>主机号</strong>。将 IP 地址和子网掩码进行逻辑与运算，结果就能区分网络号和子网号。</p>
<p>在 Linux 系统中使用<code>ifconfig</code>命令也可以查看到子网掩码：</p>
<p><img src="/resource/6458502b43244e58828762b21cbe77f4.png" alt="558bd34aadd1d65b4e6da00d91f0ae60.png"></p>
<h2 id="IP-路由选择">IP 路由选择</h2>
<p>如果发送方与接收方直接相连（点对点）或都在一个共享网络上（以太网），那么 IP 数据报就能直接送达。</p>
<p>而大多数情况则是发送方与接收方通过若干个路由器(router)连接，那么数据报就需要经过若干个路由器的转发才能送达，它是怎么选择一个合适的路径来&quot;送货&quot;的呢？</p>
<p>IP 层在内存中有一个路由表（输入命令 <code>route -n</code> 可以查看路由表），当收到一份数据报并进行发送时，都要对该表进行搜索：</p>
<ul>
<li>搜索路由表，如果能找到和目的 IP 地址完全一致的主机，则将 IP 数据报发向该主机；</li>
<li>搜索路由表，如果匹配主机失败，则匹配同子网的路由器(这需要子网掩码的协助)。如果找到路由器，则将该 IP 数据报发向该路由器；</li>
<li>搜索路由表，如果匹配同子网路由器失败，则匹配同网络号路由器，如果找到路由器，则将该 IP 数据报发向该路由器；</li>
<li>如果以上都失败了，就搜索默认路由，如果默认路由存在，则发报；</li>
<li>如果都失败了，就丢掉这个包；</li>
<li>接收到数据报的路由器再按照它自己的路由表继续转发，直到数据报被转发到目的主机；</li>
<li>如果在转发过程中，IP 数据报的 TTL（生命周期）已经被减为 0，则该 IP 数据报就被抛弃。</li>
</ul>
<p>可以使用 <code>route -n</code> 查看路由表：<br>
<img src="/resource/432cf1d025ea42c18fd56cd9a8ae87a3.png" alt="c671cdefc8f9360248463e39276433b3.png"></p>
<p>可以使用 <code>traceroute</code> 来追踪路由过程。<br>
<img src="/resource/df90993276e34fd7ba58e331b6ba8ef3.png" alt="d551e8e2b416319c051aed65d6bd9946.png"></p>
<p>记录按序列号从 1 开始，每个记录就是一跳，每跳表示一个网关，我们看到每行有三个时间，单位是 ms，这是探测数据包向每个网关发送三个数据包后，网关响应后返回的时间。用这三个时间来表示到达这个结点的网络速度。</p>
<p>我们会看到有一些行是以星号表示的。出现这样的情况，可能是防火墙封掉了 ICMP 的返回信息，所以我们得不到什么相关的数据包返回数据。</p>
<h2 id="NAT-技术">NAT 技术</h2>
<p>当你用 ifconfig 查看 IP 地址时，有时你会发现自己的 IP 地址是这样的———192.168.X.X 或 172.16.X.X。这是 C 类网和 B 类网的私有地址，就是俗称的<strong>内网 IP</strong>。这是因为你的路由器采用了 <strong>NAT 技术</strong>。</p>
<p>NAT（Network Address Translation，网络地址转换）是 1994 年提出的。当在专用网内部的一些主机本来已经分配到了内网 IP 地址，但现在又想和因特网上的主机通信时，NAT 技术将其内网 IP 地址转换成全球 IP 地址，然后与因特网连接，也就是说，内网的数台主机使用了同一个全球 IP 地址在上网。</p>
<p>NAT 技术实现了宽带共享，而且有助于缓解 IP 地址空间枯竭的问题。</p>
<p>使用 <code>ifconfig eth0</code> 查看内网 ip：<br>
<img src="/resource/1580bd4ba0b5463cb2e32d2f2e1ead8e.png" alt="9c6a98946797cee1c8574d85bd903ab8.png"></p>
<h2 id="IP-的未来">IP 的未来</h2>
<p>我们现在使用的 <strong>IPv4</strong> 协议版本从理论上讲，可以编址 1600 万个网络、40 亿台主机。但采用 A、B、C 三类编址方式后，可用的网络地址和主机地址的数目大打折扣，以至 IP 地址已于 <strong>2011 年 2 月 3 日</strong>分配完毕。</p>
<p>其中北美占有 3/4，约 30 亿个，而人口最多的亚洲只有不到 4 亿个，中国截止 2010 年 6 月 IPv4 地址数量达到 2.5 亿，落后于 4.2 亿网民的需求。地址不足，严重地制约了中国及其他国家互联网的应用和发展。</p>
<p>随着网络技术的发展，计算机网络将进入人们的日常生活，可能身边的每一样东西都需要连入全球因特网，在这样的环境下，IPv6 应运而生。</p>
<p><strong>IPv6</strong> 的地址长度是 128 位，通常将这 128 位的地址按每 16 位划分为一个段，将每个段转换成十六进制数字，并用冒号隔开，比如：<code>2000:0000:0000:0000:0001:2345:6789:abcd</code> 就是一个 IPv6 地址。</p>
<p>单从数量级上来说，IPv6 所拥有的地址容量是 IPv4 的约 $8\times10^{28}$ 倍，达到 $2^{128}$（算上全零的）个。这不但解决了网络地址资源数量的问题，同时也为除电脑外的设备连入互联网在数量限制上扫清了障碍。</p>
<p>随着 IPv4 不足，支持 IPv6 的网络迅速增长，现在全球已经有 5% 的网络使用 IPv6。</p>
]]></content>
  </entry>
  <entry>
    <title>Paddle高层API实现图像分类（CIFAR-100数据集_ResNet101）</title>
    <url>/2022/06/01/b6a6cbce3d7f45a8b2511edf86fb5a24/</url>
    <content><![CDATA[<p>[toc]</p>
<p>『深度学习 7 日打卡营·大作业』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
</ul>
<h2 id="数据集：">数据集：</h2>
<p>指定数据集：cifar100，通过高层 API 调用。</p>
<p>可以自己写数据增强和数据预处理功能。</p>
<h2 id="模型：">模型：</h2>
<p>随便选，模型参数初始化（如：uniform 和 normal）可以随意调整。</p>
<h2 id="模型训练-2">模型训练</h2>
<p>各种超参数（如：epochs、batch_size）可以随意调整。</p>
<h2 id="评判标准">评判标准</h2>
<p>最终以 model.evaluate 的精度输出值（格式如下），计算方式是将 eval_dataset 送入 evaluate 接口即可，需要在 model.prepare 中配置评估指标 Accuracy，所用数据集不能被用于训练过。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;&#x27;loss&#x27;: [6.4980035], &#x27;acc&#x27;: 0.8485721442885772&#125;</span><br></pre></td></tr></table></figure>
<h2 id="导入相关库">导入相关库</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">paddle.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;2.0.0&#x27;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置GPU</span></span><br><span class="line">paddle.set_device(<span class="string">&#x27;gpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CUDAPlace(0)</span><br></pre></td></tr></table></figure>
<h1>② 数据准备</h1>
<p><img src="https://img-blog.csdnimg.cn/img_convert/32c6c5b1cc1baffbff4b5c850d3f18f2.png" alt=""></p>
<h2 id="数据增强">数据增强</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment">#mean and std of cifar100 dataset</span></span><br><span class="line">   CIFAR100_TRAIN_MEAN = (<span class="number">0.5070751592371323</span>, <span class="number">0.48654887331495095</span>, <span class="number">0.4409178433670343</span>)</span><br><span class="line">   CIFAR100_TRAIN_STD = (<span class="number">0.2673342858792401</span>, <span class="number">0.2564384629170883</span>, <span class="number">0.27615047132568404</span>)</span><br><span class="line"></span><br><span class="line">train_transfrom = T.Compose([</span><br><span class="line">           T.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">           T.CenterCrop((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">           T.RandomHorizontalFlip(<span class="number">0.5</span>),        <span class="comment"># 随机水平翻转</span></span><br><span class="line">           T.RandomRotation(degrees=<span class="number">15</span>),       <span class="comment"># （-degrees，+degrees）</span></span><br><span class="line">           T.ToTensor(),                      <span class="comment"># 数据的格式转换和标准化 HWC =&gt; CHW</span></span><br><span class="line">           T.Normalize(mean=CIFAR100_MEAN, std=CIFAR100_STD)  <span class="comment"># 图像归一化</span></span><br><span class="line">       ])</span><br><span class="line"></span><br><span class="line">  eval_transfrom = T.Compose([</span><br><span class="line">           T.Resize(<span class="number">224</span>),</span><br><span class="line">           T.ToTensor(),                       <span class="comment"># 数据的格式转换和标准化 HWC =&gt; CHW</span></span><br><span class="line">           T.Normalize(mean=CIFAR100_MEAN, std=CIFAR100_STD)  <span class="comment"># 图像归一化</span></span><br><span class="line">       ])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>class paddle.vision.transforms.ToTensor</p>
</blockquote>
<p>将形状为 （H x W x C）的输入数据 PIL.Image 或 numpy.ndarray 转换为 (C x H x W)。 如果想保持形状不变，可以将参数 data_format 设置为 ‘HWC’。</p>
<p>同时，如果输入的 PIL.Image 的 mode 是 (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) 其中一种，或者输入的 <code>numpy.ndarray</code> 数据类型是<code> 'uint8'</code>，那个会将输入数据从<code>（0-255）</code>的范围缩放到 <code>（0-1）</code>的范围。其他的情况，则保持输入不变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">train_dataset = paddle.vision.datasets.Cifar100(mode=<span class="string">&#x27;train&#x27;</span>, transform=paddle.vision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证数据集</span></span><br><span class="line">eval_dataset = paddle.vision.datasets.Cifar100(mode=<span class="string">&#x27;test&#x27;</span>, transform=paddle.vision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并数据集</span></span><br><span class="line">dataset = paddle.concat([d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> paddle.io.DataLoader(train_dataset)] + [d[<span class="number">0</span>] <span class="keyword">for</span> d <span class="keyword">in</span> paddle.io.DataLoader(eval_dataset)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算数据均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;mean:<span class="subst">&#123;dataset.mean(axis=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>]).numpy()&#125;</span> \n std:<span class="subst">&#123;dataset.std(axis=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>]).numpy()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mean:[0.5073715 0.4867007 0.441096 ]</span><br><span class="line"> std:[0.26750046 0.25658613 0.27630225]</span><br></pre></td></tr></table></figure>
<p>由于要调用<code>resnet101</code>的预训练模型，这里把 CIFAR 的$32\times 32$的图像<code>resize</code>为$224\times 224$的图像，保持特征尺寸和感受野的一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)</span></span><br><span class="line"><span class="comment"># CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)</span></span><br><span class="line"></span><br><span class="line">CIFAR100_MEAN = [<span class="number">0.5073715</span>, <span class="number">0.4867007</span>, <span class="number">0.441096</span>]</span><br><span class="line">CIFAR100_STD = [<span class="number">0.26750046</span>, <span class="number">0.25658613</span>, <span class="number">0.27630225</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean=[0.485, 0.456, 0.406]</span></span><br><span class="line"><span class="comment"># std=[0.229, 0.224, 0.225]</span></span><br><span class="line"></span><br><span class="line">train_transfrom = T.Compose([</span><br><span class="line">            T.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">            T.CenterCrop((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">            T.RandomHorizontalFlip(<span class="number">0.5</span>),        <span class="comment"># 随机水平翻转</span></span><br><span class="line">            T.RandomRotation(degrees=<span class="number">15</span>),       <span class="comment"># （-degrees，+degrees）</span></span><br><span class="line">            T.ToTensor(),                      <span class="comment"># 数据的格式转换和标准化 HWC =&gt; CHW</span></span><br><span class="line">            T.Normalize(mean=CIFAR100_MEAN, std=CIFAR100_STD)  <span class="comment"># 图像归一化</span></span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">eval_transfrom = T.Compose([</span><br><span class="line">            T.Resize(<span class="number">224</span>),</span><br><span class="line">            T.ToTensor(),                       <span class="comment"># 数据的格式转换和标准化 HWC =&gt; CHW</span></span><br><span class="line">            T.Normalize(mean=CIFAR100_MEAN, std=CIFAR100_STD)  <span class="comment"># 图像归一化</span></span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">train_dataset = paddle.vision.datasets.Cifar100(mode=<span class="string">&#x27;train&#x27;</span>, transform=train_transfrom)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证数据集</span></span><br><span class="line">eval_dataset = paddle.vision.datasets.Cifar100(mode=<span class="string">&#x27;test&#x27;</span>, transform=eval_transfrom)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;训练集大小: <span class="subst">&#123;<span class="built_in">len</span>(train_dataset)&#125;</span>, 测试集大小: <span class="subst">&#123;<span class="built_in">len</span>(eval_dataset)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train data shape:&quot;</span>, train_dataset[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;eval data shape:&quot;</span>, eval_dataset[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_dataset[3][0]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练集大小: 50000, 测试集大小: 10000</span><br><span class="line">train data shape: [3, 224, 224]</span><br><span class="line">eval data shape: [3, 224, 224]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">idx = np.random.randint(<span class="number">0</span>, <span class="number">50000</span>, size=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> idx:</span><br><span class="line">    img = train_dataset[i][<span class="number">0</span>].numpy().transpose([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    plt.title(train_dataset[i][<span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:09,315 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193754486.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:09,435 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193804797.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:09,532 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2021021119380875.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:09,644 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br><span class="line">[WARNING 2021-02-11 13:53:09,756 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br><span class="line">[WARNING 2021-02-11 13:53:09,879 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193813389.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210211193817397.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210211193821204.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:09,994 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193825246.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:10,117 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193830261.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:10,240 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193833189.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[WARNING 2021-02-11 13:53:10,339 image.py:664] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210211193837699.png" alt="在这里插入图片描述"></p>
<h2 id="3-1-模型开发">3.1 模型开发</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">network = paddle.vision.models.resnet101(num_classes=<span class="number">100</span>, pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1263: UserWarning: Skip loading for fc.weight. fc.weight receives a shape [2048, 1000], but the expected shape is [2048, 100].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1263: UserWarning: Skip loading for fc.bias. fc.bias receives a shape [1000], but the expected shape is [100].</span><br><span class="line">  warnings.warn((&quot;Skip loading for &#123;&#125;. &quot;.format(key) + str(err)))</span><br></pre></td></tr></table></figure>
<h2 id="3-2-模型可视化">3.2 模型可视化</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = paddle.Model(network)</span><br><span class="line"></span><br><span class="line">model.summary((-<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">   Layer (type)         Input Shape          Output Shape         Param #</span><br><span class="line">===============================================================================</span><br><span class="line">    Conv2D-105       [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408</span><br><span class="line">  BatchNorm2D-105   [[1, 64, 112, 112]]   [1, 64, 112, 112]         256</span><br><span class="line">      ReLU-35       [[1, 64, 112, 112]]   [1, 64, 112, 112]          0</span><br><span class="line">    MaxPool2D-2     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0</span><br><span class="line">    Conv2D-107       [[1, 64, 56, 56]]     [1, 64, 56, 56]         4,096</span><br><span class="line">  BatchNorm2D-107    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-36        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-108       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-108    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">    Conv2D-109       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-109    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">    Conv2D-106       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-106    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">BottleneckBlock-34   [[1, 64, 56, 56]]     [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-110       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384</span><br><span class="line">  BatchNorm2D-110    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-37        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-111       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-111    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">    Conv2D-112       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-112    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">BottleneckBlock-35   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-113       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384</span><br><span class="line">  BatchNorm2D-113    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">      ReLU-38        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-114       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864</span><br><span class="line">  BatchNorm2D-114    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256</span><br><span class="line">    Conv2D-115       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384</span><br><span class="line">  BatchNorm2D-115    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024</span><br><span class="line">BottleneckBlock-36   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0</span><br><span class="line">    Conv2D-117       [[1, 256, 56, 56]]    [1, 128, 56, 56]       32,768</span><br><span class="line">  BatchNorm2D-117    [[1, 128, 56, 56]]    [1, 128, 56, 56]         512</span><br><span class="line">      ReLU-39        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-118       [[1, 128, 56, 56]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-118    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-119       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-119    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">    Conv2D-116       [[1, 256, 56, 56]]    [1, 512, 28, 28]       131,072</span><br><span class="line">  BatchNorm2D-116    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-37   [[1, 256, 56, 56]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-120       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-120    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-40        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-121       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-121    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-122       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-122    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-38   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-123       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-123    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-41        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-124       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-124    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-125       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-125    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-39   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-126       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-126    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">      ReLU-42        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-127       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456</span><br><span class="line">  BatchNorm2D-127    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512</span><br><span class="line">    Conv2D-128       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536</span><br><span class="line">  BatchNorm2D-128    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048</span><br><span class="line">BottleneckBlock-40   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0</span><br><span class="line">    Conv2D-130       [[1, 512, 28, 28]]    [1, 256, 28, 28]       131,072</span><br><span class="line">  BatchNorm2D-130    [[1, 256, 28, 28]]    [1, 256, 28, 28]        1,024</span><br><span class="line">      ReLU-43       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-131       [[1, 256, 28, 28]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-131    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-132       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-132   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">    Conv2D-129       [[1, 512, 28, 28]]   [1, 1024, 14, 14]       524,288</span><br><span class="line">  BatchNorm2D-129   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-41   [[1, 512, 28, 28]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-133      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-133    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-44       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-134       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-134    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-135       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-135   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-42  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-136      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-136    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-45       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-137       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-137    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-138       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-138   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-43  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-139      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-139    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-46       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-140       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-140    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-141       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-141   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-44  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-142      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-142    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-47       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-143       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-143    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-144       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-144   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-45  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-145      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-145    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-48       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-146       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-146    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-147       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-147   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-46  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-148      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-148    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-49       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-149       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-149    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-150       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-150   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-47  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-151      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-151    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-50       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-152       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-152    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-153       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-153   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-48  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-154      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-154    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-51       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-155       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-155    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-156       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-156   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-49  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-157      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-157    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-52       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-158       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-158    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-159       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-159   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-50  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-160      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-160    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-53       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-161       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-161    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-162       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-162   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-51  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-163      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-163    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-54       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-164       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-164    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-165       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-165   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-52  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-166      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-166    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-55       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-167       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-167    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-168       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-168   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-53  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-169      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-169    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-56       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-170       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-170    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-171       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-171   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-54  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-172      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-172    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-57       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-173       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-173    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-174       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-174   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-55  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-175      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-175    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-58       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-176       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-176    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-177       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-177   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-56  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-178      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-178    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-59       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-179       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-179    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-180       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-180   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-57  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-181      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-181    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-60       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-182       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-182    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-183       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-183   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-58  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-184      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-184    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-61       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-185       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-185    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-186       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-186   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-59  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-187      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-187    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-62       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-188       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-188    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-189       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-189   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-60  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-190      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-190    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-63       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-191       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-191    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-192       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-192   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-61  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-193      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-193    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-64       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-194       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-194    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-195       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-195   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-62  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-196      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-196    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">      ReLU-65       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-197       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824</span><br><span class="line">  BatchNorm2D-197    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024</span><br><span class="line">    Conv2D-198       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144</span><br><span class="line">  BatchNorm2D-198   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096</span><br><span class="line">BottleneckBlock-63  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0</span><br><span class="line">    Conv2D-200      [[1, 1024, 14, 14]]    [1, 512, 14, 14]       524,288</span><br><span class="line">  BatchNorm2D-200    [[1, 512, 14, 14]]    [1, 512, 14, 14]        2,048</span><br><span class="line">      ReLU-66        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-201       [[1, 512, 14, 14]]     [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-201     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">    Conv2D-202        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-202    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">    Conv2D-199      [[1, 1024, 14, 14]]    [1, 2048, 7, 7]       2,097,152</span><br><span class="line">  BatchNorm2D-199    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-64  [[1, 1024, 14, 14]]    [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-203       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-203     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">      ReLU-67        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-204        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-204     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">    Conv2D-205        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-205    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-65   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-206       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-206     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">      ReLU-68        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">    Conv2D-207        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296</span><br><span class="line">  BatchNorm2D-207     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048</span><br><span class="line">    Conv2D-208        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576</span><br><span class="line">  BatchNorm2D-208    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192</span><br><span class="line">BottleneckBlock-66   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0</span><br><span class="line">AdaptiveAvgPool2D-2  [[1, 2048, 7, 7]]     [1, 2048, 1, 1]           0</span><br><span class="line">     Linear-2           [[1, 2048]]            [1, 100]           204,900</span><br><span class="line">===============================================================================</span><br><span class="line">Total params: 42,810,404</span><br><span class="line">Trainable params: 42,599,716</span><br><span class="line">Non-trainable params: 210,688</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.57</span><br><span class="line">Forward/backward pass size (MB): 391.63</span><br><span class="line">Params size (MB): 163.31</span><br><span class="line">Estimated Total Size (MB): 555.52</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&#x27;total_params&#x27;: 42810404, &#x27;trainable_params&#x27;: 42599716&#125;</span><br></pre></td></tr></table></figure>
<h1>④ 模型训练和调优</h1>
<blockquote>
<p>class paddle.optimizer.lr.PiecewiseDecay(boundaries, values, last_epoch=- 1, verbose=False)</p>
</blockquote>
<p>该接口提供分段设置学习率的策略。</p>
<blockquote>
<p>class paddle.optimizer.lr.LinearWarmup(learing_rate, warmup_steps, start_lr, end_lr, last_epoch=- 1, verbose=False)</p>
</blockquote>
<p>该接口提供一种学习率优化策略-线性学习率热身(warm up)对学习率进行初步调整。在正常调整学习率之前，先逐步增大学习率。</p>
<blockquote>
<p>class paddle.callbacks.EarlyStopping(monitor=‘loss’, mode=‘auto’, patience=0, verbose=1, min_delta=0, baseline=None, save_best_model=True)</p>
</blockquote>
<p>在模型评估阶段，模型效果如果没有提升，<code>EarlyStopping</code> 会让模型提前停止训练。</p>
<ul>
<li>
<p>monitor (str，可选) - 监控量。该量作为模型是否停止学习的监控指标。默认值：‘loss’。</p>
</li>
<li>
<p>mode (str，可选) - 可以是’auto’、‘min’或者’max’。在 min 模式下，模型会在监控量的值不再减少时停止训练；max 模式下，模型会在监控量的值不再增加时停止训练；auto 模式下，实际的模式会从 <code>monitor </code>推断出来。如果<code>monitor</code>中有’acc’，将会认为是 max 模式，其它情况下，都会被推断为 min 模式。默认值：‘auto’。</p>
</li>
<li>
<p>patience (int，可选) - 多少个 epoch 模型效果未提升会使模型提前停止训练。默认值：0。</p>
</li>
<li>
<p>verbose (int，可选) - 可以是 0 或者 1。1 代表不打印模型提前停止训练的日志，1 代表打印日志。默认值：1。</p>
</li>
<li>
<p>min_delta (int|float，可选) - 监控量最小改变值。当 evaluation 的监控变量改变值小于<code> min_delta</code> ，就认为模型没有变化。默认值：0。</p>
</li>
<li>
<p>baseline (int|float，可选) - 监控量的基线。如果模型在训练 <code>patience</code> 个 epoch 后效果对比基线没有提升，将会停止训练。如果是 None，代表没有基线。默认值：None。</p>
</li>
<li>
<p>save_best_model (bool，可选) - 是否保存效果最好的模型（监控量的值最优）。文件会保存在 <code>fit</code> 中传入的参数 <code>save_dir</code> 下，前缀名为 best_model，默认值: True。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_optimizer</span>(<span class="params">parameters=<span class="literal">None</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>, boundaries=<span class="literal">None</span>, values=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">    learning_rate = paddle.optimizer.lr.PiecewiseDecay(</span><br><span class="line">        boundaries=boundaries,</span><br><span class="line">        values=values,</span><br><span class="line">        verbose=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># learning_rate = paddle.optimizer.lr.LinearWarmup(</span></span><br><span class="line">    <span class="comment">#     learning_rate=learning_rate,</span></span><br><span class="line">    <span class="comment">#     warmup_steps=wamup_steps,</span></span><br><span class="line">    <span class="comment">#     start_lr=base_lr / 5.,</span></span><br><span class="line">    <span class="comment">#     end_lr=base_lr,</span></span><br><span class="line">    <span class="comment">#     verbose=False)</span></span><br><span class="line"></span><br><span class="line">    optimizer = paddle.optimizer.Momentum(</span><br><span class="line">        learning_rate=learning_rate,</span><br><span class="line">        weight_decay=weight_decay,</span><br><span class="line">        momentum=momentum,</span><br><span class="line">        parameters=parameters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># optimizer = paddle.optimizer.AdamW(</span></span><br><span class="line">    <span class="comment">#     learning_rate=learning_rate,</span></span><br><span class="line">    <span class="comment">#     weight_decay=weight_decay,</span></span><br><span class="line">    <span class="comment">#     parameters=parameters)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base_lr = <span class="number">5e-4</span></span><br><span class="line">boundaries = [<span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">optimizer = make_optimizer(boundaries=boundaries, values=[base_lr, base_lr*<span class="number">0.2</span>, base_lr*<span class="number">0.1</span>], parameters=model.parameters())</span><br><span class="line"></span><br><span class="line">model.prepare(</span><br><span class="line">    <span class="comment"># optimizer=paddle.optimizer.Adam(learning_rate=5e-4, weight_decay=paddle.regularizer.L2Decay(5e-4), parameters=model.parameters()),</span></span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    loss=paddle.nn.CrossEntropyLoss(),</span><br><span class="line">    metrics=paddle.metric.Accuracy(topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># callbacks</span></span><br><span class="line">visualdl = paddle.callbacks.VisualDL(<span class="string">&#x27;./visualdl/resnet101&#x27;</span>)</span><br><span class="line">earlystop = paddle.callbacks.EarlyStopping( <span class="comment"># acc不在上升时停止</span></span><br><span class="line">    <span class="string">&#x27;acc&#x27;</span>,</span><br><span class="line">    mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">    patience=<span class="number">5</span>,</span><br><span class="line">    verbose=<span class="number">1</span>,</span><br><span class="line">    min_delta=<span class="number">0</span>,</span><br><span class="line">    baseline=<span class="literal">None</span>,</span><br><span class="line">    save_best_model=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model.fit(</span><br><span class="line">    train_dataset,</span><br><span class="line">    eval_dataset,</span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    save_freq=<span class="number">2</span>,</span><br><span class="line">    save_dir=<span class="string">&#x27;checkpoint/resnet101&#x27;</span>,</span><br><span class="line">    callbacks=[visualdl, earlystop],</span><br><span class="line">    verbose=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.6645 - acc_top1: 0.5995 - acc_top5: 0.8853 - 889ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.5161 - acc_top1: 0.6217 - acc_top5: 0.8958 - 421ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 2/20</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/hapi/callbacks.py:808: UserWarning: Monitor of EarlyStopping should be loss or metric name.</span><br><span class="line">  &#x27;Monitor of EarlyStopping should be loss or metric name.&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">step 391/391 [==============================] - loss: 1.6678 - acc_top1: 0.6264 - acc_top5: 0.8994 - 891ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.3930 - acc_top1: 0.6398 - acc_top5: 0.9048 - 421ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 3/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.4620 - acc_top1: 0.6483 - acc_top5: 0.9111 - 890ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/2</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.2873 - acc_top1: 0.6574 - acc_top5: 0.9170 - 419ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 4/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.2918 - acc_top1: 0.6655 - acc_top5: 0.9203 - 888ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.2550 - acc_top1: 0.6713 - acc_top5: 0.9235 - 422ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 5/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.2747 - acc_top1: 0.6815 - acc_top5: 0.9260 - 890ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/4</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.1627 - acc_top1: 0.6817 - acc_top5: 0.9289 - 432ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 6/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.2274 - acc_top1: 0.6934 - acc_top5: 0.9329 - 896ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.1079 - acc_top1: 0.6979 - acc_top5: 0.9341 - 429ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 7/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.0835 - acc_top1: 0.7030 - acc_top5: 0.9362 - 895ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/6</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.0728 - acc_top1: 0.7092 - acc_top5: 0.9389 - 427ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 8/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.0596 - acc_top1: 0.7117 - acc_top5: 0.9420 - 895ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.0304 - acc_top1: 0.7185 - acc_top5: 0.9434 - 428ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 9/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.0755 - acc_top1: 0.7246 - acc_top5: 0.9442 - 895ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/8</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.0199 - acc_top1: 0.7282 - acc_top5: 0.9452 - 424ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 10/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.1637 - acc_top1: 0.7312 - acc_top5: 0.9478 - 893ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 1.0062 - acc_top1: 0.7315 - acc_top5: 0.9464 - 428ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 11/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.0794 - acc_top1: 0.7399 - acc_top5: 0.9518 - 894ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/10</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9966 - acc_top1: 0.7390 - acc_top5: 0.9493 - 426ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 12/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.8478 - acc_top1: 0.7469 - acc_top5: 0.9539 - 894ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9767 - acc_top1: 0.7424 - acc_top5: 0.9512 - 425ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 13/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.9958 - acc_top1: 0.7526 - acc_top5: 0.9555 - 890ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/12</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9697 - acc_top1: 0.7490 - acc_top5: 0.9539 - 426ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 14/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.7780 - acc_top1: 0.7584 - acc_top5: 0.9579 - 894ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9391 - acc_top1: 0.7539 - acc_top5: 0.9564 - 423ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 15/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.0491 - acc_top1: 0.7658 - acc_top5: 0.9614 - 891ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9187 - acc_top1: 0.7583 - acc_top5: 0.9571 - 420ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 16/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.8458 - acc_top1: 0.7696 - acc_top5: 0.9617 - 890ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9076 - acc_top1: 0.7623 - acc_top5: 0.9589 - 422ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 17/20</span><br><span class="line">step 391/391 [==============================] - loss: 1.0365 - acc_top1: 0.7758 - acc_top5: 0.9628 - 892ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/16</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8588 - acc_top1: 0.7638 - acc_top5: 0.9598 - 424ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 18/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.9865 - acc_top1: 0.7814 - acc_top5: 0.9650 - 890ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8834 - acc_top1: 0.7701 - acc_top5: 0.9618 - 429ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 19/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.6649 - acc_top1: 0.7834 - acc_top5: 0.9670 - 893ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/18</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9119 - acc_top1: 0.7721 - acc_top5: 0.9620 - 427ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 20/20</span><br><span class="line">step 391/391 [==============================] - loss: 0.6904 - acc_top1: 0.7891 - acc_top5: 0.9680 - 898ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8655 - acc_top1: 0.7740 - acc_top5: 0.9628 - 430ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/final</span><br></pre></td></tr></table></figure>
<p><code>VisualDL</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/bb7b375314b4ac8ea386f7a9eb6f3d88.png" alt="resize_epoch20_lr5e-4"></p>
<p>loss 还在下降，继续训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载finetuning模型训练</span></span><br><span class="line">model.load(<span class="string">&#x27;./checkpoint/resnet101/14&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_optimizer</span>(<span class="params">parameters=<span class="literal">None</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>, boundaries=<span class="literal">None</span>, values=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">    learning_rate = paddle.optimizer.lr.PiecewiseDecay(</span><br><span class="line">        boundaries=boundaries,</span><br><span class="line">        values=values,</span><br><span class="line">        verbose=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># learning_rate = paddle.optimizer.lr.LinearWarmup(</span></span><br><span class="line">    <span class="comment">#     learning_rate=learning_rate,</span></span><br><span class="line">    <span class="comment">#     warmup_steps=wamup_steps,</span></span><br><span class="line">    <span class="comment">#     start_lr=base_lr / 5.,</span></span><br><span class="line">    <span class="comment">#     end_lr=base_lr,</span></span><br><span class="line">    <span class="comment">#     verbose=False)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># optimizer = paddle.optimizer.Momentum(</span></span><br><span class="line">    <span class="comment">#     learning_rate=learning_rate,</span></span><br><span class="line">    <span class="comment">#     weight_decay=weight_decay,</span></span><br><span class="line">    <span class="comment">#     momentum=momentum,</span></span><br><span class="line">    <span class="comment">#     parameters=parameters)</span></span><br><span class="line"></span><br><span class="line">    optimizer = paddle.optimizer.Adam(</span><br><span class="line">        learning_rate=learning_rate,</span><br><span class="line">        weight_decay=weight_decay,</span><br><span class="line">        parameters=parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line">base_lr = <span class="number">5e-5</span></span><br><span class="line">boundaries = [<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">optimizer = make_optimizer(boundaries=boundaries, values=[base_lr, base_lr*<span class="number">0.2</span>], parameters=model.parameters())</span><br><span class="line"></span><br><span class="line">model.prepare(</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    loss=paddle.nn.CrossEntropyLoss(),</span><br><span class="line">    metrics=paddle.metric.Accuracy(topk=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># callbacks</span></span><br><span class="line">visualdl = paddle.callbacks.VisualDL(<span class="string">&#x27;./visualdl/resnet101/14&#x27;</span>)</span><br><span class="line">earlystop = paddle.callbacks.EarlyStopping(</span><br><span class="line">    <span class="comment"># acc不在上升时停止</span></span><br><span class="line">    <span class="string">&#x27;acc&#x27;</span>,</span><br><span class="line">    mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">    patience=<span class="number">4</span>,</span><br><span class="line">    verbose=<span class="number">1</span>,</span><br><span class="line">    min_delta=<span class="number">0</span>,</span><br><span class="line">    baseline=<span class="literal">None</span>,</span><br><span class="line">    save_best_model=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model.fit(</span><br><span class="line">    train_dataset,</span><br><span class="line">    eval_dataset,</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    save_freq=<span class="number">2</span>,</span><br><span class="line">    save_dir=<span class="string">&#x27;checkpoint/resnet101/14&#x27;</span>,</span><br><span class="line">    callbacks=[visualdl],</span><br><span class="line">    verbose=<span class="number">1</span></span><br><span class="line">_dataset,</span><br><span class="line">    eval_dataset,</span><br><span class="line">    epochs=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    save_freq=<span class="number">2</span>,</span><br><span class="line">    save_dir=<span class="string">&#x27;checkpoint/resnet101/14&#x27;</span>,</span><br><span class="line">    callbacks=[visualdl],</span><br><span class="line">    verbose=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.7212 - acc_top1: 0.7760 - acc_top5: 0.9648 - 892ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14/0</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.9302 - acc_top1: 0.7996 - acc_top5: 0.9680 - 421ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 2/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.5295 - acc_top1: 0.8301 - acc_top5: 0.9776 - 893ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8164 - acc_top1: 0.8103 - acc_top5: 0.9732 - 416ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 3/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.4092 - acc_top1: 0.8622 - acc_top5: 0.9853 - 895ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14/2</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8366 - acc_top1: 0.8297 - acc_top5: 0.9747 - 416ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 4/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.3479 - acc_top1: 0.8860 - acc_top5: 0.9896 - 899ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.7397 - acc_top1: 0.8325 - acc_top5: 0.9757 - 428ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 5/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.3405 - acc_top1: 0.9086 - acc_top5: 0.9925 - 906ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14/4</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8492 - acc_top1: 0.8373 - acc_top5: 0.9780 - 430ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 6/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.1852 - acc_top1: 0.9242 - acc_top5: 0.9948 - 902ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.7244 - acc_top1: 0.8436 - acc_top5: 0.9761 - 422ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 7/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.3084 - acc_top1: 0.9387 - acc_top5: 0.9969 - 895ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14/6</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8724 - acc_top1: 0.8458 - acc_top5: 0.9767 - 419ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 8/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.1378 - acc_top1: 0.9529 - acc_top5: 0.9979 - 893ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.8844 - acc_top1: 0.8443 - acc_top5: 0.9765 - 419ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 9/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.1667 - acc_top1: 0.9623 - acc_top5: 0.9981 - 894ms/step</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14/8</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.7474 - acc_top1: 0.8490 - acc_top5: 0.9775 - 416ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">Epoch 10/10</span><br><span class="line">step 391/391 [==============================] - loss: 0.1841 - acc_top1: 0.9697 - acc_top5: 0.9989 - 895ms/step</span><br><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.7419 - acc_top1: 0.8471 - acc_top5: 0.9758 - 428ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">save checkpoint at /home/aistudio/checkpoint/resnet101/14/final</span><br></pre></td></tr></table></figure>
<h2 id="VisualDL">VisualDL</h2>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4bf477cbced30b62fd6911e12e30399b.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;./finetuning/resnet101/model&#x27;</span>, training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="评分输出">评分输出</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Eval samples: 10000</span><br><span class="line">&#123;&#x27;loss&#x27;: [1.4640276], &#x27;acc_top1&#x27;: 0.6361581096849475, &#x27;acc_top5&#x27;: 0.8786464410735122&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = model.evaluate(eval_dataset, batch_size=<span class="number">128</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Eval begin...</span><br><span class="line">The loss value printed in the log is the current batch, and the metric is the average value of previous step.</span><br><span class="line">step 79/79 [==============================] - loss: 0.7419 - acc_top1: 0.8468 - acc_top5: 0.9757 - 417ms/step</span><br><span class="line">Eval samples: 10000</span><br><span class="line">&#123;&#x27;loss&#x27;: [0.74186254], &#x27;acc_top1&#x27;: 0.8467935528120714, &#x27;acc_top5&#x27;: 0.9757373113854595&#125;</span><br></pre></td></tr></table></figure>
<h2 id="CIFAR-100-网络模型效果参考">CIFAR-100 网络模型效果参考</h2>
<p><a href="https://paperswithcode.com/sota/image-classification-on-cifar-100">https://paperswithcode.com/sota/image-classification-on-cifar-100</a></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0a7e9ebc85083f4a4edef5b43c127894.png" alt=""></p>
<p>自定义模型训练结果参考：</p>
<table>
<thead>
<tr>
<th style="text-align:center">dataset</th>
<th style="text-align:center">network</th>
<th style="text-align:center">params</th>
<th style="text-align:center">top1 err</th>
<th style="text-align:center">top5 err</th>
<th style="text-align:center">epoch(lr = 0.1)</th>
<th style="text-align:center">epoch(lr = 0.02)</th>
<th style="text-align:center">epoch(lr = 0.004)</th>
<th style="text-align:center">epoch(lr = 0.0008)</th>
<th style="text-align:center">total epoch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">mobilenet</td>
<td style="text-align:center">3.3M</td>
<td style="text-align:center">34.02</td>
<td style="text-align:center">10.56</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">mobilenetv2</td>
<td style="text-align:center">2.36M</td>
<td style="text-align:center">31.92</td>
<td style="text-align:center">09.02</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">squeezenet</td>
<td style="text-align:center">0.78M</td>
<td style="text-align:center">30.59</td>
<td style="text-align:center">8.36</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">shufflenet</td>
<td style="text-align:center">1.0M</td>
<td style="text-align:center">29.94</td>
<td style="text-align:center">8.35</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">shufflenetv2</td>
<td style="text-align:center">1.3M</td>
<td style="text-align:center">30.49</td>
<td style="text-align:center">8.49</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">vgg11_bn</td>
<td style="text-align:center">28.5M</td>
<td style="text-align:center">31.36</td>
<td style="text-align:center">11.85</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">vgg13_bn</td>
<td style="text-align:center">28.7M</td>
<td style="text-align:center">28.00</td>
<td style="text-align:center">9.71</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">vgg16_bn</td>
<td style="text-align:center">34.0M</td>
<td style="text-align:center">27.07</td>
<td style="text-align:center">8.84</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">vgg19_bn</td>
<td style="text-align:center">39.0M</td>
<td style="text-align:center">27.77</td>
<td style="text-align:center">8.84</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnet18</td>
<td style="text-align:center">11.2M</td>
<td style="text-align:center">24.39</td>
<td style="text-align:center">6.95</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnet34</td>
<td style="text-align:center">21.3M</td>
<td style="text-align:center">23.24</td>
<td style="text-align:center">6.63</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnet50</td>
<td style="text-align:center">23.7M</td>
<td style="text-align:center">22.61</td>
<td style="text-align:center">6.04</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnet101</td>
<td style="text-align:center">42.7M</td>
<td style="text-align:center">22.22</td>
<td style="text-align:center">5.61</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnet152</td>
<td style="text-align:center">58.3M</td>
<td style="text-align:center">22.31</td>
<td style="text-align:center">5.81</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">preactresnet18</td>
<td style="text-align:center">11.3M</td>
<td style="text-align:center">27.08</td>
<td style="text-align:center">8.53</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">preactresnet34</td>
<td style="text-align:center">21.5M</td>
<td style="text-align:center">24.79</td>
<td style="text-align:center">7.68</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">preactresnet50</td>
<td style="text-align:center">23.9M</td>
<td style="text-align:center">25.73</td>
<td style="text-align:center">8.15</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">preactresnet101</td>
<td style="text-align:center">42.9M</td>
<td style="text-align:center">24.84</td>
<td style="text-align:center">7.83</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">preactresnet152</td>
<td style="text-align:center">58.6M</td>
<td style="text-align:center">22.71</td>
<td style="text-align:center">6.62</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnext50</td>
<td style="text-align:center">14.8M</td>
<td style="text-align:center">22.23</td>
<td style="text-align:center">6.00</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnext101</td>
<td style="text-align:center">25.3M</td>
<td style="text-align:center">22.22</td>
<td style="text-align:center">5.99</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">resnext152</td>
<td style="text-align:center">33.3M</td>
<td style="text-align:center">22.40</td>
<td style="text-align:center">5.58</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">attention59</td>
<td style="text-align:center">55.7M</td>
<td style="text-align:center">33.75</td>
<td style="text-align:center">12.90</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">attention92</td>
<td style="text-align:center">102.5M</td>
<td style="text-align:center">36.52</td>
<td style="text-align:center">11.47</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">densenet121</td>
<td style="text-align:center">7.0M</td>
<td style="text-align:center">22.99</td>
<td style="text-align:center">6.45</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">densenet161</td>
<td style="text-align:center">26M</td>
<td style="text-align:center">21.56</td>
<td style="text-align:center">6.04</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">densenet201</td>
<td style="text-align:center">18M</td>
<td style="text-align:center">21.46</td>
<td style="text-align:center">5.9</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">googlenet</td>
<td style="text-align:center">6.2M</td>
<td style="text-align:center">21.97</td>
<td style="text-align:center">5.94</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">inceptionv3</td>
<td style="text-align:center">22.3M</td>
<td style="text-align:center">22.81</td>
<td style="text-align:center">6.39</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">inceptionv4</td>
<td style="text-align:center">41.3M</td>
<td style="text-align:center">24.14</td>
<td style="text-align:center">6.90</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">inceptionresnetv2</td>
<td style="text-align:center">65.4M</td>
<td style="text-align:center">27.51</td>
<td style="text-align:center">9.11</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">xception</td>
<td style="text-align:center">21.0M</td>
<td style="text-align:center">25.07</td>
<td style="text-align:center">7.32</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">seresnet18</td>
<td style="text-align:center">11.4M</td>
<td style="text-align:center">23.56</td>
<td style="text-align:center">6.68</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">seresnet34</td>
<td style="text-align:center">21.6M</td>
<td style="text-align:center">22.07</td>
<td style="text-align:center">6.12</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">seresnet50</td>
<td style="text-align:center">26.5M</td>
<td style="text-align:center">21.42</td>
<td style="text-align:center">5.58</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">seresnet101</td>
<td style="text-align:center">47.7M</td>
<td style="text-align:center">20.98</td>
<td style="text-align:center">5.41</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">seresnet152</td>
<td style="text-align:center">66.2M</td>
<td style="text-align:center">20.66</td>
<td style="text-align:center">5.19</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">nasnet</td>
<td style="text-align:center">5.2M</td>
<td style="text-align:center">22.71</td>
<td style="text-align:center">5.91</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">wideresnet-40-10</td>
<td style="text-align:center">55.9M</td>
<td style="text-align:center">21.25</td>
<td style="text-align:center">5.77</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">stochasticdepth18</td>
<td style="text-align:center">11.22M</td>
<td style="text-align:center">31.40</td>
<td style="text-align:center">8.84</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">stochasticdepth34</td>
<td style="text-align:center">21.36M</td>
<td style="text-align:center">27.72</td>
<td style="text-align:center">7.32</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">stochasticdepth50</td>
<td style="text-align:center">23.71M</td>
<td style="text-align:center">23.35</td>
<td style="text-align:center">5.76</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
<tr>
<td style="text-align:center">cifar100</td>
<td style="text-align:center">stochasticdepth101</td>
<td style="text-align:center">42.69M</td>
<td style="text-align:center">21.28</td>
<td style="text-align:center">5.39</td>
<td style="text-align:center">60</td>
<td style="text-align:center">60</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">200</td>
</tr>
</tbody>
</table>
]]></content>
  </entry>
  <entry>
    <title>交叉熵（Cross Entropy）</title>
    <url>/2022/06/01/9de2a5517a4448639d901c311628b8d2/</url>
    <content><![CDATA[<p>[toc]</p>
<p>交叉熵（cross entropy）是深度学习中常用的一个概念，一般用来求目标与预测值之间的差距。</p>
<p>交叉熵（Cross Entropy）是<code>Shannon信息论</code>中一个重要概念，主要用于度量两个概率分布间的差异性信息。语言模型的性能通常用交叉熵和复杂度（perplexity）来衡量。交叉熵的意义是用该模型对文本识别的难度，或者从压缩的角度来看，每个词平均要用几个位来编码。复杂度的意义是用该模型表示这一文本平均的分支数，其倒数可视为每个词的平均概率。平滑是指对没观察到的 N 元组合赋予一个概率值，以保证词序列总能通过语言模型得到一个概率值。通常使用的平滑技术有图灵估计、删除插值平滑、Katz 平滑和 Kneser-Ney 平滑。</p>
<h2 id="信息论">信息论</h2>
<p>在信息论中，交叉熵是表示两个概率分布 p,q，其中 p 表示真实分布，q 表示非真实分布，在相同的一组事件中，其中，用非真实分布 q 来表示某个事件发生所需要的平均比特数。从这个定义中，我们很难理解交叉熵的定义。下面举个例子来描述一下：</p>
<p>假设现在有一个样本集中两个概率分布 p,q，其中 p 为真实分布，q 为非真实分布。假如，按照真实分布 p 来衡量识别一个样本所需要的编码长度的期望为：</p>
<p>$$<br>
H§ = \sum_i{p(i)\cdot log(\frac{1}{p(i)})}<br>
$$</p>
<p>但是，如果采用错误的分布 q 来表示来自真实分布 p 的平均编码长度，则应该是：</p>
<p>$$<br>
H(p, q) = \sum_i{p(i)\cdot log(\frac{1}{q(i)})}<br>
$$</p>
<p>此时就将$H(p,q)$称之为交叉熵。</p>
<p>交叉熵的计算方式如下：</p>
<p>对于离散变量采用以下的方式计算:$H(p, q) = \sum_x{p(x)\cdot log(\frac{1}{q(x)})}$</p>
<p>对于连续变量采用以下的方式计算:$\int_X {P(X) log(Q(X)) dr(x)} = E_p[-logQ]$</p>
<h2 id="相对熵-2">相对熵</h2>
<p>相对熵又称 KL 散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。</p>
<p>维基百科中对相对熵的定义为：</p>
<blockquote>
<p>In the context of machine learning, DKL(P‖Q) is often called the information gain achieved if P is used instead of Q.</p>
</blockquote>
<p>即如果用 P 来描述目标问题，而不是用 Q 来描述目标问题，得到的信息增量。</p>
<p>在机器学习中，P 往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q 用来表示模型所预测的分布，比如[0.7,0.2,0.1]<br>
直观的理解就是如果用 P 来描述样本，那么就非常完美。而用 Q 来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和 P 一样完美的描述。如果我们的 Q 通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q 等价于 P。</p>
<p>KL 散度的计算公式为：</p>
<p>$$<br>
D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{1}<br>
$$</p>
<p>其中$n$为事件的所有可能性。$D_{KL}$的值越小，表示$q$分布和$p$分布越接近。</p>
<h2 id="交叉熵">交叉熵</h2>
<p>交叉熵可在神经网络(机器学习)中作为损失函数，p 表示真实标记的分布，q 则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量 p 与 q 的相似性。交叉熵作为损失函数还有一个好处是使用<code>sigmoid</code>函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。</p>
<p>在特征工程中，可以用来衡量两个随机变量之间的相似度。</p>
<p>在语言模型中（NLP）中，由于真实的分布 p 是未知的，在语言模型中，模型是通过训练集得到的，交叉熵就是衡量这个模型在测试集上的正确率。</p>
<p>对相对熵计算式(1)进行变形可以得到：</p>
<p>$$<br>
\begin{aligned}<br>
D_{KL}(p||q)<br>
&amp;= \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i)) \<br>
&amp;= -H(p(x))+ [-\sum_{i=1}^np(x_i)log(q(x_i))]<br>
\end{aligned}<br>
$$</p>
<p>等式的前一部分恰巧就是 p 的熵，等式的后一部分，就是交叉熵：</p>
<p>$$<br>
H(p,q)=-\sum_{i=1}^n p(x_i)log(q(x_i))<br>
$$</p>
<p>在机器学习中，我们需要评估<code>label</code>和<code>predicts</code>之间的差距，使用 KL 散度刚刚好，即$D_{KL}(y||\hat{y})$，由于 KL 散度中的前一部分$−H(y)$不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做 loss，评估模型。</p>
<h2 id="机器学习中的交叉熵">机器学习中的交叉熵</h2>
<h3 id="为什么要用交叉熵做损失函数？">为什么要用交叉熵做损失函数？</h3>
<p>在线性回归问题中，常常使用<code>MSE（Mean Squared Error）</code>作为 loss 函数，比如：</p>
<p>$$<br>
loss=\frac{1}{2m}\sum_{i=1}^m (y_i - \hat{y_i})^2<br>
$$</p>
<p>其中 m 表示样本数，loss 为 m 个样本的 loss 均值。<br>
MSE 在线性回归问题中比较好用，那么在逻辑分类问题中还是如此么？</p>
<h3 id="分类问题中的交叉熵">分类问题中的交叉熵</h3>
<p>单类别是指，每一张图像样本只能有一个类别，比如只能是狗或只能是猫。</p>
<p>在单类别问题中，交叉熵的计算如下：</p>
<p>$$<br>
loss=-\sum_{i=1}^{n}y_ilog(\hat{y_i})<br>
$$</p>
<p>假设<code>label</code>和<code>pred</code>的取值如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">cat</th>
<th style="text-align:center">dog</th>
<th style="text-align:center">man</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">label</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">pred</td>
<td style="text-align:center">0.7</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.1</td>
</tr>
</tbody>
</table>
<p>计算可得：</p>
<p>$$<br>
Loss_crossEntropy = - (1\times log(0.7) + 0\times log(0.2) + 0\times log(0.1)) = -log(0.7)<br>
$$</p>
<p>同理可得<code>batch_loss</code>：</p>
<p>$$<br>
loss_batch=-\frac{1}{m}\sum_{j=1}^m\sum_{i=1}^{n}y_{ji}log(\hat{y_{ji}})<br>
$$</p>
<p>如果<code>label</code>为多分类，即类别不互斥的情况下，交叉熵的计算如下：</p>
<p>$$<br>
Loss_crossEntropy =-ylog(\hat{y})-(1-y)log(1-\hat{y})<br>
$$</p>
<p>batch:</p>
<p>$$<br>
loss_batch =\frac{1}{m} \sum_{j=1}^{m}\sum_{i=1}^{n}-y_{ji}log(\hat{y_{ji}})-(1-y_{ji})log(1-\hat{y_{ji}})<br>
$$</p>
<h2 id="softmax">softmax</h2>
<p>由于做交叉熵之前一般都会用进行<code>softmax</code>处理，所以这里简单介绍一下<code>softmax</code>。</p>
<p>假设我们有一个数组$V$，$V_i$表示$V$中的第 i 个元素，那么这个元素的 Softmax 值就是</p>
<p>$$<br>
S_i = \frac{e^{V_i}}{\sum_j{e^{V_j}}}<br>
$$</p>
<p>也就是说，是该元素的指数，与所有元素指数和的比值。</p>
<p><code>softmax</code>的过程如图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0680fb59b03ab6c103276d66c20077ec.png" alt=""></p>
<p>softmax 直白来说就是将原来输出是 3,1,-3 通过 softmax 函数一作用，就映射成为(0,1)的值，而这些值的累和为 1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="softmax-cross-entropy-求导">softmax_cross_entropy 求导</h3>
<p>在多分类问题中，我们经常使用交叉熵作为损失函数:</p>
<p>$$<br>
loss=-\sum_{i=1}^{n}y_ilog(\hat{y_i})<br>
$$</p>
<p>当预测第 i 个时，$y$可以认为是 1，此时损失函数变成了：</p>
<p>$$<br>
loss_i=-log(\hat{y_i})<br>
$$</p>
<p>接下来对 Loss 求导。根据定义：</p>
<p>$$<br>
y_i = \frac{e^i}{\sum_j{e^j}}<br>
$$</p>
<p>由于 softmax 已经将数值映射到了 0-1 之间，并且和为 1，则有：</p>
<p>$$<br>
\frac{e^i}{\sum_j{e^j}} = 1 - \frac{\sum_{i\neq j}e^i}{\sum_j{e^j}}<br>
$$</p>
<p>下面是求导过程(结合链式法则)：</p>
<p>$$<br>
\begin{aligned}<br>
\frac{\partial loss_i}{\partial _i}<br>
&amp;=  - \frac{\partial lny_i}{\partial _i} \<br>
&amp;=  - \frac{\sum_j{e^j}}{e^i} \frac{\partial}{\partial <em>i}( \frac{e^i}{\sum_j{e^j}}) \<br>
&amp;=  - \frac{\sum_j{e^j}}{e^i} \frac{\partial}{\partial <em>i}( \frac{e^i}{\sum</em>{j \neq i}{e^j} + e^i}) \<br>
&amp;=  - \frac{\sum_j{e^j}}{e^i}  \frac{e^i\sum_j e^j - {e^i}^2}{(\sum</em>{j \neq i}{e^j} + e^i)^2} \<br>
&amp;=  - \frac{\sum_j e^j - {e^i}}{\sum_j{e^j}} \<br>
&amp;=  - (1 - \frac{e^i}{\sum_j{e^j}}) \<br>
&amp;=  y_i - 1<br>
\end{aligned}<br>
$$</p>
<h2 id="Python-实现单分类-softmax-交叉熵">Python 实现单分类 softmax_交叉熵</h2>
<p>通过以上理论我们就可以自己实现交叉熵函数，并与流行框架中的计算结果进行对比</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># one-hot</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y, y_hat</span>):</span><br><span class="line">    <span class="keyword">assert</span> y.shape == y_hat.shape</span><br><span class="line">    n = <span class="number">1e-6</span></span><br><span class="line">    res = -np.<span class="built_in">sum</span>(np.nan_to_num(y * np.log(y_hat + n)), axis=<span class="number">1</span>) <span class="comment"># 行求和</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">y</span>):</span><br><span class="line">    y_shift = y - np.<span class="built_in">max</span>(y, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    y_exp = np.exp(y_shift)</span><br><span class="line">    y_exp_sum = np.<span class="built_in">sum</span>(y_exp, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> y_exp / y_exp_sum</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    y_true = np.array([[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">0.8</span>, <span class="number">0.2</span>]])</span><br><span class="line">    y_pred = np.array([[<span class="number">4.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">    y_pred_softmax = softmax(y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;my_softmax:\n&quot;</span>, y_pred_softmax)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;tf_softmax:\n&quot;</span>, tf.nn.softmax(logits=y_pred).numpy())</span><br><span class="line"></span><br><span class="line">    my_loss = cross_entropy(y_true, y_pred_softmax)</span><br><span class="line">    my_loss_meam = np.mean(my_loss)</span><br><span class="line">    <span class="keyword">assert</span> my_loss.shape == (<span class="number">2</span>,)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;my_loss:\n&quot;</span>, my_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;my_loss_mean:\n&quot;</span>, my_loss_meam)</span><br><span class="line"></span><br><span class="line">    tf_loss = tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred, axis=-<span class="number">1</span>)  <span class="comment"># one-hot</span></span><br><span class="line">    tf_loss_meam = tf.reduce_mean(tf_loss)</span><br><span class="line">    <span class="keyword">assert</span> tf_loss.shape == (<span class="number">2</span>,)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;tf_loss:\n&quot;</span>, tf_loss.numpy())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;tf_loss_mean:\n&quot;</span>, tf_loss_meam.numpy())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_softmax:</span><br><span class="line"> [[0.84379473 0.1141952  0.04201007]</span><br><span class="line"> [0.00657326 0.97555875 0.01786798]]</span><br><span class="line">tf_softmax:</span><br><span class="line"> [[0.84379473 0.1141952  0.04201007]</span><br><span class="line"> [0.00657326 0.97555875 0.01786798]]</span><br><span class="line">my_loss:</span><br><span class="line"> [0.16984483 0.82473288]</span><br><span class="line">my_loss_mean:</span><br><span class="line"> 0.49728885581916177</span><br><span class="line">tf_loss:</span><br><span class="line"> [0.16984602 0.82474489]</span><br><span class="line">tf_loss_mean:</span><br><span class="line"> 0.4972954548475541</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Python编程库</title>
    <url>/2022/06/01/c0a421216c5349a9ac60432f0b0967e9/</url>
    <content><![CDATA[<p>[toc]</p>
<p>Python 被大量应用在数据挖掘和深度学习领域，其中使用极其广泛的是 Numpy、pandas、Matplotlib、PIL 等库。</p>
<h2 id="Why-Python">Why Python?</h2>
<ul>
<li>解释型语言（Interpreted Languages）</li>
<li>免费试用</li>
<li>跨平台执行</li>
</ul>
<h2 id="Python-机器学习的优势">Python 机器学习的优势</h2>
<ul>
<li>方便调试的解释型原因</li>
<li>跨平台执行作业</li>
<li>广泛的应用程序接口</li>
<li>丰富完备的开源工具包</li>
</ul>
<img src='https://ai-studio-static-online.cdn.bcebos.com/d2ab7dc4c05c42fe85c557a5ed084038822806b23ed544b5bab62517994f5383' height='400' width='400'>
<br/>
<br/>
<p><strong>numpy</strong>是 Python 科学计算库的基础。包含了强大的 N 维数组对象和向量运算。</p>
<p><strong>pandas</strong>是建立在 numpy 基础上的高效数据分析处理库，是 Python 的重要数据分析库。</p>
<p><strong>Matplotlib</strong>是一个主要用于绘制二维图形的 Python 库。用途：绘图、可视化</p>
<p><strong>PIL</strong>库是一个具有强大图像处理能力的第三方库。用途：图像处理</p>
<h1>Numpy 库</h1>
<p>NumPy 是使用 Python 进行科学计算的基础软件包。</p>
<p>NumPy：高效向量和矩阵运算<br>
SciPy：基于 NumPy，更为强大</p>
<p><a href="https://www.runoob.com/numpy/numpy-tutorial.html">菜鸟教程</a></p>
<p><a href="https://docs.scipy.org/doc/">Guidence</a></p>
<p>更多学习，可参考<strong>numpy 中文网</strong>：<a href="https://www.numpy.org.cn/">https://www.numpy.org.cn/</a></p>
<h2 id="1-数组创建">1.数组创建</h2>
<p>可以使用 array 函数从常规 Python<strong>列表或元组</strong>中创建数组。得到的数组的类型是从 Python 列表中元素的类型推导出来的。</p>
<p>创建数组最简单的办法就是使用 array 函数。它接受一切序列型的对象（包括其他数组），然后产生一个新的含有传入数据的 numpy 数组。其中，嵌套序列（比如由一组等长列表组成的列表）将会被转换为一个多维数组</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#将列表转换为数组</span></span><br><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                 [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(array)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#将元组转换为数组</span></span><br><span class="line">array = np.array(((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),</span><br><span class="line">                 (<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)))</span><br><span class="line"><span class="built_in">print</span>(array)</span><br></pre></td></tr></table></figure>
<p>下面这样可以吗？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<br/>
<p>通常，数组的元素最初是未知的，但它的大小是已知的。因此，NumPy 提供了几个函数来创建具有初始占位符内容的数组。</p>
<ul>
<li>
<p>zeros():可以创建指定长度或者形状的全 0 数组</p>
</li>
<li>
<p>ones():可以创建指定长度或者形状的全 1 数组</p>
</li>
<li>
<p>empty():创建一个数组，其初始内容是随机的,取决于内存的状态</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">zeroarray = np.zeros((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(zeroarray)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[0. 0. 0.]</span><br><span class="line"> [0. 0. 0.]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">onearray = np.ones((<span class="number">3</span>,<span class="number">4</span>),dtype=<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(onearray)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[1 1 1 1]</span><br><span class="line"> [1 1 1 1]</span><br><span class="line"> [1 1 1 1]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emptyarray = np.empty((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(emptyarray)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[6.92695269e-310 4.64024822e-310 0.00000000e+000 0.00000000e+000]</span><br><span class="line"> [0.00000000e+000 0.00000000e+000 0.00000000e+000 2.42092166e-322]</span><br><span class="line"> [4.64024821e-310 4.64024823e-310 0.00000000e+000 0.00000000e+000]]</span><br></pre></td></tr></table></figure>
<p>为了创建数字组成的数组，NumPy 提供了一个类似于 range 的函数，该函数返回数组而不是列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">array = np.arange( <span class="number">10</span>, <span class="number">31</span>,<span class="number">5</span> )</span><br><span class="line"><span class="built_in">print</span>(array)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]</span><br></pre></td></tr></table></figure>
<p>输出数组的一些信息，如维度、形状、元素个数、元素类型等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">array = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"><span class="built_in">print</span>(array)</span><br><span class="line"><span class="comment">#数组维度</span></span><br><span class="line"><span class="built_in">print</span>(array.ndim)</span><br><span class="line"><span class="comment">#数组形状</span></span><br><span class="line"><span class="built_in">print</span>(array.shape)</span><br><span class="line"><span class="comment">#数组元素个数</span></span><br><span class="line"><span class="built_in">print</span>(array.size)</span><br><span class="line"><span class="comment">#数组元素类型</span></span><br><span class="line"><span class="built_in">print</span>(array.dtype)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[ 1  2  3]</span><br><span class="line"> [ 4  5  6]</span><br><span class="line"> [ 7  8  9]</span><br><span class="line"> [10 11 12]]</span><br><span class="line">2</span><br><span class="line">(4, 3)</span><br><span class="line">12</span><br><span class="line">int64</span><br></pre></td></tr></table></figure>
<p>重新定义数字的形状</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">array1 = np.arange(<span class="number">6</span>).reshape([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(array1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array2 = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],dtype=np.int64).reshape([<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(array2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[0 1 2]</span><br><span class="line"> [3 4 5]]</span><br><span class="line">[[1 2]</span><br><span class="line"> [3 4]</span><br><span class="line"> [5 6]]</span><br></pre></td></tr></table></figure>
<h2 id="2-数组的计算">2.数组的计算</h2>
<p>数组很重要，因为它可以使我们不用编写循环即可对数据执行批量运算。这通常叫做矢量化（vectorization）。</p>
<p><strong>大小相等的数组之间的任何算术运算都会将运算应用到元素级</strong>。同样，数组与标量的算术运算也会将那个标量值传播到各个元素.</p>
<p>矩阵的基础运算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">arr1 = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">arr2 = np.ones([<span class="number">2</span>,<span class="number">3</span>],dtype=np.int64)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(arr1 + arr2)</span><br><span class="line"><span class="built_in">print</span>(arr1 - arr2)</span><br><span class="line"><span class="built_in">print</span>(arr1 * arr2)</span><br><span class="line"><span class="built_in">print</span>(arr1 / arr2)</span><br><span class="line"><span class="built_in">print</span>(arr1 ** <span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[2 3 4]</span><br><span class="line"> [5 6 7]]</span><br><span class="line">[[0 1 2]</span><br><span class="line"> [3 4 5]]</span><br><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">[[1. 2. 3.]</span><br><span class="line"> [4. 5. 6.]]</span><br><span class="line">[[ 1  4  9]</span><br><span class="line"> [16 25 36]]</span><br></pre></td></tr></table></figure>
<p>矩阵乘法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#矩阵乘法</span></span><br><span class="line">arr3 = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">arr4 = np.ones([<span class="number">3</span>,<span class="number">2</span>],dtype=np.int64)</span><br><span class="line"><span class="built_in">print</span>(arr3)</span><br><span class="line"><span class="built_in">print</span>(arr4)</span><br><span class="line"><span class="built_in">print</span>(np.dot(arr3,arr4))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">[[1 1]</span><br><span class="line"> [1 1]</span><br><span class="line"> [1 1]]</span><br><span class="line">[[ 6  6]</span><br><span class="line"> [15 15]]</span><br></pre></td></tr></table></figure>
<p>矩阵的其他计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(arr3)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(arr3,axis=<span class="number">1</span>)) <span class="comment">#axis=1,每一行求和 axie=0,每一列求和</span></span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">max</span>(arr3))</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">min</span>(arr3))</span><br><span class="line"><span class="built_in">print</span>(np.mean(arr3))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(arr3))</span><br><span class="line"><span class="built_in">print</span>(np.argmin(arr3))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[1 2 3]</span><br><span class="line"> [4 5 6]]</span><br><span class="line">[ 6 15]</span><br><span class="line">6</span><br><span class="line">1</span><br><span class="line">3.5</span><br><span class="line">5</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr3_tran = arr3.transpose()</span><br><span class="line"><span class="built_in">print</span>(arr3_tran)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(arr3.flatten())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[1 4]</span><br><span class="line"> [2 5]</span><br><span class="line"> [3 6]]</span><br><span class="line">[1 2 3 4 5 6]</span><br></pre></td></tr></table></figure>
<h2 id="3-数组的索引与切片">3.数组的索引与切片</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr5 = np.arange(<span class="number">0</span>,<span class="number">6</span>).reshape([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(arr5)</span><br><span class="line"><span class="built_in">print</span>(arr5[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(arr5[<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(arr5[<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(arr5[<span class="number">1</span>,:])</span><br><span class="line"><span class="built_in">print</span>(arr5[:,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(arr5[<span class="number">1</span>,<span class="number">0</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[0 1 2]</span><br><span class="line"> [3 4 5]]</span><br><span class="line">[3 4 5]</span><br><span class="line">5</span><br><span class="line">5</span><br><span class="line">[3 4 5]</span><br><span class="line">[1 4]</span><br><span class="line">[3 4]</span><br></pre></td></tr></table></figure>
<h1>pandas 库</h1>
<p>pandas 是 python 第三方库，提供高性能易用数据类型和分析工具。</p>
<p>pandas 基于 numpy 实现，常与 numpy 和 matplotlib 一同使用</p>
<p>更多学习，请参考<strong>pandas 中文网</strong>：<a href="https://www.pypandas.cn/">https://www.pypandas.cn/</a></p>
<br/>
<p><strong>Pandas 核心数据结构：</strong></p>
<img src="https://ai-studio-static-online.cdn.bcebos.com/a8c80653f39b479dab9f6867a638b64c405e79d6540c4307a22f43c4b0e228bc" width='300' heighr='300'>
<img src="https://ai-studio-static-online.cdn.bcebos.com/c8f06f423acc488fb391bca5dcf8f2b02d7444ef526f41599b6b430ae24659c1" width='500' height='500'>
<h2 id="1-Series">1.Series</h2>
<p>Series 是一种类似于一维数组的对象，它由一维数组（各种 numpy 数据类型）以及一组与之相关的数据标签（即索引）组成.</p>
<p>可理解为带标签的一维数组，可存储整数、浮点数、字符串、Python 对象等类型的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">s = pd.Series([<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0    a</span><br><span class="line">1    b</span><br><span class="line">2    c</span><br><span class="line">3    d</span><br><span class="line">4    e</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<p>Seris 中可以使用 index 设置索引列表。</p>
<p>与字典不同的是，Seris 允许索引重复</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#与字典不同的是：Series允许索引重复</span></span><br><span class="line">s = pd.Series([<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>],index=[<span class="number">100</span>,<span class="number">200</span>,<span class="number">100</span>,<span class="number">400</span>,<span class="number">500</span>])</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100    a</span><br><span class="line">200    b</span><br><span class="line">100    c</span><br><span class="line">400    d</span><br><span class="line">500    e</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<p>Series 可以用字典实例化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">d = &#123;<span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line">pd.Series(d)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b    1</span><br><span class="line">a    0</span><br><span class="line">c    2</span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<p>可以通过 Series 的 values 和 index 属性获取其数组表示形式和索引对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"><span class="built_in">print</span>(s.values)</span><br><span class="line"><span class="built_in">print</span>(s.index)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100    a</span><br><span class="line">200    b</span><br><span class="line">100    c</span><br><span class="line">400    d</span><br><span class="line">500    e</span><br><span class="line">dtype: object</span><br><span class="line">[&#x27;a&#x27; &#x27;b&#x27; &#x27;c&#x27; &#x27;d&#x27; &#x27;e&#x27;]</span><br><span class="line">Int64Index([100, 200, 100, 400, 500], dtype=&#x27;int64&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#与普通numpy数组相比，可以通过索引的方式选取Series中的单个或一组值</span></span><br><span class="line"><span class="built_in">print</span>(s[<span class="number">100</span>])</span><br><span class="line"><span class="built_in">print</span>(s[[<span class="number">400</span>, <span class="number">500</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100    a</span><br><span class="line">100    c</span><br><span class="line">dtype: object</span><br><span class="line">400    d</span><br><span class="line">500    e</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">s = pd.Series(np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对应元素求和</span></span><br><span class="line"><span class="built_in">print</span>(s+s)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对应元素乘</span></span><br><span class="line"><span class="built_in">print</span>(s*<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">e    5</span><br><span class="line">dtype: int64</span><br><span class="line">a     2</span><br><span class="line">b     4</span><br><span class="line">c     6</span><br><span class="line">d     8</span><br><span class="line">e    10</span><br><span class="line">dtype: int64</span><br><span class="line">a     3</span><br><span class="line">b     6</span><br><span class="line">c     9</span><br><span class="line">d    12</span><br><span class="line">e    15</span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure>
<p>Series 中最重要的一个功能是：它会在算术运算中自动对齐不同索引的数据</p>
<p>Series 和多维数组的主要区别在于， Series 之间的操作会自动基于标签对齐数据。因此，不用顾及执行计算操作的 Series 是否有相同的标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">obj1 = pd.Series(&#123;<span class="string">&quot;Ohio&quot;</span>: <span class="number">35000</span>, <span class="string">&quot;Oregon&quot;</span>: <span class="number">16000</span>, <span class="string">&quot;Texas&quot;</span>: <span class="number">71000</span>, <span class="string">&quot;Utah&quot;</span>: <span class="number">5000</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(obj1)</span><br><span class="line">obj2 = pd.Series(&#123;<span class="string">&quot;California&quot;</span>: np.nan, <span class="string">&quot;Ohio&quot;</span>: <span class="number">35000</span>, <span class="string">&quot;Oregon&quot;</span>: <span class="number">16000</span>, <span class="string">&quot;Texas&quot;</span>: <span class="number">71000</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(obj2)</span><br><span class="line"><span class="built_in">print</span>(obj1 + obj2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ohio      35000</span><br><span class="line">Oregon    16000</span><br><span class="line">Texas     71000</span><br><span class="line">Utah       5000</span><br><span class="line">dtype: int64</span><br><span class="line">California        NaN</span><br><span class="line">Ohio          35000.0</span><br><span class="line">Oregon        16000.0</span><br><span class="line">Texas         71000.0</span><br><span class="line">dtype: float64</span><br><span class="line">California         NaN</span><br><span class="line">Ohio           70000.0</span><br><span class="line">Oregon         32000.0</span><br><span class="line">Texas         142000.0</span><br><span class="line">Utah               NaN</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.Series(np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s[:-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s[<span class="number">1</span>:] + s[:-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">e    5</span><br><span class="line">dtype: int64</span><br><span class="line">a    1</span><br><span class="line">b    2</span><br><span class="line">c    3</span><br><span class="line">d    4</span><br><span class="line">dtype: int64</span><br><span class="line">a    NaN</span><br><span class="line">b    4.0</span><br><span class="line">c    6.0</span><br><span class="line">d    8.0</span><br><span class="line">e    NaN</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<h2 id="2-DataFrame">2.DataFrame</h2>
<img src="https://ai-studio-static-online.cdn.bcebos.com/c8f06f423acc488fb391bca5dcf8f2b02d7444ef526f41599b6b430ae24659c1" width='500' height='500'>
<p>DataFrame 是一个表格型的数据结构，类似于 Excel 或 sql 表</p>
<p>它含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔值等）</p>
<p>DataFrame 既有行索引也有列索引，它可以被看做由 Series 组成的字典（共用同一个索引）</p>
<p>用多维数组字典、列表字典生成 DataFrame</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = &#123;<span class="string">&#x27;state&#x27;</span>: [<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Nevada&#x27;</span>, <span class="string">&#x27;Nevada&#x27;</span>], <span class="string">&#x27;year&#x27;</span>: [<span class="number">2000</span>, <span class="number">2001</span>, <span class="number">2002</span>, <span class="number">2001</span>, <span class="number">2002</span>], <span class="string">&#x27;pop&#x27;</span>: [<span class="number">1.5</span>, <span class="number">1.7</span>, <span class="number">3.6</span>, <span class="number">2.4</span>, <span class="number">2.9</span>]&#125;</span><br><span class="line">frame = pd.DataFrame(data)</span><br><span class="line"><span class="built_in">print</span>(frame)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    state  year  pop</span><br><span class="line">0    Ohio  2000  1.5</span><br><span class="line">1    Ohio  2001  1.7</span><br><span class="line">2    Ohio  2002  3.6</span><br><span class="line">3  Nevada  2001  2.4</span><br><span class="line">4  Nevada  2002  2.9</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#如果指定了列顺序，则DataFrame的列就会按照指定顺序进行排列</span></span><br><span class="line">frame1 = pd.DataFrame(data, columns=[<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(frame1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   year   state  pop</span><br><span class="line">0  2000    Ohio  1.5</span><br><span class="line">1  2001    Ohio  1.7</span><br><span class="line">2  2002    Ohio  3.6</span><br><span class="line">3  2001  Nevada  2.4</span><br><span class="line">4  2002  Nevada  2.9</span><br></pre></td></tr></table></figure>
<p>跟原 Series 一样，如果传入的列在数据中找不到，就会产生 NAN 值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">frame2 = pd.DataFrame(data, columns=[<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>, <span class="string">&#x27;debt&#x27;</span>], index=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>, <span class="string">&#x27;four&#x27;</span>, <span class="string">&#x27;five&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(frame2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       year   state  pop debt</span><br><span class="line">one    2000    Ohio  1.5  NaN</span><br><span class="line">two    2001    Ohio  1.7  NaN</span><br><span class="line">three  2002    Ohio  3.6  NaN</span><br><span class="line">four   2001  Nevada  2.4  NaN</span><br><span class="line">five   2002  Nevada  2.9  NaN</span><br></pre></td></tr></table></figure>
<p>用 Series 字典或字典生成 DataFrame</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = &#123;<span class="string">&#x27;one&#x27;</span>: pd.Series([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]),</span><br><span class="line">     <span class="string">&#x27;two&#x27;</span>: pd.Series([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])&#125;</span><br><span class="line"><span class="built_in">print</span>(pd.DataFrame(d))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   one  two</span><br><span class="line">a  1.0  1.0</span><br><span class="line">b  2.0  2.0</span><br><span class="line">c  3.0  3.0</span><br><span class="line">d  NaN  4.0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过类似字典标记的方式或属性的方式，可以将DataFrame的列获取为一个Series,返回的Series拥有原DataFrame相同的索引</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(frame2[<span class="string">&#x27;state&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">one        Ohio</span><br><span class="line">two        Ohio</span><br><span class="line">three      Ohio</span><br><span class="line">four     Nevada</span><br><span class="line">five     Nevada</span><br><span class="line">Name: state, dtype: object</span><br></pre></td></tr></table></figure>
<p>列可以通过赋值的方式进行修改,例如，给那个空的“delt”列赋上一个标量值或一组值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">frame2[<span class="string">&#x27;debt&#x27;</span>] = <span class="number">16.5</span></span><br><span class="line"><span class="built_in">print</span>(frame2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       year   state  pop  debt</span><br><span class="line">one    2000    Ohio  1.5  16.5</span><br><span class="line">two    2001    Ohio  1.7  16.5</span><br><span class="line">three  2002    Ohio  3.6  16.5</span><br><span class="line">four   2001  Nevada  2.4  16.5</span><br><span class="line">five   2002  Nevada  2.9  16.5</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(frame2)</span><br><span class="line">frame2[<span class="string">&#x27;new&#x27;</span>] = frame2[<span class="string">&#x27;debt&#x27;</span> ]* frame2[<span class="string">&#x27;pop&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(frame2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       year   state  pop  debt</span><br><span class="line">one    2000    Ohio  1.5  16.5</span><br><span class="line">two    2001    Ohio  1.7  16.5</span><br><span class="line">three  2002    Ohio  3.6  16.5</span><br><span class="line">four   2001  Nevada  2.4  16.5</span><br><span class="line">five   2002  Nevada  2.9  16.5</span><br><span class="line">       year   state  pop  debt    new</span><br><span class="line">one    2000    Ohio  1.5  16.5  24.75</span><br><span class="line">two    2001    Ohio  1.7  16.5  28.05</span><br><span class="line">three  2002    Ohio  3.6  16.5  59.40</span><br><span class="line">four   2001  Nevada  2.4  16.5  39.60</span><br><span class="line">five   2002  Nevada  2.9  16.5  47.85</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">frame2[<span class="string">&#x27;debt&#x27;</span>] = np.arange(<span class="number">5.</span>)</span><br><span class="line"><span class="built_in">print</span>(frame2)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       year   state  pop  debt    new</span><br><span class="line">one    2000    Ohio  1.5   0.0  24.75</span><br><span class="line">two    2001    Ohio  1.7   1.0  28.05</span><br><span class="line">three  2002    Ohio  3.6   2.0  59.40</span><br><span class="line">four   2001  Nevada  2.4   3.0  39.60</span><br><span class="line">five   2002  Nevada  2.9   4.0  47.85</span><br></pre></td></tr></table></figure>
<h1>PIL 库</h1>
<p>PIL 库是一个具有强大图像处理能力的第三方库。</p>
<p>图像的组成：由 RGB 三原色组成,RGB 图像中，一种彩色由 R、G、B 三原色按照比例混合而成。0-255 区分不同亮度的颜色。</p>
<p>图像的数组表示：图像是一个由像素组成的矩阵，每个元素是一个 RGB 值</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c8f29ad77c13725c875e6d75bb6f2a41.png" alt=""></p>
<p>Image 是 PIL 库中代表一个图像的类（对象）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#安装pillow</span></span><br><span class="line"><span class="comment">#!pip install pillow</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">展示图片，并获取图像的模式，长宽，</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#显示matplotlib生成的图形</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取图片</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;/home/aistudio/work/yushuxin.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片</span></span><br><span class="line"><span class="comment">#img.show() #自动调用计算机上显示图片的工具</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show(img)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获得图像的模式</span></span><br><span class="line">img_mode = img.mode</span><br><span class="line"><span class="built_in">print</span>(img_mode)</span><br><span class="line"></span><br><span class="line">width,height = img.size</span><br><span class="line"><span class="built_in">print</span>(width,height)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091401201.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RGB</span><br><span class="line">533 300</span><br></pre></td></tr></table></figure>
<p>图片旋转</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#显示matplotlib生成的图形</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取图片</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;/home/aistudio/work/yushuxin.jpg&#x27;</span>)</span><br><span class="line"><span class="comment">#显示图片</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show(img)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将图片旋转45度</span></span><br><span class="line">img_rotate = img.rotate(<span class="number">45</span>)</span><br><span class="line"><span class="comment">#显示旋转后的图片</span></span><br><span class="line">plt.imshow(img_rotate)</span><br><span class="line">plt.show(img_rotate)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091411516.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210117091425130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>图片剪切</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#打开图片</span></span><br><span class="line">img1 = Image.<span class="built_in">open</span>(<span class="string">&#x27;/home/aistudio/work/yushuxin.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#剪切 crop()四个参数分别是：(左上角点的x坐标，左上角点的y坐标，右下角点的x坐标，右下角点的y坐标)</span></span><br><span class="line">img1_crop_result = img1.crop((<span class="number">126</span>,<span class="number">0</span>,<span class="number">381</span>,<span class="number">249</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图片</span></span><br><span class="line">img1_crop_result.save(<span class="string">&#x27;/home/aistudio/work/yushuxin_crop_result.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示图片</span></span><br><span class="line">plt.imshow(img1_crop_result)</span><br><span class="line">plt.show(img1_crop_result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-XYUTlLYq-1610845947715)(output_65_0.png)]</p>
<p>图片缩放</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#打开图片</span></span><br><span class="line">img2 = Image.<span class="built_in">open</span>(<span class="string">&#x27;/home/aistudio/work/yushuxin.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">width,height = img2.size</span><br><span class="line"></span><br><span class="line"><span class="comment">#缩放</span></span><br><span class="line">img2_resize_result = img2.resize((<span class="built_in">int</span>(width*<span class="number">0.6</span>),<span class="built_in">int</span>(height*<span class="number">0.6</span>)),Image.ANTIALIAS)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img2_resize_result.size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存图片</span></span><br><span class="line">img2_resize_result.save(<span class="string">&#x27;/home/aistudio/work/yushuxin_resize_result.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示图片</span></span><br><span class="line">plt.imshow(img2_resize_result)</span><br><span class="line">plt.show(img2_resize_result)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(319, 180)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091454104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>镜像效果：左右旋转、上下旋转</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#打开图片</span></span><br><span class="line">img3 = Image.<span class="built_in">open</span>(<span class="string">&#x27;/home/aistudio/work/yushuxin.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#左右镜像</span></span><br><span class="line">img3_lr = img3.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示左右镜像图片</span></span><br><span class="line">plt.imshow(img3_lr)</span><br><span class="line">plt.show(img3_lr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#上下镜像</span></span><br><span class="line">img3_bt = img3.transpose(Image.FLIP_TOP_BOTTOM)</span><br><span class="line"></span><br><span class="line"><span class="comment">#展示上下镜像图片</span></span><br><span class="line">plt.imshow(img3_bt)</span><br><span class="line">plt.show(img3_bt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091501180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210117091508795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1>Matplotlib 库</h1>
<p>Matplotlib 库由各种可视化类构成，内部结构复杂。</p>
<p>matplotlib.pylot 是绘制各类可视化图形的命令字库</p>
<p>更多学习，可参考<strong>Matplotlib 中文网</strong>：<a href="https://www.matplotlib.org.cn">https://www.matplotlib.org.cn</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!pip install matplotlib</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示matplotlib生成的图形</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">50</span>) <span class="comment">#等差数列</span></span><br><span class="line">y = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#传入x,y,通过plot()绘制出折线图</span></span><br><span class="line">plt.plot(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图形</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091515473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">50</span>) <span class="comment">#等差数列</span></span><br><span class="line">y1 = <span class="number">2</span>*x + <span class="number">1</span></span><br><span class="line">y2 = x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091521998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20210117091527264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(x,y1,color=<span class="string">&#x27;red&#x27;</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">plt.plot(x,y2,color=<span class="string">&#x27;blue&#x27;</span>,linewidth=<span class="number">5</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091534143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">l1, = plt.plot(x,y1,color=<span class="string">&#x27;red&#x27;</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">l2, = plt.plot(x,y2,color=<span class="string">&#x27;blue&#x27;</span>,linewidth=<span class="number">5</span>)</span><br><span class="line">plt.legend(handles=[l1,l2],labels=[<span class="string">&#x27;aa&#x27;</span>,<span class="string">&#x27;bb&#x27;</span>],loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.xlim((<span class="number">0</span>,<span class="number">1</span>))  <span class="comment">#x轴只截取一段进行显示</span></span><br><span class="line">plt.ylim((<span class="number">0</span>,<span class="number">1</span>))  <span class="comment">#y轴只截取一段进行显示</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091538999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dots1 = np.array([2,3,4,5,6])</span></span><br><span class="line"><span class="comment"># dots2 = np.array([2,3,4,5,6])</span></span><br><span class="line">dots1 =np.random.rand(<span class="number">50</span>)</span><br><span class="line">dots2 =np.random.rand(<span class="number">50</span>)</span><br><span class="line">plt.scatter(dots1,dots2,c=<span class="string">&#x27;red&#x27;</span>,alpha=<span class="number">0.5</span>) <span class="comment">#c表示颜色，alpha表示透明度</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091544326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">10</span>)</span><br><span class="line">y = <span class="number">2</span>**x+<span class="number">10</span></span><br><span class="line">plt.bar(x,y,facecolor=<span class="string">&#x27;#9999ff&#x27;</span>,edgecolor=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091551845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">10</span>)</span><br><span class="line">y = <span class="number">2</span>**x+<span class="number">10</span></span><br><span class="line">plt.bar(x,y,facecolor=<span class="string">&#x27;#9999ff&#x27;</span>,edgecolor=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> ax,ay <span class="keyword">in</span> <span class="built_in">zip</span>(x,y):</span><br><span class="line">    plt.text(ax,ay,<span class="string">&#x27;%.1f&#x27;</span> % ay,ha=<span class="string">&#x27;center&#x27;</span>,va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210117091559984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzI2Mjgw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
  </entry>
  <entry>
    <title>Linux常用指令</title>
    <url>/2022/06/01/c673a6f48a244d0ebce43f4bd42372b3/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="查找-find">查找 <code>find</code></h2>
<p>查找文件或目录路径<br>
用于在硬盘中查找文件或目录的路径(速度较慢)</p>
<p><code>find path [-option] [查找条件]</code></p>
<p><code>-name</code> 根据文件名进行精确查找</p>
<p>example:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">find / -name test.txt # 从根目录下开始查找精确匹配名字的文件路径</span><br><span class="line"></span><br><span class="line">find ./ -name &quot;*test*&quot; # 从当前目录下开始查找包含test名字的所有文件和目录路径(*通配任意字符)</span><br></pre></td></tr></table></figure>
<h3 id="查看文件内容">查看文件内容</h3>
<ul>
<li>常用的查看文件内容命令：
<ul>
<li>cat: 直接查阅文件内容，不能翻页</li>
<li>more: 翻页查看文件内容</li>
<li>less: 翻页阅读，和 more 类似，但操作按键比 more 更弹性</li>
<li>head: 查看文档的前面几行内容，默认 10 行</li>
<li>tail: 查看文档的最后几行内容，默认 10 行</li>
</ul>
</li>
</ul>
<h3 id="查找文件内容">查找文件内容</h3>
<p>grep 命令，在文本中或 stdin 中查找匹配字符串</p>
<p><code>grep [-cin] '目标字符串' filename</code></p>
<p>grep(global search regular expression(RE) and print out the line, 全面搜索正则表达式并把行打印出来)是一种强大的文本搜索工具，它能使用正则表达式搜索文件，并把匹配的行打印出来。</p>
<p><img src="/resource/e65584f217d343949c9811a6b5d7e2d5.png" alt="2022-04-19-16-34-39.png"></p>
<p>常用选项：</p>
<ul>
<li><code>-c</code>: 计算找到’搜索字符串’的行数</li>
<li><code>-i</code>: 忽略大小写的不同，所以大小写视为相同</li>
<li><code>-n</code>: 顺便输出行号</li>
<li><code>-l</code>: 根据文件内容查找文件，只显示包含该内容的文件名</li>
<li><code>-r</code>: 根据文件内容递归查找文件，并打印对应内容</li>
<li><code>-I</code>: 表示忽略二进制文件</li>
</ul>
<p><img src="/resource/b55ee69bb5424e5ea3139b96af15b0da.png" alt="2022-04-19-16-45-10.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">grep &#x27;hello&#x27; * # 在当前目录搜索带有&#x27;hello&#x27;的文件</span><br><span class="line">grep -l &#x27;hello&#x27; * # 在当前目录搜索带有&#x27;hello&#x27;的文件，只显示文件名</span><br><span class="line">grep -r &#x27;hello&#x27; * # 在当前目录及其子目录下搜索&#x27;hello&#x27;的文件</span><br></pre></td></tr></table></figure>
<h3 id="管道命令">管道命令</h3>
<p>管道是一种通信机制，通常用于进程间的通信（也可通过 socket 进行网络通信），它表现出来的形式就是将前面每一个进程的输出（stdout）直接作为下一个进程的输入（stdin）。</p>
<p>管道又分为<code>匿名管道</code>和<code>具名管道</code>（这里将不会讨论在源程序中使用系统调用创建并使用管道的情况，它与命令行的管道在内核中实际都是采用相同的机制）。我们在使用一些过滤程序时经常会用到的就是匿名管道，在命令行中由 | 分隔符表示，| 在前面的内容中我们已经多次使用到了。具名管道简单的说就是有名字的管道，通常只会在源程序中用到具名管道。</p>
<ul>
<li>管道：将一个命令的输出连接到另一个命令的输入。</li>
<li>符号：<code>|</code></li>
<li>例如：<code>cat /etc/passwd | grep oracle</code> (通常与 grep 配合用于过滤查找)</li>
</ul>
<h3 id="输出重定向">输出重定向</h3>
<p>在更多了解 Linux 的重定向之前，我们需要先知道一些基本的东西，前面我们已经提到过 Linux 默认提供了三个特殊设备，用于终端的显示和输出，分别为 <code>stdin</code>（标准输入，对应于你在终端的输入，文本描述符 0），<code>stdout</code>（标准输出，对应于终端的输出，文本描述符 1），<code>stderr</code>（标准错误输出，对应于终端的输出，文本描述符 2）。</p>
<ul>
<li>标准文件：<code>stdin</code>, <code>stdout</code>, <code>stderr</code>
<ul>
<li>对应的文件描述符为 0, 1, 2</li>
</ul>
</li>
<li>输出重定向：<code>&gt;</code>(覆盖导入), <code>&gt;&gt;</code>(从文件末尾导入)</li>
<li>输入重定向：<code>&lt;</code></li>
<li>例如：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls -l &gt; ls.out  # 将ls -l命令重定向到文件ls.out中</span><br><span class="line">find / -name filename 2&gt; find.txt  # 将命令错误输入重定向到文件中。</span><br><span class="line">find / -name filename &gt; find.txt  # 将命令正确输出重定向到文件中</span><br><span class="line">find / -name filename %&gt; find.txt  # 将命令所有输入重定向到文件中。</span><br></pre></td></tr></table></figure>
<p>管道默认是连接前一个命令的输出到下一个命令的输入，而重定向通常是需要一个文件来建立两个命令的连接。</p>
<h3 id="cut-命令">cut 命令</h3>
<p>cut 命令，打印每一行的某一字段。</p>
<p>打印 /etc/passwd 文件中以 : 为分隔符的第 1 个字段和第 6 个字段分别表示用户名和其家目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cut /etc/passwd -d &#x27;:&#x27; -f 1,6</span><br></pre></td></tr></table></figure>
<p>打印 /etc/passwd 文件中每一行的前 N 个字符：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 前五个（包含第五个）</span><br><span class="line">cut /etc/passwd -c -5</span><br><span class="line"># 前五个之后的（包含第五个）</span><br><span class="line">cut /etc/passwd -c 5-</span><br><span class="line"># 第五个</span><br><span class="line">cut /etc/passwd -c 5</span><br><span class="line"># 2 到 5 之间的（包含第五个）</span><br><span class="line">cut /etc/passwd -c 2-5</span><br></pre></td></tr></table></figure>
<h3 id="wc-命令">wc 命令</h3>
<p>wc 命令，简单小巧的计数工具</p>
<p>wc 命令用于统计并输出一个文件中行、单词和字节的数目，比如输出 /etc/passwd 文件的统计信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wc /etc/passwd</span><br></pre></td></tr></table></figure>
<p>分别只输出行数、单词数、字节数、字符数和输入文本中最长一行的字节数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 行数</span><br><span class="line">wc -l /etc/passwd</span><br><span class="line"># 单词数</span><br><span class="line">wc -w /etc/passwd</span><br><span class="line"># 字节数</span><br><span class="line">wc -c /etc/passwd</span><br><span class="line"># 字符数</span><br><span class="line">wc -m /etc/passwd</span><br><span class="line"># 最长行字节数</span><br><span class="line">wc -L /etc/passwd</span><br></pre></td></tr></table></figure>
<h3 id="sort-命令">sort 命令</h3>
<p>将输入按照一定方式排序，然后再输出，它支持的排序有按字典排序，数字排序，按月份排序，随机排序，反转排序，指定特定字段进行排序等等。</p>
<p>默认为字典排序：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /etc/passwd | sort</span><br></pre></td></tr></table></figure>
<p>反转排序：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /etc/passwd | sort -r</span><br></pre></td></tr></table></figure>
<p>按特定字段排序：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">at /etc/passwd | sort -t&#x27;:&#x27; -k 3</span><br></pre></td></tr></table></figure>
<p>上面的<code>-t</code>参数用于指定字段的分隔符，这里是以&quot;:&quot;作为分隔符；<code>-k</code> 字段号用于指定对哪一个字段进行排序。这里/etc/passwd 文件的第三个字段为数字，默认情况下是以字典序排序的，如果要按照数字排序就要加上<code>-n</code>参数。</p>
<h3 id="uniq-去重命令">uniq 去重命令</h3>
<p>uniq 命令可以用于过滤或者输出重复行。</p>
<ul>
<li>过滤重复行</li>
</ul>
<p>我们可以使用 history 命令查看最近执行过的命令（实际为读取 ${SHELL}_history 文件，如我们环境中的 .zsh_history 文件），不过你可能只想查看使用了哪个命令而不需要知道具体干了什么，那么你可能就会要想去掉命令后面的参数然后去掉重复的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">history | cut -c 8- | cut -d &#x27; &#x27; -f 1 | uniq</span><br></pre></td></tr></table></figure>
<p>然后经过层层过滤，你会发现确是只输出了执行的命令那一列，不过去重效果好像不明显，仔细看你会发现它确实去重了，只是不那么明显，之所以不明显是因为 uniq 命令只能去连续重复的行，不是全文去重，所以要达到预期效果，我们先排序：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">history | cut -c 8- | cut -d &#x27; &#x27; -f 1 | sort | uniq</span><br><span class="line"># 或者</span><br><span class="line">history | cut -c 8- | cut -d &#x27; &#x27; -f 1 | sort -u</span><br></pre></td></tr></table></figure>
<p>这就是 Linux/UNIX 哲学吸引人的地方，大繁至简，一个命令只干一件事却能干到最好。</p>
<ul>
<li>输出重复行</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 输出重复过的行（重复的只输出一个）及重复次数</span><br><span class="line">history | cut -c 8- | cut -d &#x27; &#x27; -f 1 | sort | uniq -dc</span><br><span class="line"># 输出所有重复的行</span><br><span class="line">history | cut -c 8- | cut -d &#x27; &#x27; -f 1 | sort | uniq -D</span><br></pre></td></tr></table></figure>
<p>文本处理命令还有很多，下一节将继续介绍一些常用的文本处理的命令。</p>
<h3 id="xargs-命令">xargs 命令</h3>
<p><code>xargs</code> 是一条 UNIX 和类 UNIX 操作系统的常用命令。它的作用是将参数列表转换成小块分段传递给其他命令，以避免参数列表过长的问题。</p>
<p>这个命令在有些时候十分有用，特别是当用来处理产生大量输出结果的命令如 find，locate 和 grep 的结果，详细用法请参看 man 文档。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cut -d: -f1 &lt; /etc/passwd | sort | xargs echo</span><br></pre></td></tr></table></figure>
<p>上面这个命令用于将 <code>/etc/passwd</code> 文件按 <code>:</code> 分割取第一个字段排序后，使用 <code>echo</code> 命令生成一个列表。</p>
<h2 id="Tests">Tests</h2>
<ol>
<li>data1 文件里记录是一些命令的操作记录，现在需要你从里面找出出现频率次数前 3 的命令并保存在 /home/shiyanlou/result。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> data1 |<span class="built_in">cut</span> -c 8-|<span class="built_in">sort</span>|<span class="built_in">uniq</span> -dc|<span class="built_in">sort</span> -rn -k1 |<span class="built_in">head</span> -3 &gt; /home/shiyanlou/result</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Linux 系统概述</title>
    <url>/2022/06/01/cb58b1fff61847a1bb694c22e8c7082c/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="1-1-Linux-简介">1.1 Linux 简介</h2>
<h3 id="1-1-1-操作系统的概念">1.1.1 操作系统的概念</h3>
<p>操作系统是管理和控制计算机硬件资源的计算机程序。</p>
<p><img src="/resource/270fd8bde0c94eeab93433d2172291e0.png" alt="2022-04-18-23-34-55.png"></p>
<p>核心功能：</p>
<ul>
<li>syscall 接口</li>
<li>程序管理</li>
<li>内存管理</li>
<li>文件系统管理</li>
<li>驱动管理</li>
</ul>
<h3 id="1-1-2-常见的操作系统">1.1.2 常见的操作系统</h3>
<ul>
<li>DOS</li>
<li>Windows</li>
<li>Unix</li>
<li>Linux</li>
<li>Mac OS</li>
<li>Android</li>
<li>IOS</li>
</ul>
<h3 id="1-1-3-什么是-Linux">1.1.3 什么是 Linux</h3>
<p>Linux 是一套免费使用和自由传播的类 Unix 操作系统。<br>
Linux 可以管理计算机所有硬件资源，进行 CPU 调度，分配工作桌面。</p>
<p><img src="/resource/cfa2c297f8604d17b453a5228e7fc429.png" alt="2022-04-18-23-40-34.png"></p>
<h3 id="1-1-4-Linux-的结构">1.1.4 Linux 的结构</h3>
<ul>
<li>应用程序</li>
<li>shell 程序(Bash)</li>
<li>Linux 内核 kernel</li>
<li>硬件系</li>
</ul>
<p><img src="/resource/45424a5c4472482bbd7856544afa20fd.png" alt="2022-04-18-23-44-33.png"></p>
<p>常见的 shell：</p>
<ul>
<li>Bourne shell(sh)</li>
<li>C shell(csh) &amp; korn shell(ksh)</li>
<li>Bash shell(/bin/bash)</li>
</ul>
<h4 id="shell-常用快捷键">shell 常用快捷键</h4>
<p><img src="/resource/902036be5f60464ebafd07a467ff12aa.png" alt="2022-04-19-22-12-11.png"></p>
<h4 id="shell-常用通配符">shell 常用通配符</h4>
<p><img src="/resource/ff0f0fc9b9dd440ca225b5da5a00bcc9.png" alt="2022-04-19-22-12-32.png"></p>
<h4 id="man-命令-Manual-pages">man 命令(Manual pages)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">man &lt;command_name&gt;</span><br></pre></td></tr></table></figure>
<p><img src="/resource/3c7e9e5c45c9410ea10da4f4cf1cc4df.png" alt="2022-04-19-22-14-22.png"></p>
<h3 id="1-1-5-Linux-的特点">1.1.5 Linux 的特点</h3>
<ul>
<li>多任务，多用户</li>
<li>功能强大的开发者交互界面</li>
<li>安全保护机制，稳定性好</li>
<li>用户界面，强大的网络支持</li>
<li>移植性好</li>
</ul>
<h2 id="1-2-Linux-常用远程接入工具介绍">1.2 Linux 常用远程接入工具介绍</h2>
<ul>
<li>SSH 客户端：Xshell</li>
<li>SFTP 客户端：Xftp</li>
</ul>
<h2 id="1-3-Linux-目录结构">1.3 Linux 目录结构</h2>
<p><img src="/resource/60162b2d23b046ccaa994e1d77bc71a9.png" alt="2022-04-18-23-51-37.png"></p>
<h1>2. Linux 用户管理</h1>
<h2 id="2-1-Linux-的用户和群组">2.1 Linux 的用户和群组</h2>
<h3 id="2-1-1-用户标志符：UID-与-GID">2.1.1 用户标志符：UID 与 GID</h3>
<ul>
<li>Linux 系统是通过 ID 来区分用户的，ID 分为 User ID 和 Group ID，用户名或者组名是指方便人们的记忆。</li>
<li>用户 ID 保存路径： /etc/passwd</li>
<li>组 ID 保存路径： /etc/group</li>
</ul>
<h3 id="2-1-2-etc-passwd-文件结构">2.1.2 /etc/passwd 文件结构</h3>
<ul>
<li>passwd 文件中记录的是单个用户的登录信息。</li>
<li>每一行代表一个用户，用冒号“：”分隔成七个字段，记录用户的 7 项信息。</li>
</ul>
<p><img src="/resource/f73c396217c340f6a515a68719b9ce67.png" alt="2022-04-19-15-30-42.png"></p>
<p><img src="/resource/89a5590cd44c48529efd6db9b09af618.png" alt="2022-04-19-15-24-15.png"></p>
<h3 id="2-1-3-etc-shadow-文件结构">2.1.3 /etc/shadow 文件结构</h3>
<ul>
<li>用户密码信息存储在/etc/shadow 文件中，文件中每一行表示一个系统用户的密码记录，用冒号“：”分隔成八项，支持密码过期设定等功能。</li>
</ul>
<p><img src="/resource/15f78a3dc0d14448b5daaabdd8bba9d1.png" alt="2022-04-19-15-32-31.png"></p>
<p><img src="/resource/ef713006a2d845a180f429ce846127ff.png" alt="2022-04-19-15-25-26.png"></p>
<h3 id="2-1-4-etc-group-文件结构">2.1.4 /etc/group 文件结构</h3>
<ul>
<li>/etc/group 文件记录了 GID 和组名的对应关系，以及群组中的包含的用户。</li>
</ul>
<p><img src="/resource/325af9c3314c421e84fcd147e94fd87b.png" alt="2022-04-19-15-30-04.png"></p>
<p>字段含义：</p>
<ol>
<li>组名</li>
<li>组的密码</li>
<li>群组的 ID</li>
<li>群组包含的用户</li>
</ol>
<h2 id="2-2-群组管理">2.2 群组管理</h2>
<h3 id="2-2-1-新增群组">2.2.1 新增群组</h3>
<p><img src="/resource/6d48099c9ee34c29949c9ea8df58801d.png" alt="2022-04-19-15-37-43.png"></p>
<h3 id="2-2-2-修改群组">2.2.2 修改群组</h3>
<p><img src="/resource/089a7c2e1a57409daa94cd143b2dd862.png" alt="2022-04-19-15-39-08.png"></p>
<h3 id="2-2-3-删除群组">2.2.3 删除群组</h3>
<p><img src="/resource/e1c4df5f02da420fb65267425a6903ab.png" alt="2022-04-19-15-40-04.png"></p>
<h2 id="2-3-用户管理">2.3 用户管理</h2>
<h3 id="2-3-1-新增用户">2.3.1 新增用户</h3>
<p><img src="/resource/00549df0efa54bd4adca868554ae749c.png" alt="2022-04-19-15-43-00.png"></p>
<h3 id="2-3-2-设置和修改用户密码">2.3.2 设置和修改用户密码</h3>
<p><img src="/resource/c19526780d6e427da3a8ba8cf1a6f2fd.png" alt="2022-04-19-15-44-51.png"></p>
<h3 id="2-3-3-修改用户属性">2.3.3 修改用户属性</h3>
<p><img src="/resource/8108650b298942da91dbf286dc23c6f7.png" alt="2022-04-19-15-46-42.png"></p>
<h3 id="2-3-4-删除用户">2.3.4 删除用户</h3>
<p><img src="/resource/61399595377f402bbe2e9c3216139837.png" alt="2022-04-19-15-47-17.png"></p>
<h3 id="2-3-5-用户查询相关命令">2.3.5 用户查询相关命令</h3>
<ul>
<li>who: 查询当前登录系统的所有用户</li>
<li>id: 查询当前用户的 GID, UID</li>
<li>finger: 查询用户的属性信息</li>
</ul>
<h3 id="2-3-6-切换用户">2.3.6 切换用户</h3>
<p>改变用户 ID</p>
<p><img src="/resource/be22451938b54489bc5919cfb3feb5d6.png" alt="2022-04-19-15-50-47.png"></p>
<h1>3. Linux 文件和目录管理</h1>
<h2 id="3-1-目录和路径">3.1 目录和路径</h2>
<p><img src="/resource/f88b765490ef4fccb7411ce3731a782a.png" alt="2022-04-20-14-29-24.png"></p>
<h3 id="3-1-1-绝对路径和相对路径">3.1.1 绝对路径和相对路径</h3>
<ul>
<li><strong>绝对路径</strong>：由根目录（/）开始写起的文件名或者目录名称，如/home/smc /bin/smsc</li>
<li><strong>相对路径</strong>：相当于当前路径的文件名或者目录名称写法。如 …/…/mtserver
<ul>
<li><code>.</code>表示当前目录, <code>..</code>表示上一级目录</li>
</ul>
</li>
</ul>
<h3 id="3-1-2-显示当前工作目录">3.1.2 显示当前工作目录</h3>
<p><code>pwd</code>：本命令用于显示当前的工作目录</p>
<h3 id="3-1-3-更改工作目录">3.1.3 更改工作目录</h3>
<p><code>cd [目录]</code>：本命令用于改变当前的工作目录，无参数时使用环境变量<code>$HOME</code>作为其参数，<code>$HOME</code>一般为登录时进入的路径。其中路径包括绝对路径和相对路径。</p>
<h2 id="3-2-Linux-文件和目录权限管理">3.2 Linux 文件和目录权限管理</h2>
<h3 id="3-2-1-文件的权限意义">3.2.1 文件的权限意义</h3>
<ul>
<li>文件的权限意义
<ul>
<li>r(read): 可读取此文件的实际内容</li>
<li>w(write): 可以编辑此文件内容(不含删除该文件)</li>
<li>x(execute): 该文件具有被系统执行的权限</li>
</ul>
</li>
<li>目录的权限意义
<ul>
<li>r(read): 具有读取目录结构列表的权限</li>
<li>w(write): 具有修改目录结构列表的权限</li>
<li>x(execute): 具有进入该目录的权限</li>
</ul>
</li>
</ul>
<h4 id="3-2-2-1-查看文件或目录">3.2.2.1 查看文件或目录</h4>
<p><code>ls -[option] [目录或者文件...]</code><br>
常用命令选项（可混合使用）：</p>
<ul>
<li><code>-l</code>：以长格式列出目录下的文件</li>
<li><code>-a</code>：以短格式列出目录下的所有文件(包含隐含文件</li>
<li><code>ll = ls -al</code></li>
</ul>
<p><code>ls -asSh</code>: 显示所有文件大小，其中小 s 为显示文件大小，大 S 为按文件大小排序，若需要知道如何按其它方式排序，可以使用 <code>man ls</code> 命令查询。</p>
<h4 id="3-2-2-2-文件属性和权限说明">3.2.2.2 文件属性和权限说明</h4>
<ul>
<li>文件是 Linux 操作系统中组织信息的基本单元。</li>
</ul>
<p><img src="/resource/66b61d2673b640eeb77fe2b14a3dbb7b.png" alt="2022-04-19-16-07-41.png"></p>
<p><img src="/resource/4d2ec50237be41deadf675afa7521069.png" alt="2022-04-19-16-07-48.png"></p>
<h3 id="3-3-1-修改属主">3.3.1 修改属主</h3>
<p><img src="/resource/025dbf5acb814bdc817bedeaee872935.png" alt="2022-04-19-16-10-38.png"></p>
<h3 id="3-3-2-修改所属群组">3.3.2 修改所属群组</h3>
<p><img src="/resource/807cc3dcb1414e7f8e68d004fedd6df1.png" alt="2022-04-19-16-11-49.png"></p>
<h3 id="3-3-3-修改权限">3.3.3 修改权限</h3>
<p><img src="/resource/2ed60ebb6dc344268e51a9038408903e.png" alt="2022-04-19-16-12-35.png"></p>
<h2 id="3-3-文件和目录基本操作">3.3 文件和目录基本操作</h2>
<h3 id="3-3-1-新建文件和目录">3.3.1 新建文件和目录</h3>
<p><img src="/resource/2375b26280fd40239ac39ca48dfd0ee9.png" alt="2022-04-19-16-14-08.png"></p>
<p><img src="/resource/3b424cf4c6f146ac9612cf10721ed006.png" alt="2022-04-19-16-14-41.png"></p>
<h3 id="3-3-2-复制文件或目录">3.3.2 复制文件或目录</h3>
<p>本地主机复制文件或目录，要求对其父目录有写权限：</p>
<p><img src="/resource/c7abcb3b25084586accc19e7c93ea0e8.png" alt="2022-04-19-16-15-59.png"></p>
<p>网络互通的远程主机复制文件或目录：</p>
<p><img src="/resource/747652711ce74648b4c630b760b6f7b4.png" alt="2022-04-19-16-17-37.png"></p>
<h3 id="3-3-3-移动文件或目录">3.3.3 移动文件或目录</h3>
<p><img src="/resource/a911882a7bb64124aa964cb8c48e754f.png" alt="2022-04-19-16-19-02.png"></p>
<h3 id="3-3-4-删除文件或目录">3.3.4 删除文件或目录</h3>
<p>删除空目录：</p>
<p><img src="/resource/5c9c888d1e034826802ec89fefd25a32.png" alt="2022-04-19-16-19-38.png"></p>
<p>删除文件或目录：</p>
<p><img src="/resource/5479d1bdb00a46888691e6748b26bcbd.png" alt="2022-04-19-16-23-32.png"></p>
<h1>4. Linux 下的软件安装</h1>
<p>通常 Linux 上的软件安装主要有四种方式：</p>
<ul>
<li>在线安装</li>
<li>从磁盘安装 deb 软件包</li>
<li>从二进制软件包安装</li>
<li>从源代码编译安装</li>
</ul>
<p>这几种安装方式各有优劣，而大多数软件包会采用多种方式发布软件，所以我们常常需要全部掌握这几种软件安装方式，以便适应各种环境。下面将介绍前三种安装方式，从源码编译安装你将在 Linux 程序设计中学习到。</p>
<h2 id="4-1-apt-包管理工具介绍">4.1 apt 包管理工具介绍</h2>
<p>APT 是 Advance Packaging Tool（高级包装工具）的缩写，是 Debian 及其派生发行版的软件包管理器，APT 可以自动下载，配置，安装二进制或者源代码格式的软件包，因此简化了 Unix 系统上管理软件的过程。APT 最早被设计成 dpkg 的前端，用来处理 deb 格式的软件包。现在经过 APT-RPM 组织修改，APT 已经可以安装在支持 RPM 的系统管理 RPM 包。这个包管理器包含以 <code>apt-</code> 开头的多个工具，如 <code>apt-get</code> <code>apt-cache</code> <code>apt-cdrom</code> 等，在 Debian 系列的发行版中使用。</p>
<p>当你在执行安装操作时，首先 <code>apt-get</code> 工具会在本地的一个数据库中搜索关于 <code>w3m</code> 软件的相关信息，并根据这些信息在相关的服务器上下载软件安装，这里大家可能会一个疑问：既然是在线安装软件，为啥会在本地的数据库中搜索？要解释这个问题就得提到几个名词了：</p>
<ul>
<li>软件源镜像服务器</li>
<li>软件源</li>
</ul>
<p>我们需要定期从服务器上下载一个软件包列表，使用 <code>sudo apt-get update</code> 命令来保持本地的软件包列表是最新的（有时你也需要手动执行这个操作，比如更换了软件源），而这个表里会有软件依赖信息的记录，对于软件依赖，我举个例子：我们安装 <code>w3m</code> 软件的时候，而这个软件需要 <code>libgc1c2</code> 这个软件包才能正常工作，这个时候 <code>apt-get</code> 在安装软件的时候会一并替我们安装了，以保证 <code>w3m</code> 能正常的工作。</p>
<h2 id="4-2-apt-get">4.2 apt-get</h2>
<p><code>apt-get</code> 是用于处理 <code>apt</code> 包的公用程序集，我们可以用它来在线安装、卸载和升级软件包等，下面列出一些 <code>apt-get</code> 包含的常用的一些工具：</p>
<table>
<thead>
<tr>
<th>工具</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>install</code></td>
<td>其后加上软件包名，用于安装一个软件包</td>
</tr>
<tr>
<td><code>update</code></td>
<td>从软件源镜像服务器上下载/更新用于更新本地软件源的软件包列表`</td>
</tr>
<tr>
<td><code>upgrade</code></td>
<td>升级本地可更新的全部软件包，但存在依赖问题时将不会升级，通常会在更新之前执行一次 update</td>
</tr>
<tr>
<td><code>dist-upgrade</code></td>
<td>解决依赖关系并升级（存在一定危险性）</td>
</tr>
<tr>
<td><code>remove</code></td>
<td>移除已安装的软件包，包括与被移除软件包有依赖关系的软件包，但不包含软件包的配置文件</td>
</tr>
<tr>
<td><code>autoremove</code></td>
<td>移除之前被其他软件包依赖，但现在不再被使用的软件包</td>
</tr>
<tr>
<td><code>purge</code></td>
<td>与 <code>remove</code> 相同，但会完全移除软件包，包含其配置文件</td>
</tr>
<tr>
<td><code>clean</code></td>
<td>移除下载到本地的已经安装的软件包，默认保存在 <code>/var/cache/apt/archives/</code></td>
</tr>
<tr>
<td><code>autoclean</code></td>
<td>移除已安装的软件的旧版本软件包</td>
</tr>
</tbody>
</table>
<p>下面是一些<code>apt-get</code>常用的参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-y</code></td>
<td>自动回应是否安装软件包的选项，在一些自动化安装脚本中使用这个参数将十分有用</td>
</tr>
<tr>
<td><code>-s</code></td>
<td>模拟安装</td>
</tr>
<tr>
<td><code>-q</code></td>
<td>静默安装方式，指定多个 <code>q</code> 或者 <code>-q=#</code>，<code>#</code> 表示数字，用于设定静默级别，这在你不想要在安装软件包时屏幕输出过多时很有用</td>
</tr>
<tr>
<td><code>-f</code></td>
<td>修复损坏的依赖关系</td>
</tr>
<tr>
<td><code>-d</code></td>
<td>只下载不安装</td>
</tr>
<tr>
<td><code>--reinstall</code></td>
<td>重新安装已经安装但可能存在问题的软件包</td>
</tr>
<tr>
<td><code>--install-suggests</code></td>
<td>同时安装 <code>APT</code> 给出的建议安装的软件包</td>
</tr>
</tbody>
</table>
<h2 id="4-4-软件升级">4.4 软件升级</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新软件源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级没有依赖问题的软件包</span></span><br><span class="line">sudo apt-get upgrade</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级并解决依赖关系</span></span><br><span class="line">sudo apt-get dist-upgrade</span><br></pre></td></tr></table></figure>
<h2 id="4-5-卸载软件">4.5 卸载软件</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不保留配置文件的移除</span></span><br><span class="line">sudo apt-get purge [package]</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">sudo apt-get --purge remove [package]</span><br><span class="line"><span class="comment"># 移除不再需要的被依赖的软件包</span></span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure>
<h2 id="4-6-使用-dpkg">4.6 使用 dpkg</h2>
<p><code>dpkg</code> 是 Debian 软件包管理器的基础，它被伊恩·默多克创建于 1993 年。<code>dpkg</code> 与 <code>RPM</code> 十分相似，同样被用于安装、卸载和供给和 <code>.deb</code> 软件包相关的信息。</p>
<p><code>dpkg</code> 本身是一个底层的工具。上层的工具，像是 <code>APT</code>，被用于从远程获取软件包以及处理复杂的软件包关系。&quot;dpkg&quot;是&quot;Debian Package&quot;的简写。</p>
<p>我们经常可以在网络上见到以<code>deb</code>形式打包的软件包，就需要使用<code>dpkg</code>命令来安装。</p>
<p><code>dpkg</code>常用参数介绍：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-i</code></td>
<td>安装指定 deb 包</td>
</tr>
<tr>
<td><code>-R</code></td>
<td>后面加上目录名，用于安装该目录下的所有 deb 安装包</td>
</tr>
<tr>
<td><code>-r</code></td>
<td>remove，移除某个已安装的软件包</td>
</tr>
<tr>
<td><code>-I</code></td>
<td>显示 deb 包文件的信息</td>
</tr>
<tr>
<td><code>-s</code></td>
<td>显示已安装软件的信息</td>
</tr>
<tr>
<td><code>-S</code></td>
<td>搜索已安装的软件包</td>
</tr>
<tr>
<td><code>-L</code></td>
<td>显示已安装软件包的目录信息</td>
</tr>
</tbody>
</table>
<h2 id="4-7-从二进制包安装">4.7 从二进制包安装</h2>
<p>二进制包的安装比较简单，我们需要做的只是将从网络上下载的二进制包解压后放到合适的目录，然后将包含可执行的主程序文件的目录添加进 PATH 环境变量即可。</p>
]]></content>
  </entry>
  <entry>
    <title>Linux进程管理</title>
    <url>/2022/06/01/f9b8456db95444d792d4f1d9e9236bb0/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="进程的概念">进程的概念</h2>
<p>首先程序与进程是什么？程序与进程又有什么区别？</p>
<ul>
<li>程序（procedure）：不太精确地说，程序就是执行一系列有逻辑、有顺序结构的指令，帮我们达成某个结果。就如我们去餐馆，给服务员说我要牛肉盖浇饭，她执行了做牛肉盖浇饭这么一个程序，最后我们得到了这么一盘牛肉盖浇饭。它需要去执行，不然它就像一本武功秘籍，放在那里等人翻看。</li>
<li>进程（process）：进程是程序在一个数据集合上的一次执行过程，在早期的 UNIX、Linux 2.4 及更早的版本中，它是系统进行资源分配和调度的独立基本单位。同上一个例子，就如我们去了餐馆，给服务员说我要牛肉盖浇饭，她执行了做牛肉盖浇饭这么一个程序，而里面做饭的是一个进程，做牛肉汤汁的是一个进程，把牛肉汤汁与饭混合在一起的是一个进程，把饭端上桌的是一个进程。它就像是我们在看武功秘籍这么一个过程，然后一个篇章一个篇章地去练。</li>
</ul>
<p>程序只是一些列指令的集合，是一个静止的实体，而进程不同，进程有以下的特性：</p>
<ul>
<li>动态性：进程的实质是一次程序执行的过程，有创建、撤销等状态的变化。而程序是一个静态的实体。</li>
<li>并发性：进程可以做到在一个时间段内，有多个程序在运行中。程序只是静态的实体，所以不存在并发性。</li>
<li>独立性：进程可以独立分配资源，独立接受调度，独立地运行。</li>
<li>异步性：进程以不可预知的速度向前推进。</li>
<li>结构性：进程拥有代码段、数据段、PCB（进程控制块，进程存在的唯一标志）。也正是因为有结构性，进程才可以做到独立地运行。</li>
</ul>
<p>并发和并行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">并发：在一个时间段内，宏观来看有多个程序都在活动，有条不紊的执行（每一瞬间只有一个在执行，只是在一段时间有多个程序都执行过）</span><br><span class="line"></span><br><span class="line">并行：在每一个瞬间，都有多个程序都在同时执行，这个必须有多个 CPU 才行</span><br></pre></td></tr></table></figure>
<p>引入进程是因为传统意义上的程序已经不足以描述 OS 中各种活动之间的动态性、并发性、独立性还有相互制约性。程序就像一个公司，只是一些证书，文件的堆积（静态实体）。而当公司运作起来就有各个部门的区分，财务部，技术部，销售部等等，就像各个进程，各个部门之间可以独立运作，也可以有交互（独立性、并发性）。</p>
<p>而随着程序的发展越做越大，又会继续细分，从而引入了线程的概念，当代多数操作系统、Linux 2.6 及更新的版本中，进程本身不是基本运行单位，而是线程的容器。就像上述所说的，每个部门又会细分为各个工作小组（线程），而工作小组需要的资源需要向上级（进程）申请。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">线程（thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。因为线程中几乎不包含系统资源，所以执行更快、更有效率。</span><br></pre></td></tr></table></figure>
<p>简而言之，一个程序至少有一个进程，一个进程至少有一个线程。线程的划分尺度小于进程，使得多线程程序的并发性高。另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。</p>
<h2 id="进程的分类">进程的分类</h2>
<p>进程可以从两个角度来分：</p>
<ul>
<li>以进程的功能与服务的对象来分；</li>
<li>以应用程序的服务类型来分；</li>
</ul>
<p>第一个角度来看，我们可以分为<strong>用户进程</strong>与<strong>系统进程</strong>：</p>
<ul>
<li><strong>用户进程</strong>：通过执行用户程序、应用程序或称之为内核之外的系统程序而产生的进程，此类进程可以在用户的控制下运行或关闭。</li>
<li><strong>系统进程</strong>：通过执行系统内核程序而产生的进程，比如可以执行内存资源分配和进程切换等相对底层的工作；而且该进程的运行不受用户的干预，即使是 root 用户也不能干预系统进程的运行。</li>
</ul>
<p>第二角度来看，我们可以将进程分为<strong>交互进程</strong>、<strong>批处理进程</strong>、<strong>守护进程</strong>：</p>
<ul>
<li><strong>交互进程</strong>：由一个 shell 终端启动的进程，在执行过程中，需要与用户进行交互操作，可以运行于前台，也可以运行在后台。</li>
<li><strong>批处理进程</strong>：该进程是一个进程集合，负责按顺序启动其他的进程。</li>
<li><strong>守护进程</strong>：守护进程是一直运行的一种进程，在 Linux 系统启动时启动，在系统关闭时终止。它们独立于控制终端并且周期性的执行某种任务或等待处理某些发生的事件。例如 httpd 进程，一直处于运行状态，等待用户的访问。还有经常用的 cron（在 centOS 系列为 crond）进程，这个进程为 crontab 的守护进程，可以周期性的执行用户设定的某些任务。</li>
</ul>
<h2 id="进程的衍生">进程的衍生</h2>
<p>进程有这么多的种类，那么进程之间定是有相关性的，而这些有关联性的进程又是如何产生的，如何衍生的？</p>
<p>就比如我们启动了终端，就是启动了一个 bash 进程，我们可以在 bash 中再输入 bash 则会再启动一个 bash 的进程，此时第二个 bash 进程就是由第一个 bash 进程创建出来的，他们之间又是个什么关系？</p>
<p>我们一般称呼第一个 bash 进程是第二 bash 进程的父进程，第二 bash 进程是第一个 bash 进程的子进程，这层关系是如何得来的呢？</p>
<p>关于父进程与子进程便会提及这两个系统调用 <code>fork()</code> 与 <code>exec()</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fork-exec是由 Dennis M. Ritchie 创造的</span><br><span class="line"></span><br><span class="line">fork() 是一个系统调用（system call），它的主要作用就是为当前的进程创建一个新的进程，这个新的进程就是它的子进程，这个子进程除了父进程的返回值和 PID 以外其他的都一模一样，如进程的执行代码段，内存信息，文件描述，寄存器状态等等</span><br><span class="line"></span><br><span class="line">exec() 也是系统调用，作用是切换子进程中的执行程序也就是替换其从父进程复制过来的代码段与数据段</span><br></pre></td></tr></table></figure>
<p>子进程就是父进程通过系统调用 <code>fork()</code> 而产生的复制品，<code>fork()</code> 就是把父进程的 PCB 等进程的数据结构信息直接复制过来，只是修改了 PID，所以一模一样，只有在执行 <code>exec()</code> 之后才会不同，而早先的 <code>fork()</code> 比较消耗资源后来进化成 <code>vfork()</code>，效率高了不少。</p>
<p>子进程退出后，正常情况下，父进程会收到两个返回值：<code>exit code</code>（SIGCHLD 信号）与 <code>reason for termination</code> 。之后，父进程会使用 <code>wait(&amp;status)</code> 系统调用以获取子进程的退出状态，然后内核就可以从内存中释放已结束的子进程的 PCB；而如若父进程没有这么做的话，子进程的 PCB 就会一直驻留在内存中，一直留在系统中成为僵尸进程（Zombie）。</p>
<p>虽然僵尸进程是已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，在进程列表中保留一个位置，记载该进程的退出状态等信息供其父进程收集，从而释放它。但是 Linux 系统中能使用的 PID 是有限的，如果系统中存在有大量的僵尸进程，系统将会因为没有可用的 PID 从而导致不能产生新的进程。</p>
<p>另外如果父进程结束（非正常的结束），未能及时收回子进程，子进程仍在运行，这样的子进程称之为孤儿进程。在 Linux 系统中，孤儿进程一般会被 init 进程所“收养”，成为 init 的子进程。由 init 来做善后处理，所以它并不至于像僵尸进程那样无人问津，不管不顾，大量存在会有危害。</p>
<p>进程 0 是系统引导时创建的一个特殊进程，也称之为内核初始化，其最后一个动作就是调用 <code>fork()</code> 创建出一个子进程运行 <code>/sbin/init</code> 可执行文件，而该进程就是 PID=1 的进程 1，而进程 0 就转为交换进程（也被称为空闲进程），进程 1 （init 进程）是第一个用户态的进程，再由它不断调用 fork() 来创建系统里其他的进程，所以它是所有进程的父进程或者祖先进程。同时它是一个守护程序，直到计算机关机才会停止。</p>
<h2 id="进程组和-Sessions">进程组和 Sessions</h2>
<p>每一个进程都会是一个进程组的成员，而且这个进程组是唯一存在的，他们是依靠 PGID（process group ID）来区别的，而每当一个进程被创建的时候，它便会成为其父进程所在组中的一员。</p>
<p>一般情况，进程组的 PGID 等同于进程组的第一个成员的 PID，并且这样的进程称为该进程组的领导者，也就是领导进程，进程一般通过使用 <code>getpgrp()</code> 系统调用来寻找其所在组的 PGID，领导进程可以先终结，此时进程组依然存在，并持有相同的 PGID，直到进程组中最后一个进程终结。</p>
<p>与进程组类似，每当一个进程被创建的时候，它便会成为其父进程所在 Session 中的一员，每一个进程组都会在一个 Session 中，并且这个 Session 是唯一存在的，</p>
<p>Session 主要是针对一个 tty 建立，Session 中的每个进程都称为一个工作(job)。每个会话可以连接一个终端(control terminal)。当控制终端有输入输出时，都传递给该会话的前台进程组。Session 意义在于将多个 jobs 囊括在一个终端，并取其中的一个 job 作为前台，来直接接收该终端的输入输出以及终端信号。 其他 jobs 在后台运行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">前台（foreground）就是在终端中运行，能与你有交互的</span><br><span class="line"></span><br><span class="line">后台（background）就是在终端中运行，但是你并不能与其任何的交互，也不会显示其执行的过程</span><br></pre></td></tr></table></figure>
<h2 id="工作管理">工作管理</h2>
<p>bash(Bourne-Again shell)支持工作控制（job control），而 sh（Bourne shell）并不支持。</p>
<p>并且每个终端或者说 bash 只能管理当前终端中的 job，不能管理其他终端中的 job。比如我当前存在两个 bash 分别为 bash1、bash2，bash1 只能管理其自己里面的 job 并不能管理 bash2 里面的 job</p>
<p>我们都知道当一个进程在前台运作时我们可以用 <code>ctrl + c</code> 来终止它，但是若是在后台的话就不行了。</p>
<p>我们可以通过 <code>&amp;</code> 这个符号，让我们的命令在后台中运行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls &amp;</span><br></pre></td></tr></table></figure>
<p>我们还可以通过 <code>ctrl + z</code> 使我们的当前工作停止并丢到后台中去</p>
<p>被停止并放置在后台的工作我们可以使用这个命令来查看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">jobs</span><br></pre></td></tr></table></figure>
<p>我们可以通过这样的一个命令将后台的工作拿到前台来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 后面不加参数提取预设工作，加参数提取指定工作的编号</span><br><span class="line"># ubuntu 在 zsh 中需要 %，在 bash 中不需要 %</span><br><span class="line">fg [%jobnumber]</span><br></pre></td></tr></table></figure>
<p>之前我们通过 <code>ctrl + z</code> 使得工作停止放置在后台，若是我们想让其在后台运作我们就使用这样一个命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#与fg类似，加参则指定，不加参则取预设</span><br><span class="line">bg [%jobnumber]</span><br></pre></td></tr></table></figure>
<p>既然有方法将被放置在后台的工作提至前台或者让它从停止变成继续运行在后台，当然也有方法删除一个工作，或者重启等等。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kill的使用格式如下</span><br><span class="line">kill -signal %jobnumber</span><br><span class="line"></span><br><span class="line"># signal从1-64个信号值可以选择，可以这样查看</span><br><span class="line">kill －l</span><br></pre></td></tr></table></figure>
<p>其中常用的有这些信号值</p>
<table>
<thead>
<tr>
<th>信号值</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-1</code></td>
<td>重新读取参数运行，类似与 restart</td>
</tr>
<tr>
<td><code>-2</code></td>
<td>如同 ctrl+c 的操作退出</td>
</tr>
<tr>
<td><code>-9</code></td>
<td>强制终止该任务</td>
</tr>
<tr>
<td><code>-15</code></td>
<td>正常的方式终止该任务</td>
</tr>
</tbody>
</table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">若是在使用 kill ＋信号值然后直接加 pid，你将会对 pid 对应的进程进行操作。</span><br><span class="line"></span><br><span class="line">若是在使用 kill+信号值然后 ％jobnumber，这时所操作的对象是 job，这个数字就是就当前 bash 中后台的运行的 job 的 ID</span><br></pre></td></tr></table></figure>
<h2 id="Linux-进程管理">Linux 进程管理</h2>
<h3 id="进程的查看">进程的查看</h3>
<p>不管在测试的时候、在实际的生产环境中，还是自己的使用过程中，难免会遇到一些进程异常的情况，所以 Linux 为我们提供了一些工具来查看进程的状态信息。我们可以通过 <code>top</code> 实时的查看进程的状态，以及系统的一些信息（如 CPU、内存信息等），我们还可以通过 <code>ps</code> 来静态查看当前的进程信息，同时我们还可以使用 <code>pstree</code> 来查看当前活跃进程的树形结构。</p>
<p><code>top</code> 工具是我们常用的一个查看工具，能实时的查看我们系统的一些关键信息的变化。</p>
<p><img src="/resource/6c31b0f46b9649d1aced5b2ff1f92462.png" alt="2022-05-15-13-52-12.png"></p>
<p><code>top</code> 是一个在前台执行的程序，所以执行后便进入到这样的一个交互界面，正是因为交互界面我们才可以实时的获取到系统与进程的信息。在交互界面中我们可以通过一些指令来操作和筛选。在此之前我们先来了解显示了哪些信息。</p>
<p>我们看到 <code>top</code> 显示的第一排，</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>top</td>
<td>表示当前程序的名称</td>
</tr>
<tr>
<td>11:05:18</td>
<td>表示当前的系统的时间</td>
</tr>
<tr>
<td>up 8 days,17:12</td>
<td>表示该机器已经启动了多长时间</td>
</tr>
<tr>
<td>1 user</td>
<td>表示当前系统中只有一个用户</td>
</tr>
<tr>
<td>load average: 0.29,0.20,0.25</td>
<td>分别对应 1、5、15 分钟内 cpu 的平均负载</td>
</tr>
</tbody>
</table>
<p>load average 在 wikipedia 中的解释是 the system load is a measure of the amount of work that a computer system is doing 也就是对当前 CPU 工作量的度量，具体来说也就是指运行队列的平均长度，也就是等待 CPU 的平均进程数相关的一个计算值。</p>
<p>我们该如何看待这个 load average 数据呢？</p>
<p>假设我们的系统是单 CPU、单内核的，把它比喻成是一条单向的桥，把 CPU 任务比作汽车。</p>
<ul>
<li>load = 0 的时候意味着这个桥上并没有车，cpu 没有任何任务；</li>
<li>load &lt; 1 的时候意味着桥上的车并不多，一切都还是很流畅的，cpu 的任务并不多，资源还很充足；</li>
<li>load = 1 的时候就意味着桥已经被车给占满了，没有一点空隙，cpu 已经在全力工作了，所有的资源都被用完了，当然还好，这还在能力范围之内，只是有点慢而已；</li>
<li>load &gt; 1 的时候就意味着不仅仅是桥上已经被车占满了，就连桥外都被占满了，cpu 已经在全力工作，系统资源的用完了，但是还是有大量的进程在请求，在等待。若是这个值大于 2 表示进程请求超过 CPU 工作能力的 2 倍。而若是这个值大于 5 说明系统已经在超负荷运作了。</li>
</ul>
<p>这是单个 CPU 单核的情况，而实际生活中我们需要将得到的这个值除以我们的核数来看。我们可以通过以下的命令来查看 CPU 的个数与核心数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#查看物理 CPU 的个数</span><br><span class="line">cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq |wc -l</span><br><span class="line"></span><br><span class="line">#每个 cpu 的核心数</span><br><span class="line">cat /proc/cpuinfo | grep &quot;physical id&quot; | grep &quot;0&quot; | wc -l</span><br></pre></td></tr></table></figure>
<p><code>top</code> 的第二行数据，基本上第二行是进程的一个情况统计：</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tasks: 26 total</td>
<td>进程总数</td>
</tr>
<tr>
<td>1 running</td>
<td>1 个正在运行的进程数</td>
</tr>
<tr>
<td>25 sleeping</td>
<td>25 个睡眠的进程数</td>
</tr>
<tr>
<td>0 stopped</td>
<td>没有停止的进程数</td>
</tr>
<tr>
<td>0 zombie</td>
<td>没有僵尸进程数</td>
</tr>
</tbody>
</table>
<p><code>top</code> 的第三行数据，这一行基本上是 CPU 的一个使用情况的统计了：</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cpu(s): 1.0%us</td>
<td>用户空间进程占用 CPU 百分比</td>
</tr>
<tr>
<td>1.0% sy</td>
<td>内核空间运行占用 CPU 百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>用户进程空间内改变过优先级的进程占用 CPU 百分比</td>
</tr>
<tr>
<td>97.9%id</td>
<td>空闲 CPU 百分比</td>
</tr>
<tr>
<td>0.0%wa</td>
<td>等待输入输出的 CPU 时间百分比</td>
</tr>
<tr>
<td>0.1%hi</td>
<td>硬中断(Hardware IRQ)占用 CPU 的百分比</td>
</tr>
<tr>
<td>0.0%si</td>
<td>软中断(Software IRQ)占用 CPU 的百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>(Steal time) 是 hypervisor 等虚拟服务中，虚拟 CPU 等待实际 CPU 的时间的百分比</td>
</tr>
</tbody>
</table>
<p>CPU 利用率是对一个时间段内 CPU 使用状况的统计，通过这个指标可以看出在某一个时间段内 CPU 被占用的情况，而 Load Average 是 CPU 的 Load，它所包含的信息不是 CPU 的使用率状况，而是在一段时间内 CPU 正在处理以及等待 CPU 处理的进程数情况统计信息，这两个指标并不一样。</p>
<p><code>top</code> 的第四行数据，这一行基本上是内存的一个使用情况的统计了：</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>8176740 total</td>
<td>物理内存总量</td>
</tr>
<tr>
<td>8032104 used</td>
<td>使用的物理内存总量</td>
</tr>
<tr>
<td>144636 free</td>
<td>空闲内存总量</td>
</tr>
<tr>
<td>313088 buffers</td>
<td>用作内核缓存的内存量</td>
</tr>
</tbody>
</table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">系统中可用的物理内存最大值并不是 free 这个单一的值，而是 free + buffers + swap 中的 cached 的和。</span><br></pre></td></tr></table></figure>
<p><code>top</code> 的第五行数据，这一行基本上是交换区的一个使用情况的统计了：</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>total</td>
<td>交换区总量</td>
</tr>
<tr>
<td>used</td>
<td>使用的交换区总量</td>
</tr>
<tr>
<td>free</td>
<td>空闲交换区总量</td>
</tr>
<tr>
<td>cached</td>
<td>缓冲的交换区总量，内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖</td>
</tr>
</tbody>
</table>
<p>再下面就是进程的一个情况了</p>
<table>
<thead>
<tr>
<th>列名</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>PID</td>
<td>进程 id</td>
</tr>
<tr>
<td>USER</td>
<td>该进程的所属用户</td>
</tr>
<tr>
<td>PR</td>
<td>该进程执行的优先级 priority 值</td>
</tr>
<tr>
<td>NI</td>
<td>该进程的 nice 值</td>
</tr>
<tr>
<td>VIRT</td>
<td>该进程任务所使用的虚拟内存的总数</td>
</tr>
<tr>
<td>RES</td>
<td>该进程所使用的物理内存数，也称之为驻留内存数</td>
</tr>
<tr>
<td>SHR</td>
<td>该进程共享内存的大小</td>
</tr>
<tr>
<td>S</td>
<td>该进程进程的状态: S=sleep R=running Z=zombie</td>
</tr>
<tr>
<td>%CPU</td>
<td>该进程 CPU 的利用率</td>
</tr>
<tr>
<td>%MEM</td>
<td>该进程内存的利用率</td>
</tr>
<tr>
<td>TIME+</td>
<td>该进程活跃的总时间</td>
</tr>
<tr>
<td>COMMAND</td>
<td>该进程运行的名字</td>
</tr>
</tbody>
</table>
<p><strong>NICE 值</strong>叫做静态优先级，是用户空间的一个优先级值，其取值范围是 -20 至 19。这个值越小，表示进程”优先级”越高，而值越大“优先级”越低。nice 值中的 -20 到 19，中 -20 优先级最高， 0 是默认的值，而 19 优先级最低。</p>
<p><strong>PR 值</strong>表示 Priority 值叫动态优先级，是进程在内核中实际的优先级值，进程优先级的取值范围是通过一个宏定义的，这个宏的名称是 MAX_PRIO，它的值为 140。Linux 实际上实现了 140 个优先级范围，取值范围是从 0-139，这个值越小，优先级越高。而这其中的 0-99 是实时进程的值，而 100-139 是给用户的。</p>
<p>其中 PR 中的 100 to 139 值部分有这么一个对应 PR = 20 + (-20 to +19)，这里的 -20 to +19 便是 nice 值，所以说两个虽然都是优先级，而且有千丝万缕的关系，但是他们的值，他们的作用范围并不相同。</p>
<p>VIRT 任务所使用的虚拟内存的总数，其中包含所有的代码，数据，共享库和被换出 swap 空间的页面等所占据空间的总数。</p>
<p><code>top</code> 是一个前台程序，所以是一个可以交互的：</p>
<table>
<thead>
<tr>
<th>常用交互命令</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>q</td>
<td>退出程序</td>
</tr>
<tr>
<td>I</td>
<td>切换显示平均负载和启动时间的信息</td>
</tr>
<tr>
<td>P</td>
<td>根据 CPU 使用百分比大小进行排序</td>
</tr>
<tr>
<td>M</td>
<td>根据驻留内存大小进行排序</td>
</tr>
<tr>
<td>i</td>
<td>忽略闲置和僵死的进程，这是一个开关式命令</td>
</tr>
<tr>
<td>k</td>
<td>终止一个进程，系统提示输入 PID 及发送的信号值。一般终止进程用 15 信号，不能正常结束则使用 9 信号。安全模式下该命令被屏蔽。</td>
</tr>
</tbody>
</table>
<h3 id="ps-工具的使用">ps 工具的使用</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">man ps</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 -l 参数可以显示自己这次登录的 bash 相关的进程信息罗列出来：</span></span><br><span class="line">ps -l</span><br><span class="line"><span class="comment"># 罗列出所有的进程信息：</span></span><br><span class="line">ps aux</span><br><span class="line"><span class="comment"># 若是查找其中的某个进程的话，我们还可以配合着 grep 和正则表达式一起使用：</span></span><br><span class="line">ps aux | grep zsh</span><br><span class="line"><span class="comment"># 查看时，将连同部分的进程呈树状显示出来：</span></span><br><span class="line">ps axjf</span><br><span class="line"><span class="comment"># 自定义我们所需要的参数显示：</span></span><br><span class="line">ps -afxo user,ppid,pid,pgid,<span class="built_in">command</span></span><br></pre></td></tr></table></figure>
<h3 id="pstree-的使用">pstree 的使用</h3>
<p>通过 <code>pstree</code> 可以很直接的看到相同的进程数量，最主要的还是我们可以看到所有进程之间的相关性。</p>
<table>
<thead>
<tr>
<th>参数选择</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-A</code></td>
<td>程序树之间以 ASCII 字符连接</td>
</tr>
<tr>
<td><code>-p</code></td>
<td>同时列出每个 process 的 PID</td>
</tr>
<tr>
<td><code>-u</code></td>
<td>同时列出每个 process 的所属账户名称</td>
</tr>
</tbody>
</table>
<h3 id="进程的执行顺序">进程的执行顺序</h3>
<p>我们在使用 ps 命令的时候可以看到大部分的进程都是处于休眠的状态，如果这些进程都被唤醒，那么该谁最先享受 CPU 的服务，后面的进程又该是一个什么样的顺序呢？进程调度的队列又该如何去排列呢？</p>
<p>当然就是靠该进程的优先级值来判定进程调度的优先级，而优先级的值就是上文所提到的 PR 与 nice 来控制与体现了</p>
<p>而 nice 的值我们是可以通过 nice 命令来修改的，而需要注意的是 nice 值可以调整的范围是 <code>-20 ~ 19</code>，其中 root 有着至高无上的权力，既可以调整自己的进程也可以调整其他用户的程序，并且是所有的值都可以用，而普通用户只可以调制属于自己的进程，并且其使用的范围只能是 <code>0 ~ 19</code>，因为系统为了避免一般用户抢占系统资源而设置的一个限制。</p>
]]></content>
  </entry>
  <entry>
    <title>train_test_split 参数详解</title>
    <url>/2022/06/01/fd61228310d24c999c625f49b503f7ce/</url>
    <content><![CDATA[<h2 id="简单用法如下：">简单用法如下：</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="built_in">print</span>(iris.data.shape)</span><br><span class="line"><span class="built_in">print</span>(iris.DESCR)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(150, 4)</span><br><span class="line">.. _iris_dataset:</span><br><span class="line"></span><br><span class="line">Iris plants dataset</span><br><span class="line">--------------------</span><br><span class="line"></span><br><span class="line">**Data Set Characteristics:**</span><br><span class="line"></span><br><span class="line">    :Number of Instances: 150 (50 in each of three classes)</span><br><span class="line">    :Number of Attributes: 4 numeric, predictive attributes and the class</span><br><span class="line">    :Attribute Information:</span><br><span class="line">        - sepal length in cm</span><br><span class="line">        - sepal width in cm</span><br><span class="line">        - petal length in cm</span><br><span class="line">        - petal width in cm</span><br><span class="line">        - class:</span><br><span class="line">                - Iris-Setosa</span><br><span class="line">                - Iris-Versicolour</span><br><span class="line">                - Iris-Virginica</span><br><span class="line"></span><br><span class="line">    :Summary Statistics:</span><br><span class="line"></span><br><span class="line">    ============== ==== ==== ======= ===== ====================</span><br><span class="line">                    Min  Max   Mean    SD   Class Correlation</span><br><span class="line">    ============== ==== ==== ======= ===== ====================</span><br><span class="line">    sepal length:   4.3  7.9   5.84   0.83    0.7826</span><br><span class="line">    sepal width:    2.0  4.4   3.05   0.43   -0.4194</span><br><span class="line">    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)</span><br><span class="line">    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)</span><br><span class="line">    ============== ==== ==== ======= ===== ====================</span><br><span class="line"></span><br><span class="line">    :Missing Attribute Values: None</span><br><span class="line">    :Class Distribution: 33.3% for each of 3 classes.</span><br><span class="line">    :Creator: R.A. Fisher</span><br><span class="line">    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</span><br><span class="line">    :Date: July, 1988</span><br><span class="line"></span><br><span class="line">The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken</span><br><span class="line">from Fisher&#x27;s paper. Note that it&#x27;s the same as in R, but not as in the UCI</span><br><span class="line">Machine Learning Repository, which has two wrong data points.</span><br><span class="line"></span><br><span class="line">This is perhaps the best known database to be found in the</span><br><span class="line">pattern recognition literature.  Fisher&#x27;s paper is a classic in the field and</span><br><span class="line">is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The</span><br><span class="line">data set contains 3 classes of 50 instances each, where each class refers to a</span><br><span class="line">type of iris plant.  One class is linearly separable from the other 2; the</span><br><span class="line">latter are NOT linearly separable from each other.</span><br><span class="line"></span><br><span class="line">.. topic:: References</span><br><span class="line"></span><br><span class="line">   - Fisher, R.A. &quot;The use of multiple measurements in taxonomic problems&quot;</span><br><span class="line">     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to</span><br><span class="line">     Mathematical Statistics&quot; (John Wiley, NY, 1950).</span><br><span class="line">   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.</span><br><span class="line">     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.</span><br><span class="line">   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System</span><br><span class="line">     Structure and Classification Rule for Recognition in Partially Exposed</span><br><span class="line">     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine</span><br><span class="line">     Intelligence, Vol. PAMI-2, No. 1, 67-71.</span><br><span class="line">   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions</span><br><span class="line">     on Information Theory, May 1972, 431-433.</span><br><span class="line">   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II</span><br><span class="line">     conceptual clustering system finds 3 classes in the data.</span><br><span class="line">   - Many, many more ...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">33</span>, stratify=y)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(X_test.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(112, 4)</span><br><span class="line">(38, 4)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>train_target：所要划分的样本结果</p>
</li>
<li>
<p>test_size：样本占比，如果是整数的话就是样本的数量</p>
</li>
<li>
<p>random_state：是随机数的种子。</p>
</li>
</ul>
<p>随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填 1，其他参数一样的情况下你得到的随机数组是一样的。但填 0 或不填，每次都会不一样。</p>
<ul>
<li>stratify 是为了保持 split 前类的分布。比如有 100 个数据，80 个属于 A 类，20 个属于 B 类。如果 train_test_split(… test_size=0.25, stratify = y_all), 那么 split 之后数据如下：</li>
</ul>
<p>training: 75 个数据，其中 60 个属于 A 类，15 个属于 B 类。<br>
testing: 25 个数据，其中 20 个属于 A 类，5 个属于 B 类。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">用了stratify参数，training集和testing集的类的比例是 A：B= 4：1，等同于split前的比例（80：20）。通常在这种类分布不平衡的情况下会用到stratify。</span><br><span class="line"></span><br><span class="line">将stratify=X就是按照X中的比例分配</span><br><span class="line"></span><br><span class="line">将stratify=y就是按照y中的比例分配</span><br></pre></td></tr></table></figure>
<p><strong>整体总结起来各个参数的设置及其类型如下：</strong></p>
<p>主要参数说明：</p>
<p>*arrays：可以是列表、numpy 数组、scipy 稀疏矩阵或 pandas 的数据框</p>
<p>test_size：可以为浮点、整数或 None，默认为 None</p>
<p>① 若为浮点时，表示测试集占总样本的百分比</p>
<p>② 若为整数时，表示测试样本样本数</p>
<p>③ 若为 None 时，test size 自动设置成 0.25</p>
<p>train_size：可以为浮点、整数或 None，默认为 None</p>
<p>① 若为浮点时，表示训练集占总样本的百分比</p>
<p>② 若为整数时，表示训练样本的样本数</p>
<p>③ 若为 None 时，train_size 自动被设置成 0.75</p>
<p>random_state：可以为整数、RandomState 实例或 None，默认为 None</p>
<p>① 若为 None 时，每次生成的数据都是随机，可能不一样</p>
<p>② 若为整数时，每次生成的数据都相同</p>
<p>stratify：可以为类似数组或 None</p>
<p>① 若为 None 时，划分出来的测试集或训练集中，其类标签的比例也是随机的</p>
<p>② 若不为 None 时，划分出来的测试集或训练集中，其类标签的比例同输入的数组中类标签的比例相同，可以用于处理不均衡的数据集</p>
]]></content>
  </entry>
  <entry>
    <title>paddle2.0高层API实现基于seq2seq的对联生成</title>
    <url>/2022/06/01/febcf959471d4ccb8df3760a6117820a/</url>
    <content><![CDATA[<p>[toc]</p>
<p>『深度学习 7 日打卡营·day5』</p>
<p>零基础解锁深度学习神器飞桨框架高层 API，七天时间助你掌握 CV、NLP 领域最火模型及应用。</p>
<ol>
<li>课程地址</li>
</ol>
<p>传送门：<a href="https://aistudio.baidu.com/aistudio/course/introduce/6771">https://aistudio.baidu.com/aistudio/course/introduce/6771</a></p>
<ol start="2">
<li>目标</li>
</ol>
<ul>
<li>掌握深度学习常用模型基础知识</li>
<li>熟练掌握一种国产开源深度学习框架</li>
<li>具备独立完成相关深度学习任务的能力</li>
<li>能用所学为 AI 加一份年味</li>
</ul>
<p>对联，是汉族传统文化之一，是写在纸、布上或刻在竹子、木头、柱子上的对偶语句。对联对仗工整，平仄协调，是一字一音的汉语独特的艺术形式，是中国传统文化瑰宝。</p>
<p>这里，我们将根据上联，自动写下联。这是一个典型的序列到序列(sequence2sequence, seq2seq）建模的场景，编码器-解码器（Encoder-Decoder）框架是解决 seq2seq 问题的经典方法，它能够将一个任意长度的源序列转换成另一个任意长度的目标序列：编码阶段将整个源序列编码成一个向量，解码阶段通过最大化预测序列概率，从中解码出整个目标序列。编码和解码的过程通常都使用 RNN 实现。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/e9dde4be7d0142068c5c921a1ca6a227a49aad4a8751425faead42f0348f5e01" width="500" height="313" ></center>
<br><center>图1：encoder-decoder示意图</center></br>
<p>这里的 Encoder 采用 LSTM，Decoder 采用带有 attention 机制的 LSTM。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/a791fee76388423da867676d667b7d4c2fbe9fe9096843878c6513a40c96c86d" width="500" height="313" ></center>
<br><center>图2：带有attention机制的encoder-decoder示意图</center></br>
<p>我们将以对联的上联作为 Encoder 的输出，下联作为 Decoder 的输入，训练模型。</p>
<h2 id="生成对联部分结果">生成对联部分结果</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">上联: 芳 草 绿 阳 关 塞 上 春 风 入 户	下联: 小 桥 流 水 人 家 中 喜 气 盈 门</span><br><span class="line"></span><br><span class="line">上联: 致 富 思 源 跟 党 走	下联: 脱 贫 致 富 为 民 圆</span><br><span class="line"></span><br><span class="line">上联: 欣 然 入 梦 抱 书 睡	下联: 快 意 临 风 把 酒 眠</span><br><span class="line"></span><br><span class="line">上联: 诗 赖 境 奇 赢 感 动	下联: 风 流 人 杰 显 精 神</span><br><span class="line"></span><br><span class="line">上联: 栀 子 牵 牛 犁 熟 地	下联: 莲 花 引 蝶 戏 开 花</span><br><span class="line"></span><br><span class="line">上联: 廿 载 相 交 成 知 己	下联: 千 秋 不 朽 著 文 章</span><br><span class="line"></span><br><span class="line">上联: 润	下联: 修</span><br><span class="line"></span><br><span class="line">上联: 设 帏 遇 芳 辰 百 岁 期 颐 刚 一 半	下联: 被 被 逢 盛 世 千 秋 俎 豆 尚 千 秋</span><br><span class="line"></span><br><span class="line">上联: 波 光 云 影 满 目 葱 茏 谁 道 人 间 无 胜 地	下联: 鸟 语 花 香 一 帘 幽 梦 我 听 天 下 有 知 音</span><br><span class="line"></span><br><span class="line">上联: 眸 中 映 月 心 如 镜	下联: 笔 底 生 花 气 若 虹</span><br><span class="line"></span><br><span class="line">上联: 何 事 营 生 闲 来 写 幅 青 山 卖	下联: 此 时 入 梦 醉 去 吟 诗 碧 水 流</span><br><span class="line"></span><br><span class="line">上联: 学 海 钩 深 毫 挥 具 见 三 长 足	下联: 书 山 登 绝 顶 摘 来 登 九 重 天</span><br><span class="line"></span><br><span class="line">上联: 女 子 千 金 一 笑 贵	下联: 男 儿 万 户 百 年 长</span><br><span class="line"></span><br><span class="line">上联: 柏 叶 为 铭 椒 花 献 瑞	下联: 梅 花 作 伴 凤 凤 鸣 春</span><br><span class="line"></span><br><span class="line">上联: 家 国 遽 亡 天 涯 有 客 图 恢 复	下联: 江 山 永 在 我 心 无 人 泪 滂 沱</span><br><span class="line"></span><br><span class="line">上联: 侍 郎 赋 咏 穷 三 峡	下联: 游 子 吟 诗 醉 九 江</span><br><span class="line"></span><br><span class="line">上联: 反 腐 堵 污 流 杜 渐 防 微 不 教 长 堤 崩 蚁 穴	下联: 倡 廉 增 正 气 阳 光 普 照 长 教 大 道 播 春 风</span><br><span class="line"></span><br><span class="line">上联: 已 兆 飞 熊 钓 渭 水	下联: 欲 栽 大 木 柱 长 天</span><br><span class="line"></span><br><span class="line">上联: 建 生 态 文 明 人 与 自 然 协 调 发 展	下联: 创 文 明 发 展 事 同 事 业 发 展 文 明</span><br><span class="line"></span><br><span class="line">上联: 于 自 不 高 于 他 不 下	下联: 以 人 为 本 为 我 无 为</span><br><span class="line"></span><br><span class="line">上联: 国 泰 民 安 军 民 人 人 歌 盛 世	下联: 民 安 国 泰 社 会 事 事 颂 和 谐</span><br><span class="line"></span><br><span class="line">上联: 金 龙 腾 大 地 看 四 野 平 畴 三 农 报 喜	下联: 玉 兔 跃 神 州 喜 九 州 大 地 万 户 迎 春</span><br><span class="line"></span><br><span class="line">上联: 兴 盛	下联: 平 安</span><br><span class="line"></span><br><span class="line">上联: 长 安 跑 马 谁 得 意	下联: 广 府 古 城 百 花 芳</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>AI Studio 平台后续会默认安装 PaddleNLP，在此之前可使用如下命令安装。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install --upgrade paddlenlp&gt;=<span class="number">2.0</span><span class="number">.0</span>b -i https://mirror.baidu.com/pypi/simple</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> paddlenlp</span><br><span class="line">paddlenlp.__version__</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;2.0.0rc1&#x27;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Vocab, Pad</span><br><span class="line"><span class="keyword">from</span> paddlenlp.metrics <span class="keyword">import</span> Perplexity</span><br><span class="line"><span class="keyword">from</span> paddlenlp.datasets <span class="keyword">import</span> CoupletDataset</span><br></pre></td></tr></table></figure>
<h1>数据部分</h1>
<h2 id="数据集介绍-2">数据集介绍</h2>
<p>采用开源的对联数据集<a href="https://github.com/v-zich/couplet-clean-dataset">couplet-clean-dataset</a>，该数据集过滤了<a href="https://github.com/wb14123/couplet-dataset"><br>
couplet-dataset</a>中的低俗、敏感内容。</p>
<p>这个数据集包含 70w 多条训练样本，1000 条验证样本和 1000 条测试样本。</p>
<p>下面列出一些训练集中对联样例：</p>
<p>上联：晚风摇树树还挺 下联：晨露润花花更红</p>
<p>上联：愿景天成无墨迹 下联：万方乐奏有于阗</p>
<p>上联：丹枫江冷人初去 下联：绿柳堤新燕复来</p>
<p>上联：闲来野钓人稀处 下联：兴起高歌酒醉中</p>
<h2 id="加载数据集">加载数据集</h2>
<p><code>paddlenlp.datasets</code>中内置了多个常见数据集，包括这里的对联数据集<code>CoupletDataset</code>。</p>
<br>
<p><code>paddlenlp.datasets</code>均继承<code>paddle.io.Dataset</code>，支持<code>paddle.io.Dataset</code>的所有功能：</p>
<ul>
<li>通过<code>len()</code>函数返回数据集长度，即样本数量。</li>
<li>下标索引：通过下标索引[n]获取第 n 条样本。</li>
<li>遍历数据集，获取所有样本。</li>
</ul>
<p>此外，<code>paddlenlp.datasets</code>，还支持如下操作：</p>
<ul>
<li>调用<code>get_datasets()</code>函数，传入 list 或者 string，获取相对应的 train_dataset、development_dataset、test_dataset 等。其中 train 为训练集，用于模型训练； development 为开发集，也称验证集 validation_dataset，用于模型参数调优；test 为测试集，用于评估算法的性能，但不会根据测试集上的表现再去调整模型或参数。</li>
<li>调用<code>apply()</code>函数，对数据集进行指定操作。</li>
</ul>
<br>
<p>这里的<code>CoupletDataset</code>数据集继承<code>TranslationDataset</code>，继承自<code>paddlenlp.datasets</code>，除以上通用用法外，还有一些个性设计：</p>
<ul>
<li>在<code>CoupletDataset class</code>中，还定义了<code>transform</code>函数，用于在每个句子的前后加上起始符<code>&lt;s&gt;</code>和结束符<code>&lt;/s&gt;</code>，并将原始数据映射成 id 序列。</li>
</ul>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d6c36cfd88eb4d0d87884f6c9cd47e466c2b562411394606be6683af08733045" width="200" height="200" ></center>
<br><center>图3：token-to-id示意图</center></br>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds, dev_ds, test_ds = CoupletDataset.get_datasets([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;dev&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100%|██████████| 21421/21421 [00:00&lt;00:00, 26153.43it/s]</span><br></pre></td></tr></table></figure>
<p>来看看数据集有多大，长什么样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_ds), <span class="built_in">len</span>(test_ds), <span class="built_in">len</span>(dev_ds))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入了起始符和终止符</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(train_ds[i])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(test_ds[i])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">702594 999 1000</span><br><span class="line">([1, 447, 3, 509, 153, 153, 279, 1517, 2], [1, 816, 294, 378, 9, 9, 142, 32, 2])</span><br><span class="line">([1, 594, 185, 10, 71, 18, 158, 912, 2], [1, 14, 105, 107, 835, 20, 268, 3855, 2])</span><br><span class="line">([1, 335, 830, 68, 425, 4, 482, 246, 2], [1, 94, 51, 1115, 23, 141, 761, 17, 2])</span><br><span class="line">([1, 126, 17, 217, 802, 4, 1103, 118, 2], [1, 125, 205, 47, 55, 57, 78, 15, 2])</span><br><span class="line">([1, 1203, 228, 390, 10, 1921, 827, 474, 2], [1, 1699, 89, 426, 317, 314, 43, 374, 2])</span><br><span class="line"></span><br><span class="line">([1, 6, 201, 350, 54, 1156, 2], [1, 64, 522, 305, 543, 102, 2])</span><br><span class="line">([1, 168, 1402, 61, 270, 11, 195, 253, 2], [1, 435, 782, 1046, 36, 188, 1016, 56, 2])</span><br><span class="line">([1, 744, 185, 744, 6, 18, 452, 16, 1410, 2], [1, 286, 102, 286, 74, 20, 669, 280, 261, 2])</span><br><span class="line">([1, 2577, 496, 1133, 60, 107, 2], [1, 1533, 318, 625, 1401, 172, 2])</span><br><span class="line">([1, 163, 261, 6, 64, 116, 350, 253, 2], [1, 96, 579, 13, 463, 16, 774, 586, 2])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab, _ = CoupletDataset.get_vocab()</span><br><span class="line">trg_idx2word = vocab.idx_to_token</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">pad_id = vocab[CoupletDataset.EOS_TOKEN]</span><br><span class="line">bos_id = vocab[CoupletDataset.BOS_TOKEN]</span><br><span class="line">eos_id = vocab[CoupletDataset.EOS_TOKEN]</span><br><span class="line"><span class="built_in">print</span> (pad_id, bos_id, eos_id)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2 1 2</span><br></pre></td></tr></table></figure>
<h2 id="构造-dataloder-2">构造 dataloder</h2>
<p>使用<code>paddle.io.DataLoader</code>来创建训练和预测时所需要的<code>DataLoader</code>对象。</p>
<p><code>paddle.io.DataLoader</code>返回一个迭代器，该迭代器根据<code>batch_sampler</code>指定的顺序迭代返回 dataset 数据。支持单进程或多进程加载数据，快！</p>
<br>
<p>接收如下重要参数：</p>
<ul>
<li><code>batch_sampler</code>：批采样器实例，用于在<code>paddle.io.DataLoader</code> 中迭代式获取 mini-batch 的样本下标数组，数组长度与 batch_size 一致。</li>
<li><code>collate_fn</code>：指定如何将样本列表组合为 mini-batch 数据。传给它参数需要是一个<code>callable</code>对象，需要实现对组建的 batch 的处理逻辑，并返回每个 batch 的数据。在这里传入的是<code>prepare_input</code>函数，对产生的数据进行 pad 操作，并返回实际长度等。</li>
</ul>
<p>PaddleNLP 提供了许多 NLP 任务中，用于数据处理、组 batch 数据的相关 API。</p>
<table>
<thead>
<tr>
<th>API</th>
<th style="text-align:left">简介</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>paddlenlp.data.Stack</code></td>
<td style="text-align:left">堆叠 N 个具有相同 shape 的输入数据来构建一个 batch</td>
</tr>
<tr>
<td><code>paddlenlp.data.Pad</code></td>
<td style="text-align:left">将长度不同的多个句子 padding 到统一长度，取 N 个输入数据中的最大长度</td>
</tr>
<tr>
<td><code>paddlenlp.data.Tuple</code></td>
<td style="text-align:left">将多个 batchify 函数包装在一起</td>
</tr>
</tbody>
</table>
<p>更多数据处理操作详见： <a href="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md">https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_data_loader</span>(<span class="params">dataset</span>):</span><br><span class="line">    data_loader = paddle.io.DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_sampler=<span class="literal">None</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        collate_fn=partial(prepare_input, pad_id=pad_id))</span><br><span class="line">    <span class="keyword">return</span> data_loader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_input</span>(<span class="params">insts, pad_id</span>):</span><br><span class="line">    src, src_length = Pad(pad_val=pad_id, ret_length=<span class="literal">True</span>)([inst[<span class="number">0</span>] <span class="keyword">for</span> inst <span class="keyword">in</span> insts])</span><br><span class="line">    tgt, tgt_length = Pad(pad_val=pad_id, ret_length=<span class="literal">True</span>)([inst[<span class="number">1</span>] <span class="keyword">for</span> inst <span class="keyword">in</span> insts])</span><br><span class="line">    tgt_mask = (tgt[:, :-<span class="number">1</span>] != pad_id).astype(paddle.get_default_dtype())</span><br><span class="line">    <span class="keyword">return</span> src, src_length, tgt[:, :-<span class="number">1</span>], tgt[:, <span class="number">1</span>:, np.newaxis], tgt_mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">use_gpu = <span class="literal">True</span></span><br><span class="line">device = paddle.set_device(<span class="string">&quot;gpu&quot;</span> <span class="keyword">if</span> use_gpu <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">hidden_size =<span class="number">256</span></span><br><span class="line">max_grad_norm = <span class="number">5.0</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">max_epoch = <span class="number">20</span></span><br><span class="line">model_path = <span class="string">&#x27;./couplet_models&#x27;</span></span><br><span class="line">log_freq = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define dataloader</span></span><br><span class="line">train_loader = create_data_loader(train_ds)</span><br><span class="line">test_loader = create_data_loader(test_ds)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_ds), <span class="built_in">len</span>(train_loader), batch_size)</span><br><span class="line"><span class="comment"># 702594 5490 128  共5490个batch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span> (<span class="built_in">len</span>(i))</span><br><span class="line">    <span class="keyword">for</span> ind, each <span class="keyword">in</span> <span class="built_in">enumerate</span>(i):</span><br><span class="line">        <span class="built_in">print</span> (ind, each.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">702594 5490 128</span><br><span class="line">5</span><br><span class="line">0 [128, 18]</span><br><span class="line">1 [128]</span><br><span class="line">2 [128, 17]</span><br><span class="line">3 [128, 17, 1]</span><br><span class="line">4 [128, 17]</span><br></pre></td></tr></table></figure>
<h1>模型部分</h1>
<p>下图是带有 Attention 的 Seq2Seq 模型结构。下面我们分别定义网络的每个部分，最后构建 Seq2Seq 主网络。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/8a9dda0434a14fb2a0837702e5f2f1096346810702aa4a6ab1fa7dafe548add6" width="600" height="600" ></center>
<br><center>图5：带有attention机制的encoder-decoder原理示意图</center></br>
<h2 id="定义-Encoder">定义 Encoder</h2>
<p>Encoder 部分非常简单，可以直接利用 PaddlePaddle2.0 提供的 RNN 系列 API 的<code>nn.LSTM</code>。</p>
<ol>
<li><code>nn.Embedding</code>：该接口用于构建 Embedding 的一个可调用对象，根据输入的 size (vocab_size, embedding_dim)自动构造一个二维 embedding 矩阵，用于 table-lookup。查表过程如下：</li>
</ol>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/54276195f4ce44b9ace89c5153300782a744a98343454410898fcfe81333f131" width="700" height="600" ></center>
<br><center>图5：token-to-id & 查表获取向量示意图</center></br>
<ol start="2">
<li><code>nn.LSTM</code>：提供序列，得到<code>encoder_output</code>和<code>encoder_state</code>。</li>
</ol>
<p>参数：</p>
<ul>
<li>input_size (int) 输入的大小。</li>
<li>hidden_size (int) - 隐藏状态大小。</li>
<li>num_layers (int，可选) - 网络层数。默认为 1。</li>
<li>direction (str，可选) - 网络迭代方向，可设置为 forward 或 bidirect（或 bidirectional）。默认为 forward。</li>
<li>time_major (bool，可选) - 指定 input 的第一个维度是否是 time steps。默认为 False。</li>
<li>dropout (float，可选) - dropout 概率，指的是出第一层外每层输入时的 dropout 概率。默认为 0。</li>
</ul>
<p><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/layer/rnn/LSTM_cn.html">https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/layer/rnn/LSTM_cn.html</a></p>
<p>输出:</p>
<ul>
<li>
<p><code>outputs (Tensor)</code> - 输出，由前向和后向 cell 的输出拼接得到。</p>
<p>如果<code>time_major</code>为 True，则 Tensor 的形状为 <code>[time_steps, batch_size, num_directions * hidden_size]</code></p>
<p>如果<code>time_major</code>为 False，则 Tensor 的形状为 <code>[batch_size, time_steps, num_directions * hidden_size]</code>，当 direction 设置为 bidirectional 时，num_directions 等于 2，否则等于 1。</p>
</li>
<li>
<p><code>final_states (tuple)</code> - 最终状态,一个包含 h 和 c 的元组。</p>
<p>形状为<code>[num_lauers * num_directions, batch_size, hidden_size]</code>,当 direction 设置为 bidirectional 时，num_directions 等于 2，否则等于 1。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqEncoder</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_size, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__()</span><br><span class="line">        self.embedder = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.lstm = nn.LSTM(</span><br><span class="line">            input_size=embed_dim,      <span class="comment"># 句子长度</span></span><br><span class="line">            hidden_size=hidden_size,   <span class="comment"># 隐藏层大小</span></span><br><span class="line">            num_layers=num_layers,     <span class="comment"># lstm层数</span></span><br><span class="line">            dropout=<span class="number">0.2</span> <span class="keyword">if</span> num_layers &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0.</span>)  <span class="comment"># 随机丢弃神经元</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sequence, sequence_length</span>):</span><br><span class="line">        inputs = self.embedder(sequence)</span><br><span class="line">        encoder_output, encoder_state = self.lstm(</span><br><span class="line">            inputs, sequence_length=sequence_length)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># y_out, (h, c)</span></span><br><span class="line">        <span class="comment"># encoder_output [128, 18, 256]  [batch_size,time_steps,hidden_size]</span></span><br><span class="line">        <span class="comment"># encoder_state (tuple) - 最终状态,一个包含h和c的元组。 [2, 128, 256] [2, 128, 256] [num_lauers * num_directions, batch_size, hidden_size]</span></span><br><span class="line">        <span class="keyword">return</span> encoder_output, encoder_state</span><br></pre></td></tr></table></figure>
<h2 id="定义-Decoder">定义 Decoder</h2>
<h3 id="定义-AttentionLayer">定义 AttentionLayer</h3>
<ol>
<li><code>nn.Linear</code>线性变换层传入 2 个参数</li>
</ol>
<ul>
<li>in_features (int) – 线性变换层输入单元的数目。</li>
<li>out_features (int) – 线性变换层输出单元的数目。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/bfc49550cd55599e607e2eb1d188149a.png" alt=""></p>
<ol start="2">
<li><code>paddle.matmul</code>用于计算两个 Tensor 的乘积，遵循完整的广播规则，关于广播规则，请参考<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/01_paddle2.0_introduction/basic_concept/broadcasting_cn.html#cn-user-guide-broadcasting">广播 (broadcasting)</a> 。 并且其行为与 numpy.matmul 一致。</li>
</ol>
<ul>
<li>x (Tensor) : 输入变量，类型为 Tensor，数据类型为 float32， float64。</li>
<li>y (Tensor) : 输入变量，类型为 Tensor，数据类型为 float32， float64。</li>
<li>transpose_x (bool，可选) : 相乘前是否转置 x，默认值为 False。</li>
<li>transpose_y (bool，可选) : 相乘前是否转置 y，默认值为 False。</li>
</ul>
<br>
<ol start="3">
<li>
<p><code>paddle.unsqueeze</code>用于向输入 Tensor 的 Shape 中一个或多个位置（axis）插入尺寸为 1 的维度</p>
</li>
<li>
<p><code>paddle.add</code>逐元素相加算子，输入 x 与输入 y 逐元素相加，并将各个位置的输出元素保存到返回结果中。</p>
</li>
</ol>
<p>输入 x 与输入 y 必须可以广播为相同形状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionLayer</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># MLP</span></span><br><span class="line">        self.input_proj = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.output_proj = nn.Linear(hidden_size + hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden, encoder_output, encoder_padding_mask</span>):</span><br><span class="line"></span><br><span class="line">        encoder_output = self.input_proj(encoder_output)</span><br><span class="line"></span><br><span class="line">        attn_scores = paddle.matmul(</span><br><span class="line">            paddle.unsqueeze(hidden, [<span class="number">1</span>]), encoder_output, transpose_y=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;attention score&#x27;, attn_scores.shape) #[128, 1, 18]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_scores = paddle.add(attn_scores, encoder_padding_mask)</span><br><span class="line"></span><br><span class="line">        attn_scores = F.softmax(attn_scores)</span><br><span class="line"></span><br><span class="line">        attn_out = paddle.squeeze(</span><br><span class="line">            paddle.matmul(attn_scores, encoder_output), [<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># print(&#x27;1 attn_out&#x27;, attn_out.shape) #[128, 256]</span></span><br><span class="line"></span><br><span class="line">        attn_out = paddle.concat([attn_out, hidden], <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(&#x27;2 attn_out&#x27;, attn_out.shape) #[128, 512]</span></span><br><span class="line"></span><br><span class="line">        attn_out = self.output_proj(attn_out)</span><br><span class="line">        <span class="comment"># print(&#x27;3 attn_out&#x27;, attn_out.shape) #[128, 256]</span></span><br><span class="line">        <span class="keyword">return</span> attn_out</span><br></pre></td></tr></table></figure>
<h3 id="定义-Seq2SeqDecoderCell">定义 Seq2SeqDecoderCell</h3>
<p>由于 Decoder 部分是带有 attention 的 LSTM，我们不能复用<code>nn.LSTM</code>，所以需要定义<code>Seq2SeqDecoderCell</code></p>
<ol>
<li><code>nn.LayerList</code> 用于保存子层列表，它包含的子层将被正确地注册和添加。列表中的子层可以像常规 python 列表一样被索引。这里添加了 num_layers=2 层 lstm。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoderCell</span>(nn.RNNCellBase):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoderCell, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># num_layers=2时 第一层输入为input_size + hidden_size 第二层输入为 hidden_size</span></span><br><span class="line">        self.lstm_cells = nn.LayerList([</span><br><span class="line">            nn.LSTMCell(</span><br><span class="line">                input_size=input_size + hidden_size <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> hidden_size,</span><br><span class="line">                hidden_size=hidden_size) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.attention_layer = AttentionLayer(hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                step_input,</span></span><br><span class="line"><span class="params">                states,</span></span><br><span class="line"><span class="params">                encoder_output,</span></span><br><span class="line"><span class="params">                encoder_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        lstm_states, input_feed = states</span><br><span class="line">        new_lstm_states = []</span><br><span class="line"></span><br><span class="line">        step_input = paddle.concat([step_input, input_feed], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i, lstm_cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.lstm_cells):</span><br><span class="line">            out, new_lstm_state = lstm_cell(step_input, lstm_states[i])</span><br><span class="line">            step_input = self.dropout(out)</span><br><span class="line">            new_lstm_states.append(new_lstm_state)</span><br><span class="line">        out = self.attention_layer(step_input, encoder_output,</span><br><span class="line">                                   encoder_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, [new_lstm_states, out]</span><br></pre></td></tr></table></figure>
<h3 id="定义-Seq2SeqDecoder">定义 Seq2SeqDecoder</h3>
<p>有了<code>Seq2SeqDecoderCell</code>，就可以构建<code>Seq2SeqDecoder</code>了</p>
<br>
<ol>
<li><code>paddle.nn.RNN</code> 该 OP 是循环神经网络（RNN）的封装，将输入的 Cell 封装为一个循环神经网络。它能够重复执行 cell.forward() 直到遍历完 input 中的所有 Tensor。</li>
</ol>
<ul>
<li>cell (RNNCellBase) - RNNCellBase 类的一个实例。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_size, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__()</span><br><span class="line">        self.embedder = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.lstm_attention = nn.RNN(</span><br><span class="line">            Seq2SeqDecoderCell(num_layers, embed_dim, hidden_size))</span><br><span class="line">        self.output_layer = nn.Linear(hidden_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, trg, decoder_initial_states, encoder_output,</span></span><br><span class="line"><span class="params">                encoder_padding_mask</span>):</span><br><span class="line">        inputs = self.embedder(trg)</span><br><span class="line"></span><br><span class="line">        decoder_output, _ = self.lstm_attention(</span><br><span class="line">            inputs,</span><br><span class="line">            initial_states=decoder_initial_states,</span><br><span class="line">            encoder_output=encoder_output,</span><br><span class="line">            encoder_padding_mask=encoder_padding_mask)</span><br><span class="line">        predict = self.output_layer(decoder_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> predict</span><br></pre></td></tr></table></figure>
<h2 id="构建主网络-Seq2SeqAttnModel">构建主网络 Seq2SeqAttnModel</h2>
<p>Encoder 和 Decoder 定义好之后，网络就可以构建起来了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttnModel</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_size, num_layers,</span></span><br><span class="line"><span class="params">                 eos_id=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttnModel, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.eos_id = eos_id</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.INF = <span class="number">1e9</span></span><br><span class="line">        self.encoder = Seq2SeqEncoder(vocab_size, embed_dim, hidden_size,</span><br><span class="line">                                      num_layers)</span><br><span class="line">        self.decoder = Seq2SeqDecoder(vocab_size, embed_dim, hidden_size,</span><br><span class="line">                                      num_layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, src_length, trg</span>):</span><br><span class="line">        <span class="comment"># encoder_output 各时刻的输出h</span></span><br><span class="line">        <span class="comment"># encoder_final_state 最后时刻的输出h，和记忆信号c</span></span><br><span class="line">        encoder_output, encoder_final_state = self.encoder(src, src_length)</span><br><span class="line">        <span class="comment"># print(&#x27;encoder_output shape&#x27;, encoder_output.shape)  #  [128, 18, 256]  [batch_size,time_steps,hidden_size]</span></span><br><span class="line">        <span class="comment"># print(&#x27;encoder_final_states shape&#x27;, encoder_final_state[0].shape, encoder_final_state[1].shape) #[2, 128, 256] [2, 128, 256] [num_lauers * num_directions, batch_size, hidden_size]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transfer shape of encoder_final_states to [num_layers, 2, batch_size, hidden_size]？？？</span></span><br><span class="line">        encoder_final_states = [</span><br><span class="line">            (encoder_final_state[<span class="number">0</span>][i], encoder_final_state[<span class="number">1</span>][i])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers)</span><br><span class="line">        ]</span><br><span class="line">        <span class="comment"># print(&#x27;encoder_final_states shape&#x27;, encoder_final_states[0][0].shape, encoder_final_states[0][1].shape) #[128, 256] [128, 256]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct decoder initial states: use input_feed and the shape is</span></span><br><span class="line">        <span class="comment"># [[h,c] * num_layers, input_feed], consistent with Seq2SeqDecoderCell.states</span></span><br><span class="line">        decoder_initial_states = [</span><br><span class="line">            encoder_final_states,</span><br><span class="line">            self.decoder.lstm_attention.cell.get_initial_states(</span><br><span class="line">                batch_ref=encoder_output, shape=[self.hidden_size])</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build attention mask to avoid paying attention on padddings</span></span><br><span class="line">        src_mask = (src != self.eos_id).astype(paddle.get_default_dtype())</span><br><span class="line">        <span class="comment"># print (&#x27;src_mask shape&#x27;, src_mask.shape)  #[128, 18]</span></span><br><span class="line">        <span class="comment"># print(src_mask[0, :])</span></span><br><span class="line"></span><br><span class="line">        encoder_padding_mask = (src_mask - <span class="number">1.0</span>) * self.INF</span><br><span class="line">        <span class="comment"># print (&#x27;encoder_padding_mask&#x27;, encoder_padding_mask.shape)  #[128, 18]</span></span><br><span class="line">        <span class="comment"># print(encoder_padding_mask[0, :])</span></span><br><span class="line"></span><br><span class="line">        encoder_padding_mask = paddle.unsqueeze(encoder_padding_mask, [<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># print(&#x27;encoder_padding_mask&#x27;, encoder_padding_mask.shape)  #[128, 1, 18]</span></span><br><span class="line"></span><br><span class="line">        predict = self.decoder(trg, decoder_initial_states, encoder_output,</span><br><span class="line">                               encoder_padding_mask)</span><br><span class="line">        <span class="comment"># print(&#x27;predict&#x27;, predict.shape)   #[128, 17, 7931]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> predict</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数">定义损失函数</h2>
<p>这里使用的是交叉熵损失函数，我们需要将 padding 位置的 loss 置为 0，因此需要在损失函数中引入<code>trg_mask</code>参数，由于 PaddlePaddle 框架提供的<code>paddle.nn.CrossEntropyLoss</code>不能接受<code>trg_mask</code>参数，因此在这里需要重新定义：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CrossEntropyCriterion</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CrossEntropyCriterion, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, predict, label, trg_mask</span>):</span><br><span class="line">        cost = F.softmax_with_cross_entropy(</span><br><span class="line">            logits=predict, label=label, soft_label=<span class="literal">False</span>)</span><br><span class="line">        cost = paddle.squeeze(cost, axis=[<span class="number">2</span>])</span><br><span class="line">        masked_cost = cost * trg_mask</span><br><span class="line">        batch_mean_cost = paddle.mean(masked_cost, axis=[<span class="number">0</span>])</span><br><span class="line">        seq_cost = paddle.<span class="built_in">sum</span>(batch_mean_cost)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_cost</span><br></pre></td></tr></table></figure>
<h1>执行过程</h1>
<h2 id="训练过程">训练过程</h2>
<p>使用高层 API 执行训练，需要调用<code>prepare</code>和<code>fit</code>函数。</p>
<p>在<code>prepare</code>函数中，配置优化器、损失函数，以及评价指标。其中评价指标使用的是 PaddleNLP 提供的困惑度计算 API <code>paddlenlp.metrics.Perplexity</code>。</p>
<p>如果你安装了 VisualDL，可以在 fit 中添加一个 callbacks 参数使用 VisualDL 观测你的训练过程，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(train_data=train_loader,</span><br><span class="line">            epochs=max_epoch,</span><br><span class="line">            eval_freq=<span class="number">1</span>,</span><br><span class="line">            save_freq=<span class="number">1</span>,</span><br><span class="line">            save_dir=model_path,</span><br><span class="line">            log_freq=log_freq,</span><br><span class="line">            callbacks=[paddle.callbacks.VisualDL(<span class="string">&#x27;./log&#x27;</span>)])</span><br></pre></td></tr></table></figure>
<p>在这里，由于对联生成任务没有明确的评价指标，因此，可以在保存的多个模型中，通过人工评判生成结果选择最好的模型。</p>
<p>本项目中，为了便于演示，已经将训练好的模型参数载入模型，并省略了训练过程。读者自己实验的时候，可以尝试自行修改超参数，调用下面被注释掉的<code>fit</code>函数，重新进行训练。</p>
<p>如果读者想要在更短的时间内得到效果不错的模型，可以使用预训练模型技术，例如<a href="https://aistudio.baidu.com/aistudio/projectdetail/1339888">《预训练模型 ERNIE-GEN 自动写诗》</a>项目为大家展示了如何利用预训练的生成模型进行训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = paddle.Model(Seq2SeqAttnModel(vocab_size, hidden_size, hidden_size, num_layers, pad_id))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = paddle.optimizer.Adam(</span><br><span class="line">    learning_rate=learning_rate, parameters=model.parameters())</span><br><span class="line"></span><br><span class="line">model.load(<span class="string">&#x27;couplet_models/model_18&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model.prepare(optimizer, CrossEntropyCriterion(), Perplexity())</span><br><span class="line"></span><br><span class="line">model.fit(train_data=train_loader,</span><br><span class="line">            epochs=<span class="number">1</span>,</span><br><span class="line">            eval_freq=<span class="number">1</span>,</span><br><span class="line">            save_freq=<span class="number">5</span>,</span><br><span class="line">            save_dir=model_path,</span><br><span class="line">            log_freq=log_freq)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The loss value printed in the log is the current step, and the metric is the average value of previous step.</span><br><span class="line">Epoch 1/1</span><br><span class="line">step  200/5490 - loss: 29.0412 - Perplexity: 28.2248 - 72ms/step</span><br><span class="line">step  400/5490 - loss: 31.3796 - Perplexity: 28.4654 - 72ms/step</span><br><span class="line">step  600/5490 - loss: 30.2077 - Perplexity: 28.4107 - 72ms/step</span><br><span class="line">step  800/5490 - loss: 30.8788 - Perplexity: 28.4995 - 73ms/step</span><br><span class="line">step 1000/5490 - loss: 28.7426 - Perplexity: 28.7224 - 72ms/step</span><br><span class="line">step 1200/5490 - loss: 32.3817 - Perplexity: 28.8973 - 71ms/step</span><br><span class="line">step 1400/5490 - loss: 31.8954 - Perplexity: 28.9987 - 71ms/step</span><br><span class="line">step 1600/5490 - loss: 30.3898 - Perplexity: 29.0871 - 71ms/step</span><br><span class="line">step 1800/5490 - loss: 32.1190 - Perplexity: 29.1689 - 72ms/step</span><br><span class="line">step 2000/5490 - loss: 32.3143 - Perplexity: 29.2302 - 72ms/step</span><br><span class="line">step 2200/5490 - loss: 31.6980 - Perplexity: 29.2954 - 71ms/step</span><br><span class="line">step 2400/5490 - loss: 29.9607 - Perplexity: 29.3576 - 71ms/step</span><br><span class="line">step 2600/5490 - loss: 31.6618 - Perplexity: 29.3990 - 71ms/step</span><br><span class="line">step 2800/5490 - loss: 35.0776 - Perplexity: 29.4590 - 71ms/step</span><br><span class="line">step 3000/5490 - loss: 30.2568 - Perplexity: 29.5161 - 71ms/step</span><br><span class="line">step 3200/5490 - loss: 29.4113 - Perplexity: 29.5687 - 70ms/step</span><br><span class="line">step 3400/5490 - loss: 32.5356 - Perplexity: 29.6236 - 71ms/step</span><br><span class="line">step 3600/5490 - loss: 30.4489 - Perplexity: 29.6678 - 71ms/step</span><br><span class="line">step 3800/5490 - loss: 30.7146 - Perplexity: 29.6957 - 71ms/step</span><br><span class="line">step 4000/5490 - loss: 31.3794 - Perplexity: 29.7266 - 71ms/step</span><br><span class="line">step 4200/5490 - loss: 33.3526 - Perplexity: 29.7954 - 71ms/step</span><br><span class="line">step 4400/5490 - loss: 30.6265 - Perplexity: 29.8421 - 71ms/step</span><br><span class="line">step 4600/5490 - loss: 30.8788 - Perplexity: 29.8787 - 71ms/step</span><br><span class="line">step 4800/5490 - loss: 28.6094 - Perplexity: 29.9268 - 71ms/step</span><br><span class="line">step 5000/5490 - loss: 31.5489 - Perplexity: 29.9706 - 71ms/step</span><br><span class="line">step 5200/5490 - loss: 31.6076 - Perplexity: 30.0078 - 71ms/step</span><br><span class="line">step 5400/5490 - loss: 31.7482 - Perplexity: 30.0651 - 72ms/step</span><br><span class="line">step 5490/5490 - loss: 31.3067 - Perplexity: 30.0837 - 72ms/step</span><br><span class="line">save checkpoint at /home/aistudio/couplet_models/0</span><br><span class="line">save checkpoint at /home/aistudio/couplet_models/final</span><br></pre></td></tr></table></figure>
<h2 id="模型预测-3">模型预测</h2>
<h3 id="定义预测网络-Seq2SeqAttnInferModel">定义预测网络 Seq2SeqAttnInferModel</h3>
<p>预测网络继承上面的主网络<code>Seq2SeqAttnModel</code>，定义子类<code>Seq2SeqAttnInferModel</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttnInferModel</span>(<span class="title class_ inherited__">Seq2SeqAttnModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size,</span></span><br><span class="line"><span class="params">                 embed_dim,</span></span><br><span class="line"><span class="params">                 hidden_size,</span></span><br><span class="line"><span class="params">                 num_layers,</span></span><br><span class="line"><span class="params">                 bos_id=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 eos_id=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 beam_size=<span class="number">4</span>,</span></span><br><span class="line"><span class="params">                 max_out_len=<span class="number">256</span></span>):</span><br><span class="line">        self.bos_id = bos_id</span><br><span class="line">        self.beam_size = beam_size</span><br><span class="line">        self.max_out_len = max_out_len</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttnInferModel, self).__init__(</span><br><span class="line">            vocab_size, embed_dim, hidden_size, num_layers, eos_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dynamic decoder for inference</span></span><br><span class="line">        self.beam_search_decoder = nn.BeamSearchDecoder(</span><br><span class="line">            self.decoder.lstm_attention.cell,</span><br><span class="line">            start_token=bos_id,</span><br><span class="line">            end_token=eos_id,</span><br><span class="line">            beam_size=beam_size,</span><br><span class="line">            embedding_fn=self.decoder.embedder,</span><br><span class="line">            output_fn=self.decoder.output_layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, src_length</span>):</span><br><span class="line">        encoder_output, encoder_final_state = self.encoder(src, src_length)</span><br><span class="line"></span><br><span class="line">        encoder_final_state = [</span><br><span class="line">            (encoder_final_state[<span class="number">0</span>][i], encoder_final_state[<span class="number">1</span>][i])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initial decoder initial states</span></span><br><span class="line">        decoder_initial_states = [</span><br><span class="line">            encoder_final_state,</span><br><span class="line">            self.decoder.lstm_attention.cell.get_initial_states(</span><br><span class="line">                batch_ref=encoder_output, shape=[self.hidden_size])</span><br><span class="line">        ]</span><br><span class="line">        <span class="comment"># Build attention mask to avoid paying attention on paddings</span></span><br><span class="line">        src_mask = (src != self.eos_id).astype(paddle.get_default_dtype())</span><br><span class="line"></span><br><span class="line">        encoder_padding_mask = (src_mask - <span class="number">1.0</span>) * self.INF</span><br><span class="line">        encoder_padding_mask = paddle.unsqueeze(encoder_padding_mask, [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Tile the batch dimension with beam_size</span></span><br><span class="line">        encoder_output = nn.BeamSearchDecoder.tile_beam_merge_with_batch(</span><br><span class="line">            encoder_output, self.beam_size)</span><br><span class="line">        encoder_padding_mask = nn.BeamSearchDecoder.tile_beam_merge_with_batch(</span><br><span class="line">            encoder_padding_mask, self.beam_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dynamic decoding with beam search</span></span><br><span class="line">        seq_output, _ = nn.dynamic_decode(</span><br><span class="line">            decoder=self.beam_search_decoder,</span><br><span class="line">            inits=decoder_initial_states,</span><br><span class="line">            max_step_num=self.max_out_len,</span><br><span class="line">            encoder_output=encoder_output,</span><br><span class="line">            encoder_padding_mask=encoder_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> seq_output</span><br></pre></td></tr></table></figure>
<h2 id="解码部分">解码部分</h2>
<p>接下来对我们的任务选择 beam search 解码方式，可以指定 beam_size 为 10。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">post_process_seq</span>(<span class="params">seq, bos_idx, eos_idx, output_bos=<span class="literal">False</span>, output_eos=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Post-process the decoded sequence.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    eos_pos = <span class="built_in">len</span>(seq) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">        <span class="keyword">if</span> idx == eos_idx:</span><br><span class="line">            eos_pos = i</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    seq = [</span><br><span class="line">        idx <span class="keyword">for</span> idx <span class="keyword">in</span> seq[:eos_pos + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> (output_bos <span class="keyword">or</span> idx != bos_idx) <span class="keyword">and</span> (output_eos <span class="keyword">or</span> idx != eos_idx)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> seq</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">beam_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># init_from_ckpt = &#x27;./couplet_models/0&#x27; # for test</span></span><br><span class="line"><span class="comment"># infer_output_file = &#x27;./infer_output.txt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># test_loader, vocab_size, pad_id, bos_id, eos_id = create_data_loader(test_ds, batch_size)</span></span><br><span class="line"><span class="comment"># vocab, _ = CoupletDataset.get_vocab()</span></span><br><span class="line"><span class="comment"># trg_idx2word = vocab.idx_to_token</span></span><br><span class="line"></span><br><span class="line">model = paddle.Model(</span><br><span class="line">    Seq2SeqAttnInferModel(</span><br><span class="line">        vocab_size,</span><br><span class="line">        hidden_size,</span><br><span class="line">        hidden_size,</span><br><span class="line">        num_layers,</span><br><span class="line">        bos_id=bos_id,</span><br><span class="line">        eos_id=eos_id,</span><br><span class="line">        beam_size=beam_size,</span><br><span class="line">        max_out_len=<span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">model.prepare()</span><br></pre></td></tr></table></figure>
<p>在预测之前，我们需要将训练好的模型参数 load 进预测网络，之后我们就可以根据对联的上联，生成对联的下联啦！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load(<span class="string">&#x27;couplet_models/final&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_ds = CoupletDataset.get_datasets([<span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader():</span><br><span class="line">    inputs = data[:<span class="number">2</span>]</span><br><span class="line">    finished_seq = model.predict_batch(inputs=<span class="built_in">list</span>(inputs))[<span class="number">0</span>]</span><br><span class="line">    finished_seq = finished_seq[:, :, np.newaxis] <span class="keyword">if</span> <span class="built_in">len</span>(</span><br><span class="line">        finished_seq.shape) == <span class="number">2</span> <span class="keyword">else</span> finished_seq</span><br><span class="line">    finished_seq = np.transpose(finished_seq, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ins <span class="keyword">in</span> finished_seq:</span><br><span class="line">        <span class="keyword">for</span> beam <span class="keyword">in</span> ins:</span><br><span class="line">            id_list = post_process_seq(beam, bos_id, eos_id)</span><br><span class="line">            word_list_l = [trg_idx2word[<span class="built_in">id</span>] <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> test_ds[idx][<span class="number">0</span>]][<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            word_list_r = [trg_idx2word[<span class="built_in">id</span>] <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> id_list]</span><br><span class="line">            sequence = <span class="string">&quot;上联: &quot;</span>+<span class="string">&quot; &quot;</span>.join(word_list_l)+<span class="string">&quot;\t下联: &quot;</span>+<span class="string">&quot; &quot;</span>.join(word_list_r) + <span class="string">&quot;\n&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(sequence)</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h1>PaddleNLP 更多教程</h1>
<ul>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1283423">使用 seq2vec 模块进行句子情感分析</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1294333">使用预训练模型 ERNIE 优化情感分析</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1317771">使用 BiGRU-CRF 模型完成快递单信息抽取</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1329361">使用预训练模型 ERNIE 优化快递单信息抽取</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1339888">使用预训练模型 ERNIE-GEN 实现智能写诗</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1290873">使用 TCN 网络完成新冠疫情病例数预测</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1339612">使用预训练模型完成阅读理解</a></li>
<li><a href="https://aistudio.baidu.com/aistudio/projectdetail/1468469">自定义数据集实现文本多分类任务</a></li>
</ul>
<h1>加入交流群，一起学习吧</h1>
<p>现在就加入 PaddleNLP 的 QQ 技术交流群，一起交流 NLP 技术吧！</p>
<img src="https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394" width="200" height="250" >
]]></content>
  </entry>
</search>
